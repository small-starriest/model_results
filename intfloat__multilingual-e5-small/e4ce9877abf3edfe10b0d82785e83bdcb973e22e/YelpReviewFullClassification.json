{
  "dataset_revision": "c1f9ee939b7d05667af864ee1cb066393154bf85",
  "evaluation_time": 26.398935317993164,
  "kg_co2_emissions": 0.004039669578175343,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.54169921875,
        "f1": 0.5286240846450607,
        "f1_weighted": 0.5286050437931678,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.54169921875,
        "scores_per_experiment": [
          {
            "accuracy": 0.54248046875,
            "f1": 0.5298271079685711,
            "f1_weighted": 0.5297984179307731
          },
          {
            "accuracy": 0.54296875,
            "f1": 0.5270516278600212,
            "f1_weighted": 0.527045640490579
          },
          {
            "accuracy": 0.53369140625,
            "f1": 0.5212844069723379,
            "f1_weighted": 0.5212620766656453
          },
          {
            "accuracy": 0.552734375,
            "f1": 0.5430126107084371,
            "f1_weighted": 0.5429958038382512
          },
          {
            "accuracy": 0.5361328125,
            "f1": 0.5221720494617621,
            "f1_weighted": 0.5221504164310689
          },
          {
            "accuracy": 0.5419921875,
            "f1": 0.525207143257741,
            "f1_weighted": 0.525184589792998
          },
          {
            "accuracy": 0.55859375,
            "f1": 0.5440074871790593,
            "f1_weighted": 0.5440012831224663
          },
          {
            "accuracy": 0.548828125,
            "f1": 0.5366028402744158,
            "f1_weighted": 0.5365797768536992
          },
          {
            "accuracy": 0.5341796875,
            "f1": 0.5227191273774846,
            "f1_weighted": 0.5227063531925424
          },
          {
            "accuracy": 0.525390625,
            "f1": 0.5143564453907764,
            "f1_weighted": 0.5143260796136538
          }
        ]
      }
    ]
  },
  "task_name": "YelpReviewFullClassification"
}