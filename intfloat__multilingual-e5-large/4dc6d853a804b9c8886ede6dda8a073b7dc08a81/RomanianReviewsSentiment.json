{
  "dataset_revision": "358bcc95aeddd5d07a4524ee416f03d993099b23",
  "evaluation_time": 13.233479738235474,
  "kg_co2_emissions": 0.00210672709977742,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.53974609375,
        "f1": 0.45316743384959,
        "f1_weighted": 0.5739864747894015,
        "hf_subset": "default",
        "languages": [
          "ron-Latn"
        ],
        "main_score": 0.53974609375,
        "scores_per_experiment": [
          {
            "accuracy": 0.48388671875,
            "f1": 0.4158121808843158,
            "f1_weighted": 0.5217578158228238
          },
          {
            "accuracy": 0.4326171875,
            "f1": 0.3886562137257773,
            "f1_weighted": 0.46036972159580314
          },
          {
            "accuracy": 0.4775390625,
            "f1": 0.42219089348568495,
            "f1_weighted": 0.5178974996035091
          },
          {
            "accuracy": 0.61328125,
            "f1": 0.4975519123087014,
            "f1_weighted": 0.6431222467310993
          },
          {
            "accuracy": 0.59375,
            "f1": 0.48798037859095417,
            "f1_weighted": 0.6155209529358256
          },
          {
            "accuracy": 0.55517578125,
            "f1": 0.4462598187902921,
            "f1_weighted": 0.5780927765883288
          },
          {
            "accuracy": 0.552734375,
            "f1": 0.46872285227320093,
            "f1_weighted": 0.5982773899861099
          },
          {
            "accuracy": 0.5791015625,
            "f1": 0.48299876255970303,
            "f1_weighted": 0.6218951338639651
          },
          {
            "accuracy": 0.56689453125,
            "f1": 0.47941466740794625,
            "f1_weighted": 0.6051438371010696
          },
          {
            "accuracy": 0.54248046875,
            "f1": 0.44208665846932377,
            "f1_weighted": 0.5777873736654795
          }
        ]
      }
    ]
  },
  "task_name": "RomanianReviewsSentiment"
}