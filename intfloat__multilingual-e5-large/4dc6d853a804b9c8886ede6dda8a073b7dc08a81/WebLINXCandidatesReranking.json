{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 6663.114423751831,
  "kg_co2_emissions": 1.307892583177313,
  "mteb_version": "1.12.75",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06470750847155342,
        "map": 0.0822762338662915,
        "mrr": 0.06470750847155342,
        "nAUC_map_diff1": 0.08764552015316791,
        "nAUC_map_max": 0.09484097306750225,
        "nAUC_map_std": 0.31176058380700505,
        "nAUC_mrr_diff1": 0.09285220128829375,
        "nAUC_mrr_max": 0.1091082073468003,
        "nAUC_mrr_std": 0.28475050346278713
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09496614553062885,
        "map": 0.1148437307669231,
        "mrr": 0.09496614553062885,
        "nAUC_map_diff1": 0.17561325007362008,
        "nAUC_map_max": 0.0452455952047916,
        "nAUC_map_std": 0.07771160789109144,
        "nAUC_mrr_diff1": 0.18198793116790488,
        "nAUC_mrr_max": 0.055753247248055214,
        "nAUC_mrr_std": 0.07809033835007997
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07205664834315738,
        "map": 0.08992405814500713,
        "mrr": 0.07205664834315738,
        "nAUC_map_diff1": 0.07252683513045247,
        "nAUC_map_max": 0.033064232331541134,
        "nAUC_map_std": 0.17815941373562758,
        "nAUC_mrr_diff1": 0.05765340353172872,
        "nAUC_mrr_max": 0.03753934366187255,
        "nAUC_mrr_std": 0.16321853314207158
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08525529250331063,
        "map": 0.10363443118427126,
        "mrr": 0.08525529250331063,
        "nAUC_map_diff1": 0.12978139441492095,
        "nAUC_map_max": 0.049657387807703526,
        "nAUC_map_std": 0.16269406309324555,
        "nAUC_mrr_diff1": 0.1349916200036057,
        "nAUC_mrr_max": 0.0639135760980854,
        "nAUC_mrr_std": 0.13686394868134125
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06167757179207561,
        "map": 0.07970141327691567,
        "mrr": 0.06167757179207561,
        "nAUC_map_diff1": 0.1884533687316294,
        "nAUC_map_max": 0.05319840504369158,
        "nAUC_map_std": 0.05497614422885487,
        "nAUC_mrr_diff1": 0.20004096155794832,
        "nAUC_mrr_max": 0.04723318679940537,
        "nAUC_mrr_std": 0.03988774210800338
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08815044593292096,
        "map": 0.10778669826424032,
        "mrr": 0.08815044593292096,
        "nAUC_map_diff1": 0.1656309644930538,
        "nAUC_map_max": -0.016735690618462033,
        "nAUC_map_std": 0.09450606431931399,
        "nAUC_mrr_diff1": 0.17390200304658396,
        "nAUC_mrr_max": -0.009816730280088318,
        "nAUC_mrr_std": 0.08784336030214239
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}