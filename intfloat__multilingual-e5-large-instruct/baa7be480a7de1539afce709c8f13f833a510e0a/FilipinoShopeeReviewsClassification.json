{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 38.07623362541199,
  "kg_co2_emissions": 0.00719288548282903,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.40712890625,
        "f1": 0.36989544105480804,
        "f1_weighted": 0.36987855563889543,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.40712890625,
        "scores_per_experiment": [
          {
            "accuracy": 0.412109375,
            "f1": 0.38029416155341395,
            "f1_weighted": 0.38026463329508003
          },
          {
            "accuracy": 0.404296875,
            "f1": 0.3581368937743914,
            "f1_weighted": 0.3581389686618936
          },
          {
            "accuracy": 0.4091796875,
            "f1": 0.35690076964400896,
            "f1_weighted": 0.35689816360641813
          },
          {
            "accuracy": 0.416015625,
            "f1": 0.3882220677190714,
            "f1_weighted": 0.3881798364295841
          },
          {
            "accuracy": 0.40576171875,
            "f1": 0.3597198904957727,
            "f1_weighted": 0.35969557928302476
          },
          {
            "accuracy": 0.40185546875,
            "f1": 0.3558669355022719,
            "f1_weighted": 0.3558086023604885
          },
          {
            "accuracy": 0.42626953125,
            "f1": 0.38910713938856906,
            "f1_weighted": 0.38906719497182696
          },
          {
            "accuracy": 0.42578125,
            "f1": 0.4001030268420072,
            "f1_weighted": 0.4001298487388487
          },
          {
            "accuracy": 0.42333984375,
            "f1": 0.3945770619080148,
            "f1_weighted": 0.39454108287853823
          },
          {
            "accuracy": 0.3466796875,
            "f1": 0.31602646372055937,
            "f1_weighted": 0.3160616461632515
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.408056640625,
        "f1": 0.37164051409279375,
        "f1_weighted": 0.3716222476060453,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.408056640625,
        "scores_per_experiment": [
          {
            "accuracy": 0.41162109375,
            "f1": 0.38197475951958737,
            "f1_weighted": 0.3819463553670168
          },
          {
            "accuracy": 0.3984375,
            "f1": 0.34885409544223284,
            "f1_weighted": 0.34886294203441076
          },
          {
            "accuracy": 0.40576171875,
            "f1": 0.3512290420660923,
            "f1_weighted": 0.3512239547077066
          },
          {
            "accuracy": 0.42236328125,
            "f1": 0.3947839750239684,
            "f1_weighted": 0.39474727523297093
          },
          {
            "accuracy": 0.41259765625,
            "f1": 0.37071526192949217,
            "f1_weighted": 0.37068687983657694
          },
          {
            "accuracy": 0.412109375,
            "f1": 0.36936163887238166,
            "f1_weighted": 0.3693052281189743
          },
          {
            "accuracy": 0.41015625,
            "f1": 0.37415654822023126,
            "f1_weighted": 0.37413001760504766
          },
          {
            "accuracy": 0.42822265625,
            "f1": 0.403083521411855,
            "f1_weighted": 0.40308994411563026
          },
          {
            "accuracy": 0.41650390625,
            "f1": 0.3892077693274648,
            "f1_weighted": 0.3891646441037867
          },
          {
            "accuracy": 0.36279296875,
            "f1": 0.33303852911463244,
            "f1_weighted": 0.33306523493833223
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}