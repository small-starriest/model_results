{
  "dataset_revision": "952c9525954c1dac50d5f95945eb5585bb6464e7",
  "evaluation_time": 13.140016794204712,
  "kg_co2_emissions": 0.002127196950497458,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.689013671875,
        "f1": 0.5375846611904154,
        "f1_weighted": 0.7381353527893025,
        "hf_subset": "default",
        "languages": [
          "heb-Hebr"
        ],
        "main_score": 0.689013671875,
        "scores_per_experiment": [
          {
            "accuracy": 0.787109375,
            "f1": 0.6093085798282588,
            "f1_weighted": 0.7983232152464239
          },
          {
            "accuracy": 0.7041015625,
            "f1": 0.5601901085504452,
            "f1_weighted": 0.7607683977219533
          },
          {
            "accuracy": 0.61474609375,
            "f1": 0.5083663514280626,
            "f1_weighted": 0.6986397625680327
          },
          {
            "accuracy": 0.6923828125,
            "f1": 0.5388286398992883,
            "f1_weighted": 0.7477552650669288
          },
          {
            "accuracy": 0.71875,
            "f1": 0.5470499226656287,
            "f1_weighted": 0.7612716457365427
          },
          {
            "accuracy": 0.6181640625,
            "f1": 0.496862092495989,
            "f1_weighted": 0.6647536079256813
          },
          {
            "accuracy": 0.73779296875,
            "f1": 0.5539431046978529,
            "f1_weighted": 0.7695910751617868
          },
          {
            "accuracy": 0.56005859375,
            "f1": 0.4491317335793499,
            "f1_weighted": 0.6326626326545958
          },
          {
            "accuracy": 0.73583984375,
            "f1": 0.5754084940400696,
            "f1_weighted": 0.7873215187910377
          },
          {
            "accuracy": 0.72119140625,
            "f1": 0.5367575847192088,
            "f1_weighted": 0.7602664070200421
          }
        ]
      }
    ]
  },
  "task_name": "HebrewSentimentAnalysis"
}