{
  "dataset_revision": "12ca3b695563788fead87a982ad1a068284413f4",
  "evaluation_time": 10.448941230773926,
  "kg_co2_emissions": 0.0015513757182790348,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.305810546875,
        "f1": 0.16884998921990838,
        "f1_weighted": 0.32164237699932985,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.305810546875,
        "scores_per_experiment": [
          {
            "accuracy": 0.34130859375,
            "f1": 0.1748308312945704,
            "f1_weighted": 0.357540933663206
          },
          {
            "accuracy": 0.2744140625,
            "f1": 0.17009651733350117,
            "f1_weighted": 0.29248977772217144
          },
          {
            "accuracy": 0.24755859375,
            "f1": 0.156765926917781,
            "f1_weighted": 0.25477150795949927
          },
          {
            "accuracy": 0.2802734375,
            "f1": 0.15856695635430545,
            "f1_weighted": 0.2961950150862466
          },
          {
            "accuracy": 0.279296875,
            "f1": 0.17885692397527492,
            "f1_weighted": 0.3114163951310328
          },
          {
            "accuracy": 0.31640625,
            "f1": 0.16452794249949246,
            "f1_weighted": 0.32885502479136214
          },
          {
            "accuracy": 0.3447265625,
            "f1": 0.19615554654292577,
            "f1_weighted": 0.3947728921799572
          },
          {
            "accuracy": 0.27978515625,
            "f1": 0.15494009168006143,
            "f1_weighted": 0.29283759554416366
          },
          {
            "accuracy": 0.3359375,
            "f1": 0.1627593480276862,
            "f1_weighted": 0.33641473731345106
          },
          {
            "accuracy": 0.3583984375,
            "f1": 0.17099980757348504,
            "f1_weighted": 0.3511298906022076
          }
        ]
      }
    ]
  },
  "task_name": "MAUDLegalBenchClassification"
}