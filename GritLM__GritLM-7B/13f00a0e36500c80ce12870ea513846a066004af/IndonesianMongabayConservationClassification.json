{
  "dataset_revision": "c9e9f2c09836bfec57c543ab65983f3398e9657a",
  "evaluation_time": 42.03318476676941,
  "kg_co2_emissions": 0.01015644489731042,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.22661207778915043,
        "f1": 0.22546498710191512,
        "f1_weighted": 0.22854897573280447,
        "hf_subset": "default",
        "languages": [
          "ind-Latn"
        ],
        "main_score": 0.22546498710191512,
        "scores_per_experiment": [
          {
            "accuracy": 0.21903787103377687,
            "f1": 0.221178369320145,
            "f1_weighted": 0.2201269546608793
          },
          {
            "accuracy": 0.20777891504605936,
            "f1": 0.21521565791213315,
            "f1_weighted": 0.2057075868696013
          },
          {
            "accuracy": 0.27021494370522003,
            "f1": 0.26558155341897544,
            "f1_weighted": 0.26473006553559236
          },
          {
            "accuracy": 0.20266120777891505,
            "f1": 0.18630976233238247,
            "f1_weighted": 0.19370641678657333
          },
          {
            "accuracy": 0.28249744114636643,
            "f1": 0.2847070135385765,
            "f1_weighted": 0.29265901276670986
          },
          {
            "accuracy": 0.18833162743091095,
            "f1": 0.18991255315729613,
            "f1_weighted": 0.19643946700722462
          },
          {
            "accuracy": 0.2507676560900716,
            "f1": 0.2556043915364255,
            "f1_weighted": 0.2600727341934016
          },
          {
            "accuracy": 0.218014329580348,
            "f1": 0.22487240192258084,
            "f1_weighted": 0.2233290524014953
          },
          {
            "accuracy": 0.2507676560900716,
            "f1": 0.23216595962402153,
            "f1_weighted": 0.24850069995000074
          },
          {
            "accuracy": 0.17604912998976457,
            "f1": 0.17910220825661438,
            "f1_weighted": 0.1802177671565665
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.23495934959349593,
        "f1": 0.23177075458173033,
        "f1_weighted": 0.23891870061724346,
        "hf_subset": "default",
        "languages": [
          "ind-Latn"
        ],
        "main_score": 0.23177075458173033,
        "scores_per_experiment": [
          {
            "accuracy": 0.22764227642276422,
            "f1": 0.22406927557415898,
            "f1_weighted": 0.23088987955855242
          },
          {
            "accuracy": 0.2459349593495935,
            "f1": 0.2530543569999772,
            "f1_weighted": 0.24968627803172513
          },
          {
            "accuracy": 0.2967479674796748,
            "f1": 0.28838441514497853,
            "f1_weighted": 0.2900478692989858
          },
          {
            "accuracy": 0.1991869918699187,
            "f1": 0.1848992170581376,
            "f1_weighted": 0.18208109940384046
          },
          {
            "accuracy": 0.29065040650406504,
            "f1": 0.29237202487307173,
            "f1_weighted": 0.3043129814588566
          },
          {
            "accuracy": 0.19308943089430894,
            "f1": 0.19359123789129354,
            "f1_weighted": 0.20240410734191314
          },
          {
            "accuracy": 0.2601626016260163,
            "f1": 0.26440810432040307,
            "f1_weighted": 0.2717551608923537
          },
          {
            "accuracy": 0.18495934959349594,
            "f1": 0.18760343566691573,
            "f1_weighted": 0.1986366795617854
          },
          {
            "accuracy": 0.2703252032520325,
            "f1": 0.24715956558061822,
            "f1_weighted": 0.2749184954255557
          },
          {
            "accuracy": 0.18089430894308944,
            "f1": 0.18216591270774898,
            "f1_weighted": 0.1844544551988661
          }
        ]
      }
    ]
  },
  "task_name": "IndonesianMongabayConservationClassification"
}