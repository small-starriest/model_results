{
  "dataset_revision": "358bcc95aeddd5d07a4524ee416f03d993099b23",
  "evaluation_time": 10.602832078933716,
  "kg_co2_emissions": 0.0015976561448303194,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.505517578125,
        "f1": 0.43430094358096083,
        "f1_weighted": 0.547187757531981,
        "hf_subset": "default",
        "languages": [
          "ron-Latn"
        ],
        "main_score": 0.505517578125,
        "scores_per_experiment": [
          {
            "accuracy": 0.49609375,
            "f1": 0.43875830155810924,
            "f1_weighted": 0.5414862020752023
          },
          {
            "accuracy": 0.46142578125,
            "f1": 0.4047551873202879,
            "f1_weighted": 0.5086854979893631
          },
          {
            "accuracy": 0.45703125,
            "f1": 0.40419058397903496,
            "f1_weighted": 0.5168175664319622
          },
          {
            "accuracy": 0.5390625,
            "f1": 0.4616214471613585,
            "f1_weighted": 0.5816199409780294
          },
          {
            "accuracy": 0.50830078125,
            "f1": 0.4315941144435041,
            "f1_weighted": 0.5547815493411012
          },
          {
            "accuracy": 0.5537109375,
            "f1": 0.4536590170129019,
            "f1_weighted": 0.5833363964298183
          },
          {
            "accuracy": 0.47802734375,
            "f1": 0.4119131090962883,
            "f1_weighted": 0.5132630135872271
          },
          {
            "accuracy": 0.52783203125,
            "f1": 0.45529515026770806,
            "f1_weighted": 0.572033869155378
          },
          {
            "accuracy": 0.57373046875,
            "f1": 0.48102963791094466,
            "f1_weighted": 0.6066431423210863
          },
          {
            "accuracy": 0.4599609375,
            "f1": 0.40019288705947104,
            "f1_weighted": 0.4932103970106422
          }
        ]
      }
    ]
  },
  "task_name": "RomanianReviewsSentiment"
}