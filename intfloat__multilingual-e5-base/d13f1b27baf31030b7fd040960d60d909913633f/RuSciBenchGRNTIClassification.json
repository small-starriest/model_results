{
  "dataset_revision": "673a610d6d3dd91a547a0d57ae1b56f37ebbf6a1",
  "evaluation_time": 19.45749855041504,
  "kg_co2_emissions": 0.0037563616489725933,
  "mteb_version": "1.14.12",
  "scores": {
    "test": [
      {
        "accuracy": 0.56279296875,
        "f1": 0.5487391938251276,
        "f1_weighted": 0.5488523841403546,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.56279296875,
        "scores_per_experiment": [
          {
            "accuracy": 0.57373046875,
            "f1": 0.5542489466824276,
            "f1_weighted": 0.5543881021816051
          },
          {
            "accuracy": 0.560546875,
            "f1": 0.5519263627623658,
            "f1_weighted": 0.5520122705718993
          },
          {
            "accuracy": 0.5439453125,
            "f1": 0.5322236842097618,
            "f1_weighted": 0.5324289135952875
          },
          {
            "accuracy": 0.57958984375,
            "f1": 0.5708143107660683,
            "f1_weighted": 0.5709590067384952
          },
          {
            "accuracy": 0.5693359375,
            "f1": 0.5597376336739195,
            "f1_weighted": 0.5597403321393681
          },
          {
            "accuracy": 0.54443359375,
            "f1": 0.5291953301489356,
            "f1_weighted": 0.5292727385475077
          },
          {
            "accuracy": 0.572265625,
            "f1": 0.554532254537572,
            "f1_weighted": 0.5546458352874326
          },
          {
            "accuracy": 0.5625,
            "f1": 0.5478121934723782,
            "f1_weighted": 0.5479516818490563
          },
          {
            "accuracy": 0.560546875,
            "f1": 0.5427477602532372,
            "f1_weighted": 0.5428478436143783
          },
          {
            "accuracy": 0.56103515625,
            "f1": 0.5441534617446097,
            "f1_weighted": 0.5442771168785163
          }
        ]
      }
    ]
  },
  "task_name": "RuSciBenchGRNTIClassification"
}