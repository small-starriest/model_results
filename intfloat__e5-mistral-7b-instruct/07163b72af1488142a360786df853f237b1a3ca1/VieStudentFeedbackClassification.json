{
  "dataset_revision": "7b56c6cb1c9c8523249f407044c838660df3811a",
  "evaluation_time": 31.487823009490967,
  "kg_co2_emissions": 0.007147376788608388,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.7314453125,
        "f1": 0.604559603604067,
        "f1_weighted": 0.7752041846813997,
        "hf_subset": "default",
        "languages": [
          "vie-Latn"
        ],
        "main_score": 0.7314453125,
        "scores_per_experiment": [
          {
            "accuracy": 0.78076171875,
            "f1": 0.6350328585504531,
            "f1_weighted": 0.7986274287460997
          },
          {
            "accuracy": 0.736328125,
            "f1": 0.6052660879198256,
            "f1_weighted": 0.7849503600053098
          },
          {
            "accuracy": 0.8056640625,
            "f1": 0.6498830821346034,
            "f1_weighted": 0.8209586577844732
          },
          {
            "accuracy": 0.669921875,
            "f1": 0.5713250069888822,
            "f1_weighted": 0.7404792046276878
          },
          {
            "accuracy": 0.71337890625,
            "f1": 0.6088605161954342,
            "f1_weighted": 0.7750427713367223
          },
          {
            "accuracy": 0.7861328125,
            "f1": 0.6278423314421091,
            "f1_weighted": 0.8049485508746955
          },
          {
            "accuracy": 0.72998046875,
            "f1": 0.6049232895731868,
            "f1_weighted": 0.7807902815209239
          },
          {
            "accuracy": 0.63623046875,
            "f1": 0.5132720333102013,
            "f1_weighted": 0.6733144724393867
          },
          {
            "accuracy": 0.7451171875,
            "f1": 0.6214847091803878,
            "f1_weighted": 0.7961090250909513
          },
          {
            "accuracy": 0.7109375,
            "f1": 0.6077061207455867,
            "f1_weighted": 0.7768210943877472
          }
        ]
      }
    ]
  },
  "task_name": "VieStudentFeedbackClassification"
}