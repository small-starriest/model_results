{
  "dataset_revision": "0ded8ff72cc68cbb7bb5c01b0a9157982b73ddaf",
  "evaluation_time": 10.390405178070068,
  "kg_co2_emissions": 0.0015578528044913385,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.45927734375,
        "f1": 0.44357907311843264,
        "f1_weighted": 0.44305759573104986,
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.45927734375,
        "scores_per_experiment": [
          {
            "accuracy": 0.48046875,
            "f1": 0.46148299991947866,
            "f1_weighted": 0.4608682962622317
          },
          {
            "accuracy": 0.486328125,
            "f1": 0.45426757450380495,
            "f1_weighted": 0.4546098851891652
          },
          {
            "accuracy": 0.43896484375,
            "f1": 0.42775650683551664,
            "f1_weighted": 0.42912235096602475
          },
          {
            "accuracy": 0.451171875,
            "f1": 0.4331920271291714,
            "f1_weighted": 0.4332745564830079
          },
          {
            "accuracy": 0.478515625,
            "f1": 0.4636395325399377,
            "f1_weighted": 0.46390125227917633
          },
          {
            "accuracy": 0.48583984375,
            "f1": 0.4684008126477051,
            "f1_weighted": 0.46836188515389293
          },
          {
            "accuracy": 0.43505859375,
            "f1": 0.42243443069633557,
            "f1_weighted": 0.418648197829319
          },
          {
            "accuracy": 0.41650390625,
            "f1": 0.40922282031135365,
            "f1_weighted": 0.4036997380171036
          },
          {
            "accuracy": 0.4755859375,
            "f1": 0.46148915936323176,
            "f1_weighted": 0.4638019519447513
          },
          {
            "accuracy": 0.4443359375,
            "f1": 0.4339048672377911,
            "f1_weighted": 0.4342878431858259
          }
        ]
      }
    ]
  },
  "task_name": "TweetEmotionClassification"
}