{
  "dataset_revision": "e7fc9f3d8d6c5640a26679d8a50b1666b02cc41f",
  "evaluation_time": 34.35564374923706,
  "kg_co2_emissions": 0.008423344088216106,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.6456347333647947,
        "ap": 0.6566674014233376,
        "ap_weighted": 0.6566674014233376,
        "f1": 0.6390351201632187,
        "f1_weighted": 0.6419142241495317,
        "hf_subset": "default",
        "languages": [
          "hrv-Latn"
        ],
        "main_score": 0.6456347333647947,
        "scores_per_experiment": [
          {
            "accuracy": 0.6465313827277017,
            "ap": 0.6755751971256291,
            "ap_weighted": 0.6755751971256291,
            "f1": 0.6447986317707329,
            "f1_weighted": 0.6417194864890925
          },
          {
            "accuracy": 0.6569136385087305,
            "ap": 0.6801910270405134,
            "ap_weighted": 0.6801910270405134,
            "f1": 0.6561782385538226,
            "f1_weighted": 0.6542046652054475
          },
          {
            "accuracy": 0.6790939122227466,
            "ap": 0.6567125462220271,
            "ap_weighted": 0.6567125462220271,
            "f1": 0.659443924482638,
            "f1_weighted": 0.6695970615664268
          },
          {
            "accuracy": 0.658329400660689,
            "ap": 0.6684393973401697,
            "ap_weighted": 0.6684393973401697,
            "f1": 0.6580460231274685,
            "f1_weighted": 0.6592677983936486
          },
          {
            "accuracy": 0.6408683341198679,
            "ap": 0.6600576822177477,
            "ap_weighted": 0.6600576822177477,
            "f1": 0.640842418110056,
            "f1_weighted": 0.64046375641114
          },
          {
            "accuracy": 0.6705993393109958,
            "ap": 0.6661387738663308,
            "ap_weighted": 0.6661387738663308,
            "f1": 0.6664265519649111,
            "f1_weighted": 0.6710571134502288
          },
          {
            "accuracy": 0.586125530910807,
            "ap": 0.5928279562889509,
            "ap_weighted": 0.5928279562889509,
            "f1": 0.5473767226100107,
            "f1_weighted": 0.5638137170988968
          },
          {
            "accuracy": 0.658329400660689,
            "ap": 0.6690252927828066,
            "ap_weighted": 0.6690252927828066,
            "f1": 0.6581155211358503,
            "f1_weighted": 0.6591768478345784
          },
          {
            "accuracy": 0.6370929683813119,
            "ap": 0.651354783233226,
            "ap_weighted": 0.651354783233226,
            "f1": 0.6367405627306582,
            "f1_weighted": 0.638144845853718
          },
          {
            "accuracy": 0.6224634261444077,
            "ap": 0.6463513581159731,
            "ap_weighted": 0.6463513581159731,
            "f1": 0.6223826071460394,
            "f1_weighted": 0.6216969491921401
          }
        ]
      }
    ]
  },
  "task_name": "FrenkHrClassification"
}