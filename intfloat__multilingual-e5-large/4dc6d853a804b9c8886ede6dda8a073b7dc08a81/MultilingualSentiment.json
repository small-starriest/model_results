{
  "dataset_revision": "46958b007a63fdbf239b7672c25d0bea67b5ea1a",
  "evaluation_time": 27.998555898666382,
  "kg_co2_emissions": 0.004561397681888554,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.7098,
        "f1": 0.7024774701009988,
        "f1_weighted": 0.7024774701009988,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.7098,
        "scores_per_experiment": [
          {
            "accuracy": 0.6886666666666666,
            "f1": 0.6774998223408494,
            "f1_weighted": 0.6774998223408494
          },
          {
            "accuracy": 0.692,
            "f1": 0.6805340529896303,
            "f1_weighted": 0.6805340529896303
          },
          {
            "accuracy": 0.7103333333333334,
            "f1": 0.7066260734590976,
            "f1_weighted": 0.7066260734590976
          },
          {
            "accuracy": 0.7093333333333334,
            "f1": 0.7043214659697323,
            "f1_weighted": 0.7043214659697323
          },
          {
            "accuracy": 0.7096666666666667,
            "f1": 0.7017261189432844,
            "f1_weighted": 0.7017261189432844
          },
          {
            "accuracy": 0.7296666666666667,
            "f1": 0.7269912672419726,
            "f1_weighted": 0.7269912672419726
          },
          {
            "accuracy": 0.71,
            "f1": 0.6960002418817429,
            "f1_weighted": 0.696000241881743
          },
          {
            "accuracy": 0.7253333333333334,
            "f1": 0.7208497031868765,
            "f1_weighted": 0.7208497031868764
          },
          {
            "accuracy": 0.7073333333333334,
            "f1": 0.6971001106643837,
            "f1_weighted": 0.6971001106643836
          },
          {
            "accuracy": 0.7156666666666667,
            "f1": 0.7131258443324177,
            "f1_weighted": 0.7131258443324178
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7081333333333333,
        "f1": 0.7005898849106014,
        "f1_weighted": 0.7005898849106014,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.7081333333333333,
        "scores_per_experiment": [
          {
            "accuracy": 0.6953333333333334,
            "f1": 0.6848870258515968,
            "f1_weighted": 0.6848870258515967
          },
          {
            "accuracy": 0.6873333333333334,
            "f1": 0.6750745961618901,
            "f1_weighted": 0.6750745961618901
          },
          {
            "accuracy": 0.7073333333333334,
            "f1": 0.702997951690547,
            "f1_weighted": 0.7029979516905469
          },
          {
            "accuracy": 0.7073333333333334,
            "f1": 0.7015853199203037,
            "f1_weighted": 0.7015853199203037
          },
          {
            "accuracy": 0.702,
            "f1": 0.6932636570214052,
            "f1_weighted": 0.6932636570214052
          },
          {
            "accuracy": 0.728,
            "f1": 0.7244845482927565,
            "f1_weighted": 0.7244845482927565
          },
          {
            "accuracy": 0.7043333333333334,
            "f1": 0.6898821985705906,
            "f1_weighted": 0.6898821985705907
          },
          {
            "accuracy": 0.7256666666666667,
            "f1": 0.7224630561816476,
            "f1_weighted": 0.7224630561816475
          },
          {
            "accuracy": 0.715,
            "f1": 0.7053500407617802,
            "f1_weighted": 0.7053500407617801
          },
          {
            "accuracy": 0.709,
            "f1": 0.7059104546534961,
            "f1_weighted": 0.7059104546534962
          }
        ]
      }
    ]
  },
  "task_name": "MultilingualSentiment"
}