{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 2064.819472551346,
  "kg_co2_emissions": 0.3861363678933358,
  "mteb_version": "1.12.75",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.05531534242910647,
        "map": 0.06962947735094112,
        "mrr": 0.05531534242910647,
        "nAUC_map_diff1": 0.08671412718258809,
        "nAUC_map_max": -0.1502461360846733,
        "nAUC_map_std": -0.09665719166952912,
        "nAUC_mrr_diff1": 0.07041815335350589,
        "nAUC_mrr_max": -0.1479249450465851,
        "nAUC_mrr_std": -0.097745936241823
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07569993348573495,
        "map": 0.09507145863849355,
        "mrr": 0.07569993348573495,
        "nAUC_map_diff1": 0.021599889174834956,
        "nAUC_map_max": -0.015516911282767575,
        "nAUC_map_std": 0.060711121009838206,
        "nAUC_mrr_diff1": 0.006431918616534706,
        "nAUC_mrr_max": -0.023655420552834424,
        "nAUC_mrr_std": 0.028303380751351956
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06272021325915624,
        "map": 0.08127090376399906,
        "mrr": 0.06272021325915624,
        "nAUC_map_diff1": 0.14229419096126883,
        "nAUC_map_max": 0.06280745483185385,
        "nAUC_map_std": 0.14037333965906032,
        "nAUC_mrr_diff1": 0.13564301686499183,
        "nAUC_mrr_max": 0.05882904223388701,
        "nAUC_mrr_std": 0.11807279327936228
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.05468865160258139,
        "map": 0.07268906962089348,
        "mrr": 0.05468865160258139,
        "nAUC_map_diff1": 0.05902610409147627,
        "nAUC_map_max": -0.040836366670392234,
        "nAUC_map_std": 0.11850310807658822,
        "nAUC_mrr_diff1": 0.05788045616806838,
        "nAUC_mrr_max": -0.046006101100427965,
        "nAUC_mrr_std": 0.0991750451756711
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06105406114948099,
        "map": 0.07533017526243106,
        "mrr": 0.06105406114948099,
        "nAUC_map_diff1": 0.045404065665069986,
        "nAUC_map_max": 0.19954223648722588,
        "nAUC_map_std": 0.3209210519782453,
        "nAUC_mrr_diff1": 0.02461445688744399,
        "nAUC_mrr_max": 0.15888145076185967,
        "nAUC_mrr_std": 0.29370788497879746
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0708786281614875,
        "map": 0.08833252833922427,
        "mrr": 0.0708786281614875,
        "nAUC_map_diff1": 0.14060605789380567,
        "nAUC_map_max": 0.1635157098276131,
        "nAUC_map_std": 0.1364902813738113,
        "nAUC_mrr_diff1": 0.1480833276628093,
        "nAUC_mrr_max": 0.17116253870900766,
        "nAUC_mrr_std": 0.12829003313090986
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}