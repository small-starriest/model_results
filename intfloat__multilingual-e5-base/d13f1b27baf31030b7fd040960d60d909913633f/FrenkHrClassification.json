{
  "dataset_revision": "e7fc9f3d8d6c5640a26679d8a50b1666b02cc41f",
  "evaluation_time": 10.16502332687378,
  "kg_co2_emissions": 0.0015328439141007527,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.6003303445021236,
        "ap": 0.6267970827937025,
        "ap_weighted": 0.6267970827937025,
        "f1": 0.5933717534921237,
        "f1_weighted": 0.5949113091852125,
        "hf_subset": "default",
        "languages": [
          "hrv-Latn"
        ],
        "main_score": 0.6003303445021236,
        "scores_per_experiment": [
          {
            "accuracy": 0.5521472392638037,
            "ap": 0.602175091295215,
            "ap_weighted": 0.602175091295215,
            "f1": 0.5498309065677918,
            "f1_weighted": 0.5458230414424552
          },
          {
            "accuracy": 0.6352052855120339,
            "ap": 0.643705508980397,
            "ap_weighted": 0.643705508980397,
            "f1": 0.6328421493384941,
            "f1_weighted": 0.6364980600069704
          },
          {
            "accuracy": 0.6470033034450212,
            "ap": 0.6484009815451938,
            "ap_weighted": 0.6484009815451938,
            "f1": 0.6424548391171747,
            "f1_weighted": 0.6474600530009557
          },
          {
            "accuracy": 0.655497876356772,
            "ap": 0.6558247994883637,
            "ap_weighted": 0.6558247994883637,
            "f1": 0.651778398091036,
            "f1_weighted": 0.6562451687937235
          },
          {
            "accuracy": 0.5785747994336952,
            "ap": 0.6214569498187746,
            "ap_weighted": 0.6214569498187746,
            "f1": 0.5759725541471854,
            "f1_weighted": 0.5718497197474739
          },
          {
            "accuracy": 0.6087777253421425,
            "ap": 0.6353703719987701,
            "ap_weighted": 0.6353703719987701,
            "f1": 0.6087355504638752,
            "f1_weighted": 0.6082313689645892
          },
          {
            "accuracy": 0.49598867390278434,
            "ap": 0.5492232634758528,
            "ap_weighted": 0.5492232634758528,
            "f1": 0.46408700050581686,
            "f1_weighted": 0.48031551134411943
          },
          {
            "accuracy": 0.6465313827277017,
            "ap": 0.6507570454983762,
            "ap_weighted": 0.6507570454983762,
            "f1": 0.6436054006728049,
            "f1_weighted": 0.6476133865084189
          },
          {
            "accuracy": 0.6451156205757432,
            "ap": 0.6547634486013041,
            "ap_weighted": 0.6547634486013041,
            "f1": 0.6441741025572318,
            "f1_weighted": 0.6464458386936401
          },
          {
            "accuracy": 0.5384615384615384,
            "ap": 0.6062933672347777,
            "ap_weighted": 0.6062933672347777,
            "f1": 0.5202366334598267,
            "f1_weighted": 0.5086309433497779
          }
        ]
      }
    ]
  },
  "task_name": "FrenkHrClassification"
}