{
  "dataset_revision": "e7fc9f3d8d6c5640a26679d8a50b1666b02cc41f",
  "evaluation_time": 10.218534231185913,
  "kg_co2_emissions": 0.0015204431063295656,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.6154318074563474,
        "ap": 0.6362761566957551,
        "ap_weighted": 0.6362761566957551,
        "f1": 0.6113823021098184,
        "f1_weighted": 0.6124993826397198,
        "hf_subset": "default",
        "languages": [
          "hrv-Latn"
        ],
        "main_score": 0.6154318074563474,
        "scores_per_experiment": [
          {
            "accuracy": 0.5431807456347334,
            "ap": 0.600449792067481,
            "ap_weighted": 0.600449792067481,
            "f1": 0.5370931720976586,
            "f1_weighted": 0.5305045637015652
          },
          {
            "accuracy": 0.6521944313355357,
            "ap": 0.64976038589152,
            "ap_weighted": 0.64976038589152,
            "f1": 0.6459234799871311,
            "f1_weighted": 0.6517719204290828
          },
          {
            "accuracy": 0.6625766871165644,
            "ap": 0.6486985056613257,
            "ap_weighted": 0.6486985056613257,
            "f1": 0.6473726312741748,
            "f1_weighted": 0.6564605101072395
          },
          {
            "accuracy": 0.6234072675790467,
            "ap": 0.6546909843575496,
            "ap_weighted": 0.6546909843575496,
            "f1": 0.6218264916160336,
            "f1_weighted": 0.6187918633074758
          },
          {
            "accuracy": 0.5927324209532798,
            "ap": 0.6243311539006212,
            "ap_weighted": 0.6243311539006212,
            "f1": 0.5926275421836242,
            "f1_weighted": 0.5918162740536409
          },
          {
            "accuracy": 0.6403964134025484,
            "ap": 0.6451744211994761,
            "ap_weighted": 0.6451744211994761,
            "f1": 0.636795152104084,
            "f1_weighted": 0.6412839280353453
          },
          {
            "accuracy": 0.558282208588957,
            "ap": 0.5944693128479501,
            "ap_weighted": 0.5944693128479501,
            "f1": 0.5567397379303687,
            "f1_weighted": 0.5599850961960386
          },
          {
            "accuracy": 0.6630486078338839,
            "ap": 0.6617629587323917,
            "ap_weighted": 0.6617629587323917,
            "f1": 0.6595437017994858,
            "f1_weighted": 0.6638310984834239
          },
          {
            "accuracy": 0.6144407739499764,
            "ap": 0.6433504797748484,
            "ap_weighted": 0.6433504797748484,
            "f1": 0.6138904370683451,
            "f1_weighted": 0.6120812045699822
          },
          {
            "accuracy": 0.6040585181689476,
            "ap": 0.6400735725243883,
            "ap_weighted": 0.6400735725243883,
            "f1": 0.6020106750372785,
            "f1_weighted": 0.5984673675134036
          }
        ]
      }
    ]
  },
  "task_name": "FrenkHrClassification"
}