{
  "dataset_revision": "0ded8ff72cc68cbb7bb5c01b0a9157982b73ddaf",
  "evaluation_time": 37.22665572166443,
  "kg_co2_emissions": 0.008240009753224942,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.534423828125,
        "f1": 0.46141356981469767,
        "f1_weighted": 0.4684394043298547,
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.534423828125,
        "scores_per_experiment": [
          {
            "accuracy": 0.5458984375,
            "f1": 0.47799026258028743,
            "f1_weighted": 0.4830048693127195
          },
          {
            "accuracy": 0.55078125,
            "f1": 0.484073181292904,
            "f1_weighted": 0.488323563994027
          },
          {
            "accuracy": 0.54833984375,
            "f1": 0.4857492515286271,
            "f1_weighted": 0.4920637457643506
          },
          {
            "accuracy": 0.5498046875,
            "f1": 0.48053351092703467,
            "f1_weighted": 0.48647852884431797
          },
          {
            "accuracy": 0.5302734375,
            "f1": 0.45185166856346165,
            "f1_weighted": 0.4622560516106524
          },
          {
            "accuracy": 0.54296875,
            "f1": 0.4742236847777495,
            "f1_weighted": 0.4805428465743183
          },
          {
            "accuracy": 0.484375,
            "f1": 0.38600576268869435,
            "f1_weighted": 0.39475620978029924
          },
          {
            "accuracy": 0.5322265625,
            "f1": 0.472514187975802,
            "f1_weighted": 0.4775660138875056
          },
          {
            "accuracy": 0.53759765625,
            "f1": 0.4591521937919365,
            "f1_weighted": 0.4662747534682033
          },
          {
            "accuracy": 0.52197265625,
            "f1": 0.44204199402047917,
            "f1_weighted": 0.4531274600621533
          }
        ]
      }
    ]
  },
  "task_name": "TweetEmotionClassification"
}