{
  "dataset_revision": "e7fc9f3d8d6c5640a26679d8a50b1666b02cc41f",
  "evaluation_time": 10.023736715316772,
  "kg_co2_emissions": 0.0014895100795063701,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.5753185464841907,
        "ap": 0.6062944123411911,
        "ap_weighted": 0.6062944123411911,
        "f1": 0.5691145083309243,
        "f1_weighted": 0.5725810140775286,
        "hf_subset": "default",
        "languages": [
          "hrv-Latn"
        ],
        "main_score": 0.5753185464841907,
        "scores_per_experiment": [
          {
            "accuracy": 0.5068428504011326,
            "ap": 0.5749876842644566,
            "ap_weighted": 0.5749876842644566,
            "f1": 0.5037233829164329,
            "f1_weighted": 0.4988399308421709
          },
          {
            "accuracy": 0.6040585181689476,
            "ap": 0.6256991624570735,
            "ap_weighted": 0.6256991624570735,
            "f1": 0.6033107548323771,
            "f1_weighted": 0.6054483826314863
          },
          {
            "accuracy": 0.610193487494101,
            "ap": 0.6155826547114109,
            "ap_weighted": 0.6155826547114109,
            "f1": 0.5957153020307852,
            "f1_weighted": 0.6052109698034835
          },
          {
            "accuracy": 0.6139688532326569,
            "ap": 0.6194487274087342,
            "ap_weighted": 0.6194487274087342,
            "f1": 0.6020311920269491,
            "f1_weighted": 0.6105859737629196
          },
          {
            "accuracy": 0.5781028787163757,
            "ap": 0.614826701325386,
            "ap_weighted": 0.614826701325386,
            "f1": 0.5778771592063747,
            "f1_weighted": 0.5766656442853489
          },
          {
            "accuracy": 0.6007550731477111,
            "ap": 0.6058185329250123,
            "ap_weighted": 0.6058185329250123,
            "f1": 0.5773151684916391,
            "f1_weighted": 0.5896692665368234
          },
          {
            "accuracy": 0.4200094384143464,
            "ap": 0.5325949184193628,
            "ap_weighted": 0.5325949184193628,
            "f1": 0.4196599544017915,
            "f1_weighted": 0.4178923717998311
          },
          {
            "accuracy": 0.6243511090136857,
            "ap": 0.6304658635703317,
            "ap_weighted": 0.6304658635703317,
            "f1": 0.6178203266754762,
            "f1_weighted": 0.6240210333720434
          },
          {
            "accuracy": 0.5946201038225578,
            "ap": 0.6189178655421955,
            "ap_weighted": 0.6189178655421955,
            "f1": 0.5937511787038978,
            "f1_weighted": 0.5960830899917303
          },
          {
            "accuracy": 0.6002831524303917,
            "ap": 0.6246020127879472,
            "ap_weighted": 0.6246020127879472,
            "f1": 0.5999406640235186,
            "f1_weighted": 0.601393477749448
          }
        ]
      }
    ]
  },
  "task_name": "FrenkHrClassification"
}