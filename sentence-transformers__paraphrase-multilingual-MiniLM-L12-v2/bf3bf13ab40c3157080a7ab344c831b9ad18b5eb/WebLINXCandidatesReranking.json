{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 1198.3707253932953,
  "kg_co2_emissions": 0.19781478328000873,
  "mteb_version": "1.12.75",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06708612002853577,
        "map": 0.08106054836243831,
        "mrr": 0.06708612002853577,
        "nAUC_map_diff1": 0.1511274873939875,
        "nAUC_map_max": -0.06479604849691775,
        "nAUC_map_std": -0.11574919838083143,
        "nAUC_mrr_diff1": 0.1389933344823611,
        "nAUC_mrr_max": -0.07016773885295778,
        "nAUC_mrr_std": -0.12027589156015142
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0705964973458871,
        "map": 0.08888323305918808,
        "mrr": 0.0705964973458871,
        "nAUC_map_diff1": 0.0946554645141619,
        "nAUC_map_max": -0.023876658118764216,
        "nAUC_map_std": 0.09463360541182543,
        "nAUC_mrr_diff1": 0.0770559387192119,
        "nAUC_mrr_max": -0.04674556230484657,
        "nAUC_mrr_std": 0.06691507562726107
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07507643994083493,
        "map": 0.08902255398542558,
        "mrr": 0.07507643994083493,
        "nAUC_map_diff1": 0.10649672940641913,
        "nAUC_map_max": 0.02068391860842803,
        "nAUC_map_std": 0.1360824163928118,
        "nAUC_mrr_diff1": 0.09387455528205314,
        "nAUC_mrr_max": 0.014978859519340892,
        "nAUC_mrr_std": 0.1207093449633163
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.055838456560427116,
        "map": 0.07226928238751383,
        "mrr": 0.055838456560427116,
        "nAUC_map_diff1": 0.0839221641514376,
        "nAUC_map_max": -0.0051232432159589395,
        "nAUC_map_std": 0.18864167182269326,
        "nAUC_mrr_diff1": 0.0777363934942771,
        "nAUC_mrr_max": -0.014607783785561374,
        "nAUC_mrr_std": 0.17372815286625934
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.048601771073145124,
        "map": 0.0646435435514456,
        "mrr": 0.048601771073145124,
        "nAUC_map_diff1": 0.10103639037024263,
        "nAUC_map_max": 0.1298788859329458,
        "nAUC_map_std": 0.23186617265159742,
        "nAUC_mrr_diff1": 0.10313655837478831,
        "nAUC_mrr_max": 0.13547698284200516,
        "nAUC_mrr_std": 0.2278814092407324
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08154197625757964,
        "map": 0.09610543419225534,
        "mrr": 0.08154197625757964,
        "nAUC_map_diff1": 0.06270763590023176,
        "nAUC_map_max": -0.010658820060867715,
        "nAUC_map_std": 0.1937140690161118,
        "nAUC_mrr_diff1": 0.045269896011642864,
        "nAUC_mrr_max": -0.019961154552531286,
        "nAUC_mrr_std": 0.20338362423783568
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}