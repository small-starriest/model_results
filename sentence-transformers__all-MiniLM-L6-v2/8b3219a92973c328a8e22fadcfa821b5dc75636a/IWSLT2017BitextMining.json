{
  "dataset_revision": "c18a4f81a47ae6fa079fe9d32db288ddde38451d",
  "evaluation_time": 15.948943853378296,
  "kg_co2_emissions": 0.0024931660286673,
  "mteb_version": "1.12.75",
  "scores": {
    "validation": [
      {
        "accuracy": 0.01126126126126126,
        "f1": 0.008690806190806189,
        "hf_subset": "ar-en",
        "languages": [
          "ara-Arab",
          "eng-Latn"
        ],
        "main_score": 0.008690806190806189,
        "precision": 0.008474916678041679,
        "recall": 0.01126126126126126
      },
      {
        "accuracy": 0.30405405405405406,
        "f1": 0.24180534065402484,
        "hf_subset": "de-en",
        "languages": [
          "deu-Latn",
          "eng-Latn"
        ],
        "main_score": 0.24180534065402484,
        "precision": 0.2225535066925227,
        "recall": 0.30405405405405406
      },
      {
        "accuracy": 0.01463963963963964,
        "f1": 0.004613174598002219,
        "hf_subset": "en-ar",
        "languages": [
          "eng-Latn",
          "ara-Arab"
        ],
        "main_score": 0.004613174598002219,
        "precision": 0.003664194730222956,
        "recall": 0.01463963963963964
      },
      {
        "accuracy": 0.28716216216216217,
        "f1": 0.21331834811001477,
        "hf_subset": "en-de",
        "languages": [
          "eng-Latn",
          "deu-Latn"
        ],
        "main_score": 0.21331834811001477,
        "precision": 0.19137066428733096,
        "recall": 0.28716216216216217
      },
      {
        "accuracy": 0.37865168539325844,
        "f1": 0.2919482138302363,
        "hf_subset": "en-fr",
        "languages": [
          "eng-Latn",
          "fra-Latn"
        ],
        "main_score": 0.2919482138302363,
        "precision": 0.2654317082638299,
        "recall": 0.37865168539325844
      },
      {
        "accuracy": 0.24219590958019377,
        "f1": 0.17445630241750704,
        "hf_subset": "en-it",
        "languages": [
          "eng-Latn",
          "ita-Latn"
        ],
        "main_score": 0.17445630241750704,
        "precision": 0.15766340012093422,
        "recall": 0.24219590958019377
      },
      {
        "accuracy": 0.05166475315729047,
        "f1": 0.023975612165172192,
        "hf_subset": "en-ja",
        "languages": [
          "eng-Latn",
          "jpn-Jpan"
        ],
        "main_score": 0.023975612165172192,
        "precision": 0.01945854403234314,
        "recall": 0.05166475315729047
      },
      {
        "accuracy": 0.04323094425483504,
        "f1": 0.024521851893242488,
        "hf_subset": "en-ko",
        "languages": [
          "eng-Latn",
          "kor-Hang"
        ],
        "main_score": 0.024521851893242488,
        "precision": 0.020783673727359734,
        "recall": 0.04323094425483504
      },
      {
        "accuracy": 0.2123629112662014,
        "f1": 0.14574749664629114,
        "hf_subset": "en-nl",
        "languages": [
          "eng-Latn",
          "nld-Latn"
        ],
        "main_score": 0.14574749664629114,
        "precision": 0.12852377657772293,
        "recall": 0.2123629112662014
      },
      {
        "accuracy": 0.22975929978118162,
        "f1": 0.15731043911633436,
        "hf_subset": "en-ro",
        "languages": [
          "eng-Latn",
          "ron-Latn"
        ],
        "main_score": 0.15731043911633436,
        "precision": 0.13940799981932858,
        "recall": 0.22975929978118162
      },
      {
        "accuracy": 0.06598407281001138,
        "f1": 0.03437519807203098,
        "hf_subset": "en-zh",
        "languages": [
          "eng-Latn",
          "cmn-Hans"
        ],
        "main_score": 0.03437519807203098,
        "precision": 0.027500634241248576,
        "recall": 0.06598407281001138
      },
      {
        "accuracy": 0.4191011235955056,
        "f1": 0.3583354887750129,
        "hf_subset": "fr-en",
        "languages": [
          "fra-Latn",
          "eng-Latn"
        ],
        "main_score": 0.3583354887750129,
        "precision": 0.33880443469074456,
        "recall": 0.4191011235955056
      },
      {
        "accuracy": 0.26695371367061355,
        "f1": 0.21871124203136716,
        "hf_subset": "it-en",
        "languages": [
          "ita-Latn",
          "eng-Latn"
        ],
        "main_score": 0.21871124203136716,
        "precision": 0.20446325737473645,
        "recall": 0.26695371367061355
      },
      {
        "accuracy": 0.13986013986013987,
        "f1": 0.10836745802368415,
        "hf_subset": "it-nl",
        "languages": [
          "ita-Latn",
          "nld-Latn"
        ],
        "main_score": 0.10836745802368415,
        "precision": 0.10154028332753443,
        "recall": 0.13986013986013987
      },
      {
        "accuracy": 0.3150984682713348,
        "f1": 0.2630888363125459,
        "hf_subset": "it-ro",
        "languages": [
          "ita-Latn",
          "ron-Latn"
        ],
        "main_score": 0.2630888363125459,
        "precision": 0.24628925552481046,
        "recall": 0.3150984682713348
      },
      {
        "accuracy": 0.06314580941446613,
        "f1": 0.03872791498045124,
        "hf_subset": "ja-en",
        "languages": [
          "jpn-Jpan",
          "eng-Latn"
        ],
        "main_score": 0.03872791498045124,
        "precision": 0.03407426299217344,
        "recall": 0.06314580941446613
      },
      {
        "accuracy": 0.03299203640500569,
        "f1": 0.025364482320059493,
        "hf_subset": "ko-en",
        "languages": [
          "kor-Hang",
          "eng-Latn"
        ],
        "main_score": 0.025364482320059493,
        "precision": 0.023539764495395896,
        "recall": 0.03299203640500569
      },
      {
        "accuracy": 0.226321036889332,
        "f1": 0.18464926777645396,
        "hf_subset": "nl-en",
        "languages": [
          "nld-Latn",
          "eng-Latn"
        ],
        "main_score": 0.18464926777645396,
        "precision": 0.17352003051267406,
        "recall": 0.226321036889332
      },
      {
        "accuracy": 0.12087912087912088,
        "f1": 0.0996584313117625,
        "hf_subset": "nl-it",
        "languages": [
          "nld-Latn",
          "ita-Latn"
        ],
        "main_score": 0.0996584313117625,
        "precision": 0.09371461871461872,
        "recall": 0.12087912087912088
      },
      {
        "accuracy": 0.140197152245345,
        "f1": 0.11164137067751524,
        "hf_subset": "nl-ro",
        "languages": [
          "nld-Latn",
          "ron-Latn"
        ],
        "main_score": 0.11164137067751524,
        "precision": 0.10311796637097842,
        "recall": 0.140197152245345
      },
      {
        "accuracy": 0.22319474835886213,
        "f1": 0.18401851528430802,
        "hf_subset": "ro-en",
        "languages": [
          "ron-Latn",
          "eng-Latn"
        ],
        "main_score": 0.18401851528430802,
        "precision": 0.1728202240329376,
        "recall": 0.22319474835886213
      },
      {
        "accuracy": 0.30306345733041573,
        "f1": 0.24505333433361215,
        "hf_subset": "ro-it",
        "languages": [
          "ron-Latn",
          "ita-Latn"
        ],
        "main_score": 0.24505333433361215,
        "precision": 0.2278814987303853,
        "recall": 0.30306345733041573
      },
      {
        "accuracy": 0.1588170865279299,
        "f1": 0.11957956145158334,
        "hf_subset": "ro-nl",
        "languages": [
          "ron-Latn",
          "nld-Latn"
        ],
        "main_score": 0.11957956145158334,
        "precision": 0.11015069235206988,
        "recall": 0.1588170865279299
      },
      {
        "accuracy": 0.05915813424345848,
        "f1": 0.0365780772251943,
        "hf_subset": "zh-en",
        "languages": [
          "cmn-Hans",
          "eng-Latn"
        ],
        "main_score": 0.0365780772251943,
        "precision": 0.032152640445639834,
        "recall": 0.05915813424345848
      }
    ]
  },
  "task_name": "IWSLT2017BitextMining"
}