{
  "dataset_revision": "ba52e9d114a4a145d79b4293afab31304a999a4c",
  "evaluation_time": 47.962666749954224,
  "kg_co2_emissions": 0.007554495261567613,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.888,
        "f1": 0.8608285714285714,
        "hf_subset": "ind-abs",
        "languages": [
          "ind-Latn",
          "abs-Latn"
        ],
        "main_score": 0.8608285714285714,
        "precision": 0.8495769230769231,
        "recall": 0.888
      },
      {
        "accuracy": 0.6165151515151515,
        "f1": 0.5660063395208498,
        "hf_subset": "ind-btk",
        "languages": [
          "ind-Latn",
          "bbc-Latn"
        ],
        "main_score": 0.5660063395208498,
        "precision": 0.549377364906455,
        "recall": 0.6165151515151515
      },
      {
        "accuracy": 0.7293939393939394,
        "f1": 0.6880499474391325,
        "hf_subset": "ind-bew",
        "languages": [
          "ind-Latn",
          "bew-Latn"
        ],
        "main_score": 0.6880499474391325,
        "precision": 0.6733168249975554,
        "recall": 0.7293939393939394
      },
      {
        "accuracy": 0.706,
        "f1": 0.6670198709191698,
        "hf_subset": "ind-bhp",
        "languages": [
          "ind-Latn",
          "bhp-Latn"
        ],
        "main_score": 0.6670198709191698,
        "precision": 0.654038995215311,
        "recall": 0.706
      },
      {
        "accuracy": 0.7962121212121213,
        "f1": 0.7608736143774929,
        "hf_subset": "ind-jav",
        "languages": [
          "ind-Latn",
          "jav-Latn"
        ],
        "main_score": 0.7608736143774929,
        "precision": 0.7479431425607648,
        "recall": 0.7962121212121213
      },
      {
        "accuracy": 0.5819696969696969,
        "f1": 0.5340654282884786,
        "hf_subset": "ind-mad",
        "languages": [
          "ind-Latn",
          "mad-Latn"
        ],
        "main_score": 0.5340654282884786,
        "precision": 0.5178422253746555,
        "recall": 0.5819696969696969
      },
      {
        "accuracy": 0.47075757575757576,
        "f1": 0.42040520610632837,
        "hf_subset": "ind-mak",
        "languages": [
          "ind-Latn",
          "mak-Latn"
        ],
        "main_score": 0.42040520610632837,
        "precision": 0.40524867619021654,
        "recall": 0.47075757575757576
      },
      {
        "accuracy": 0.8295454545454546,
        "f1": 0.7979068784258153,
        "hf_subset": "ind-min",
        "languages": [
          "ind-Latn",
          "min-Latn"
        ],
        "main_score": 0.7979068784258153,
        "precision": 0.7861372295351001,
        "recall": 0.8295454545454546
      },
      {
        "accuracy": 0.913,
        "f1": 0.8925055555555556,
        "hf_subset": "ind-mui",
        "languages": [
          "ind-Latn",
          "mui-Latn"
        ],
        "main_score": 0.8925055555555556,
        "precision": 0.8846178571428571,
        "recall": 0.913
      },
      {
        "accuracy": 0.594,
        "f1": 0.544186290404725,
        "hf_subset": "ind-rej",
        "languages": [
          "ind-Latn",
          "rej-Latn"
        ],
        "main_score": 0.544186290404725,
        "precision": 0.5284041898853108,
        "recall": 0.594
      },
      {
        "accuracy": 0.8012121212121213,
        "f1": 0.7701653716135173,
        "hf_subset": "ind-sun",
        "languages": [
          "ind-Latn",
          "sun-Latn"
        ],
        "main_score": 0.7701653716135173,
        "precision": 0.7589581734305018,
        "recall": 0.8012121212121213
      }
    ]
  },
  "task_name": "NusaTranslationBitextMining"
}