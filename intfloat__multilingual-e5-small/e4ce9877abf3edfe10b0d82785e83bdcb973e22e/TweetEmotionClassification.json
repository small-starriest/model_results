{
  "dataset_revision": "0ded8ff72cc68cbb7bb5c01b0a9157982b73ddaf",
  "evaluation_time": 9.929121017456055,
  "kg_co2_emissions": 0.001475255802281512,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.485302734375,
        "f1": 0.43040143841555195,
        "f1_weighted": 0.43423851908638256,
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.485302734375,
        "scores_per_experiment": [
          {
            "accuracy": 0.49560546875,
            "f1": 0.4338913329381159,
            "f1_weighted": 0.4372757817472058
          },
          {
            "accuracy": 0.5107421875,
            "f1": 0.46153240396344775,
            "f1_weighted": 0.46498979954404746
          },
          {
            "accuracy": 0.4775390625,
            "f1": 0.43232252966473617,
            "f1_weighted": 0.4366842349191673
          },
          {
            "accuracy": 0.470703125,
            "f1": 0.4194062567630532,
            "f1_weighted": 0.42378284405516625
          },
          {
            "accuracy": 0.48046875,
            "f1": 0.43265154974433334,
            "f1_weighted": 0.4351171756042301
          },
          {
            "accuracy": 0.4951171875,
            "f1": 0.43699244660842856,
            "f1_weighted": 0.4404777391848227
          },
          {
            "accuracy": 0.474609375,
            "f1": 0.4157442915718097,
            "f1_weighted": 0.4182358473168799
          },
          {
            "accuracy": 0.4482421875,
            "f1": 0.39996520894205395,
            "f1_weighted": 0.3993837813859953
          },
          {
            "accuracy": 0.51708984375,
            "f1": 0.44918308280155317,
            "f1_weighted": 0.45906793449228056
          },
          {
            "accuracy": 0.48291015625,
            "f1": 0.4223252811579875,
            "f1_weighted": 0.4273700526140297
          }
        ]
      }
    ]
  },
  "task_name": "TweetEmotionClassification"
}