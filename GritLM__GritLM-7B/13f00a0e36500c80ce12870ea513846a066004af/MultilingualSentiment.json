{
  "dataset_revision": "46958b007a63fdbf239b7672c25d0bea67b5ea1a",
  "evaluation_time": 60.37501335144043,
  "kg_co2_emissions": 0.01823961053201652,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.7623666666666666,
        "f1": 0.7588557383799758,
        "f1_weighted": 0.7588557383799757,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.7623666666666666,
        "scores_per_experiment": [
          {
            "accuracy": 0.751,
            "f1": 0.7470120250941106,
            "f1_weighted": 0.7470120250941106
          },
          {
            "accuracy": 0.757,
            "f1": 0.7473939641018476,
            "f1_weighted": 0.7473939641018476
          },
          {
            "accuracy": 0.773,
            "f1": 0.7682850076858673,
            "f1_weighted": 0.7682850076858672
          },
          {
            "accuracy": 0.7653333333333333,
            "f1": 0.7632164806042535,
            "f1_weighted": 0.7632164806042533
          },
          {
            "accuracy": 0.7636666666666667,
            "f1": 0.7606686555946855,
            "f1_weighted": 0.7606686555946854
          },
          {
            "accuracy": 0.7596666666666667,
            "f1": 0.7583165463784662,
            "f1_weighted": 0.7583165463784662
          },
          {
            "accuracy": 0.7496666666666667,
            "f1": 0.7466298726928464,
            "f1_weighted": 0.7466298726928465
          },
          {
            "accuracy": 0.7673333333333333,
            "f1": 0.7630625636856059,
            "f1_weighted": 0.7630625636856059
          },
          {
            "accuracy": 0.769,
            "f1": 0.765544402736006,
            "f1_weighted": 0.7655444027360059
          },
          {
            "accuracy": 0.768,
            "f1": 0.768427865226069,
            "f1_weighted": 0.768427865226069
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7551333333333333,
        "f1": 0.752602785793475,
        "f1_weighted": 0.752602785793475,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.7551333333333333,
        "scores_per_experiment": [
          {
            "accuracy": 0.7443333333333333,
            "f1": 0.742207024829773,
            "f1_weighted": 0.742207024829773
          },
          {
            "accuracy": 0.748,
            "f1": 0.7393236888155746,
            "f1_weighted": 0.7393236888155746
          },
          {
            "accuracy": 0.7676666666666667,
            "f1": 0.7642494954549622,
            "f1_weighted": 0.7642494954549621
          },
          {
            "accuracy": 0.7546666666666667,
            "f1": 0.7531087306194776,
            "f1_weighted": 0.7531087306194776
          },
          {
            "accuracy": 0.7593333333333333,
            "f1": 0.7563739109677782,
            "f1_weighted": 0.7563739109677781
          },
          {
            "accuracy": 0.756,
            "f1": 0.7558727977948766,
            "f1_weighted": 0.7558727977948765
          },
          {
            "accuracy": 0.7383333333333333,
            "f1": 0.7365387613292295,
            "f1_weighted": 0.7365387613292295
          },
          {
            "accuracy": 0.7586666666666667,
            "f1": 0.7553023419252107,
            "f1_weighted": 0.7553023419252107
          },
          {
            "accuracy": 0.768,
            "f1": 0.7649889828681781,
            "f1_weighted": 0.7649889828681781
          },
          {
            "accuracy": 0.7563333333333333,
            "f1": 0.7580621233296899,
            "f1_weighted": 0.75806212332969
          }
        ]
      }
    ]
  },
  "task_name": "MultilingualSentiment"
}