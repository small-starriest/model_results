{
  "dataset_revision": "12ca3b695563788fead87a982ad1a068284413f4",
  "evaluation_time": 12.734838724136353,
  "kg_co2_emissions": 0.0020294481170483655,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.30791015625,
        "f1": 0.16378088791901146,
        "f1_weighted": 0.31302836608462276,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.30791015625,
        "scores_per_experiment": [
          {
            "accuracy": 0.3203125,
            "f1": 0.15590891446271618,
            "f1_weighted": 0.32227557669328977
          },
          {
            "accuracy": 0.2939453125,
            "f1": 0.170876199975657,
            "f1_weighted": 0.294378157511506
          },
          {
            "accuracy": 0.24267578125,
            "f1": 0.13387115874981861,
            "f1_weighted": 0.2304720369739665
          },
          {
            "accuracy": 0.27099609375,
            "f1": 0.15029974274273977,
            "f1_weighted": 0.2675359550927536
          },
          {
            "accuracy": 0.26611328125,
            "f1": 0.16557821833918585,
            "f1_weighted": 0.2802527123453651
          },
          {
            "accuracy": 0.34765625,
            "f1": 0.17046248003013428,
            "f1_weighted": 0.3577022092096728
          },
          {
            "accuracy": 0.36767578125,
            "f1": 0.19713458626294109,
            "f1_weighted": 0.399740058112228
          },
          {
            "accuracy": 0.28466796875,
            "f1": 0.16306876742794468,
            "f1_weighted": 0.2822366026003211
          },
          {
            "accuracy": 0.33642578125,
            "f1": 0.17640014106898408,
            "f1_weighted": 0.3578036416842989
          },
          {
            "accuracy": 0.3486328125,
            "f1": 0.15420867012999287,
            "f1_weighted": 0.3378867106228259
          }
        ]
      }
    ]
  },
  "task_name": "MAUDLegalBenchClassification"
}