{
  "dataset_revision": "e7fc9f3d8d6c5640a26679d8a50b1666b02cc41f",
  "evaluation_time": 12.293050050735474,
  "kg_co2_emissions": 0.0019313542538015654,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.6132609721566776,
        "ap": 0.6365788199937201,
        "ap_weighted": 0.6365788199937201,
        "f1": 0.6073125786886682,
        "f1_weighted": 0.6086972554551522,
        "hf_subset": "default",
        "languages": [
          "hrv-Latn"
        ],
        "main_score": 0.6132609721566776,
        "scores_per_experiment": [
          {
            "accuracy": 0.5677206229353469,
            "ap": 0.6151776050731557,
            "ap_weighted": 0.6151776050731557,
            "f1": 0.5640285921184798,
            "f1_weighted": 0.5590490838885513
          },
          {
            "accuracy": 0.6559697970740915,
            "ap": 0.6686582793259498,
            "ap_weighted": 0.6686582793259498,
            "f1": 0.6558913216504643,
            "f1_weighted": 0.6565362915384013
          },
          {
            "accuracy": 0.6616328456819255,
            "ap": 0.6574782059473864,
            "ap_weighted": 0.6574782059473864,
            "f1": 0.656132386461241,
            "f1_weighted": 0.6615302251740769
          },
          {
            "accuracy": 0.656441717791411,
            "ap": 0.6565307040327242,
            "ap_weighted": 0.6565307040327242,
            "f1": 0.6527324298770879,
            "f1_weighted": 0.657186962851823
          },
          {
            "accuracy": 0.5955639452571968,
            "ap": 0.6325245557494334,
            "ap_weighted": 0.6325245557494334,
            "f1": 0.5938413152686455,
            "f1_weighted": 0.5905583320295946
          },
          {
            "accuracy": 0.6526663520528552,
            "ap": 0.661134735449767,
            "ap_weighted": 0.661134735449767,
            "f1": 0.6518754631737783,
            "f1_weighted": 0.6539349064925624
          },
          {
            "accuracy": 0.49551675318546484,
            "ap": 0.5504669348799738,
            "ap_weighted": 0.5504669348799738,
            "f1": 0.47010934848234975,
            "f1_weighted": 0.4845105283032964
          },
          {
            "accuracy": 0.6715431807456347,
            "ap": 0.6669905425235908,
            "ap_weighted": 0.6669905425235908,
            "f1": 0.667453149267543,
            "f1_weighted": 0.6720305036451519
          },
          {
            "accuracy": 0.6328456819254366,
            "ap": 0.6499511019446167,
            "ap_weighted": 0.6499511019446167,
            "f1": 0.6327337065295073,
            "f1_weighted": 0.6335296397492207
          },
          {
            "accuracy": 0.5427088249174139,
            "ap": 0.606875535010603,
            "ap_weighted": 0.606875535010603,
            "f1": 0.5283280740575858,
            "f1_weighted": 0.5181060808788431
          }
        ]
      }
    ]
  },
  "task_name": "FrenkHrClassification"
}