{
  "dataset_revision": "12ca3b695563788fead87a982ad1a068284413f4",
  "evaluation_time": 29.142988443374634,
  "kg_co2_emissions": 0.005567002740445485,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.31728515625,
        "f1": 0.15988570845873873,
        "f1_weighted": 0.3219501407647919,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.31728515625,
        "scores_per_experiment": [
          {
            "accuracy": 0.33837890625,
            "f1": 0.16702094161351103,
            "f1_weighted": 0.34181912657046715
          },
          {
            "accuracy": 0.29931640625,
            "f1": 0.17597550674827725,
            "f1_weighted": 0.32689411919756745
          },
          {
            "accuracy": 0.2265625,
            "f1": 0.12031022137018739,
            "f1_weighted": 0.1813815124526777
          },
          {
            "accuracy": 0.27978515625,
            "f1": 0.13828641123014376,
            "f1_weighted": 0.27024829742181744
          },
          {
            "accuracy": 0.32080078125,
            "f1": 0.17042127326639483,
            "f1_weighted": 0.3551957375080044
          },
          {
            "accuracy": 0.36328125,
            "f1": 0.17722838767194155,
            "f1_weighted": 0.3641730889007147
          },
          {
            "accuracy": 0.3623046875,
            "f1": 0.18466955485600076,
            "f1_weighted": 0.39765306496844244
          },
          {
            "accuracy": 0.29833984375,
            "f1": 0.14881814232259058,
            "f1_weighted": 0.3159554741160661
          },
          {
            "accuracy": 0.32470703125,
            "f1": 0.15909096374345166,
            "f1_weighted": 0.33059068852448914
          },
          {
            "accuracy": 0.359375,
            "f1": 0.1570356817648884,
            "f1_weighted": 0.3355902979876721
          }
        ]
      }
    ]
  },
  "task_name": "MAUDLegalBenchClassification"
}