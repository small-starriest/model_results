{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "evaluation_time": 14.181386709213257,
  "kg_co2_emissions": 0.002341960690470913,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.815087890625,
        "f1": 0.7191231869252334,
        "f1_weighted": 0.8183552954482801,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ],
        "main_score": 0.815087890625,
        "scores_per_experiment": [
          {
            "accuracy": 0.8369140625,
            "f1": 0.7203103051615501,
            "f1_weighted": 0.8286896318252521
          },
          {
            "accuracy": 0.80908203125,
            "f1": 0.7322712154382568,
            "f1_weighted": 0.8208382130000493
          },
          {
            "accuracy": 0.83154296875,
            "f1": 0.7409680285585988,
            "f1_weighted": 0.8344499199305719
          },
          {
            "accuracy": 0.830078125,
            "f1": 0.7224036988890775,
            "f1_weighted": 0.8260888452389079
          },
          {
            "accuracy": 0.7958984375,
            "f1": 0.6960106183220987,
            "f1_weighted": 0.8030724327025183
          },
          {
            "accuracy": 0.828125,
            "f1": 0.6985668044207128,
            "f1_weighted": 0.8161086211472693
          },
          {
            "accuracy": 0.8154296875,
            "f1": 0.7236815568089007,
            "f1_weighted": 0.8191287839215264
          },
          {
            "accuracy": 0.82177734375,
            "f1": 0.752343508397407,
            "f1_weighted": 0.8349487907527074
          },
          {
            "accuracy": 0.73974609375,
            "f1": 0.663671938300596,
            "f1_weighted": 0.7620128836072966
          },
          {
            "accuracy": 0.84228515625,
            "f1": 0.7410041949551346,
            "f1_weighted": 0.8382148323567019
          }
        ]
      }
    ]
  },
  "task_name": "SweRecClassification"
}