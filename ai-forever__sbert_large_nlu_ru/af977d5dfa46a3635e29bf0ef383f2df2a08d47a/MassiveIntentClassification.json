{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 20.942862510681152,
  "kg_co2_emissions": 0.003716254450954701,
  "mteb_version": "1.14.12",
  "scores": {
    "test": [
      {
        "accuracy": 0.6109280430396772,
        "f1": 0.5644916148136062,
        "f1_weighted": 0.6012067996741102,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6109280430396772,
        "scores_per_experiment": [
          {
            "accuracy": 0.6183591123066577,
            "f1": 0.5768051086846571,
            "f1_weighted": 0.6083213850465107
          },
          {
            "accuracy": 0.6297915265635508,
            "f1": 0.582183725370267,
            "f1_weighted": 0.6270073841046323
          },
          {
            "accuracy": 0.6002017484868863,
            "f1": 0.5601571747952011,
            "f1_weighted": 0.5924814948013409
          },
          {
            "accuracy": 0.6338264963012777,
            "f1": 0.5707038942913306,
            "f1_weighted": 0.6308552853599828
          },
          {
            "accuracy": 0.6039004707464694,
            "f1": 0.5519431809550628,
            "f1_weighted": 0.589035772830018
          },
          {
            "accuracy": 0.582044384667115,
            "f1": 0.551761816886186,
            "f1_weighted": 0.5625956658689234
          },
          {
            "accuracy": 0.6096166778749159,
            "f1": 0.5594329060345532,
            "f1_weighted": 0.5900550317030864
          },
          {
            "accuracy": 0.6109616677874916,
            "f1": 0.5626926564575798,
            "f1_weighted": 0.6059708038071389
          },
          {
            "accuracy": 0.5954942837928715,
            "f1": 0.5499670906912725,
            "f1_weighted": 0.5808216538580258
          },
          {
            "accuracy": 0.625084061869536,
            "f1": 0.5792685939699516,
            "f1_weighted": 0.6249235193614429
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6171175602557797,
        "f1": 0.5469450508736646,
        "f1_weighted": 0.6082162652885544,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6171175602557797,
        "scores_per_experiment": [
          {
            "accuracy": 0.6286276438760453,
            "f1": 0.5655619323040794,
            "f1_weighted": 0.6214688891729783
          },
          {
            "accuracy": 0.646827348745696,
            "f1": 0.573728443948369,
            "f1_weighted": 0.6449129699035235
          },
          {
            "accuracy": 0.6099360550909986,
            "f1": 0.5430553338373719,
            "f1_weighted": 0.6076291137296398
          },
          {
            "accuracy": 0.6389572060993606,
            "f1": 0.5652594294105568,
            "f1_weighted": 0.6369988154503399
          },
          {
            "accuracy": 0.6074766355140186,
            "f1": 0.5316048947734724,
            "f1_weighted": 0.5910229418631837
          },
          {
            "accuracy": 0.5946876537137236,
            "f1": 0.5373779981892092,
            "f1_weighted": 0.5745451797999167
          },
          {
            "accuracy": 0.602065912444663,
            "f1": 0.5187772888290059,
            "f1_weighted": 0.5826342577632605
          },
          {
            "accuracy": 0.6094441711756026,
            "f1": 0.529684923341182,
            "f1_weighted": 0.6031590822703893
          },
          {
            "accuracy": 0.6055090998524348,
            "f1": 0.5420131696872067,
            "f1_weighted": 0.5933509117602522
          },
          {
            "accuracy": 0.6276438760452533,
            "f1": 0.5623870944161927,
            "f1_weighted": 0.6264404911720607
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}