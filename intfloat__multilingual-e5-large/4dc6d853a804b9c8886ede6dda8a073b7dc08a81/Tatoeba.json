{
  "dataset_revision": "69e8f12da6e31d59addadda9a9c8a2e601a0e282",
  "evaluation_time": 152.81587076187134,
  "kg_co2_emissions": 0.02585157358760609,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.129,
        "f1": 0.1109764972372626,
        "hf_subset": "bre-eng",
        "languages": [
          "bre-Latn",
          "eng-Latn"
        ],
        "main_score": 0.1109764972372626,
        "precision": 0.10587861004191675,
        "recall": 0.129
      },
      {
        "accuracy": 0.594,
        "f1": 0.5490521367521367,
        "hf_subset": "oci-eng",
        "languages": [
          "oci-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5490521367521367,
        "precision": 0.5343284002547161,
        "recall": 0.594
      },
      {
        "accuracy": 0.4538922155688623,
        "f1": 0.39874210934206034,
        "hf_subset": "orv-eng",
        "languages": [
          "orv-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.39874210934206034,
        "precision": 0.38193326799614224,
        "recall": 0.4538922155688623
      },
      {
        "accuracy": 0.972,
        "f1": 0.9626666666666667,
        "hf_subset": "tur-eng",
        "languages": [
          "tur-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9626666666666667,
        "precision": 0.958,
        "recall": 0.972
      },
      {
        "accuracy": 0.924,
        "f1": 0.9022333333333332,
        "hf_subset": "afr-eng",
        "languages": [
          "afr-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9022333333333332,
        "precision": 0.8920833333333333,
        "recall": 0.924
      },
      {
        "accuracy": 0.081,
        "f1": 0.07034300144300144,
        "hf_subset": "dtp-eng",
        "languages": [
          "dtp-Latn",
          "eng-Latn"
        ],
        "main_score": 0.07034300144300144,
        "precision": 0.06758308249721293,
        "recall": 0.081
      },
      {
        "accuracy": 0.947,
        "f1": 0.9334222222222223,
        "hf_subset": "glg-eng",
        "languages": [
          "glg-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9334222222222223,
        "precision": 0.9275416666666667,
        "recall": 0.947
      },
      {
        "accuracy": 0.959,
        "f1": 0.947,
        "hf_subset": "sqi-eng",
        "languages": [
          "sqi-Latn",
          "eng-Latn"
        ],
        "main_score": 0.947,
        "precision": 0.9411666666666667,
        "recall": 0.959
      },
      {
        "accuracy": 0.6501809408926418,
        "f1": 0.5899790264687732,
        "hf_subset": "gla-eng",
        "languages": [
          "gla-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5899790264687732,
        "precision": 0.5682118444482739,
        "recall": 0.6501809408926418
      },
      {
        "accuracy": 0.894,
        "f1": 0.8660666666666668,
        "hf_subset": "heb-eng",
        "languages": [
          "heb-Hebr",
          "eng-Latn"
        ],
        "main_score": 0.8660666666666668,
        "precision": 0.8531166666666666,
        "recall": 0.894
      },
      {
        "accuracy": 0.982532751091703,
        "f1": 0.9769529354682193,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.9769529354682193,
        "precision": 0.9742843279961184,
        "recall": 0.982532751091703
      },
      {
        "accuracy": 0.8030660377358491,
        "f1": 0.7632622978436658,
        "hf_subset": "yid-eng",
        "languages": [
          "yid-Hebr",
          "eng-Latn"
        ],
        "main_score": 0.7632622978436658,
        "precision": 0.747868935309973,
        "recall": 0.8030660377358491
      },
      {
        "accuracy": 0.979,
        "f1": 0.972,
        "hf_subset": "nob-eng",
        "languages": [
          "nob-Latn",
          "eng-Latn"
        ],
        "main_score": 0.972,
        "precision": 0.9685,
        "recall": 0.979
      },
      {
        "accuracy": 0.9653284671532847,
        "f1": 0.9537712895377128,
        "hf_subset": "tha-eng",
        "languages": [
          "tha-Thai",
          "eng-Latn"
        ],
        "main_score": 0.9537712895377128,
        "precision": 0.947992700729927,
        "recall": 0.9653284671532847
      },
      {
        "accuracy": 0.943,
        "f1": 0.929,
        "hf_subset": "ind-eng",
        "languages": [
          "ind-Latn",
          "eng-Latn"
        ],
        "main_score": 0.929,
        "precision": 0.9226666666666667,
        "recall": 0.943
      },
      {
        "accuracy": 0.938,
        "f1": 0.9208999999999999,
        "hf_subset": "isl-eng",
        "languages": [
          "isl-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9208999999999999,
        "precision": 0.91275,
        "recall": 0.938
      },
      {
        "accuracy": 0.96,
        "f1": 0.9489,
        "hf_subset": "ces-eng",
        "languages": [
          "ces-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9489,
        "precision": 0.9439166666666666,
        "recall": 0.96
      },
      {
        "accuracy": 0.77,
        "f1": 0.721652380952381,
        "hf_subset": "uig-eng",
        "languages": [
          "uig-Arab",
          "eng-Latn"
        ],
        "main_score": 0.721652380952381,
        "precision": 0.7028944444444444,
        "recall": 0.77
      },
      {
        "accuracy": 0.941,
        "f1": 0.9231666666666667,
        "hf_subset": "rus-eng",
        "languages": [
          "rus-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.9231666666666667,
        "precision": 0.9143333333333332,
        "recall": 0.941
      },
      {
        "accuracy": 0.957,
        "f1": 0.9452999999999999,
        "hf_subset": "zsm-eng",
        "languages": [
          "zsm-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9452999999999999,
        "precision": 0.94,
        "recall": 0.957
      },
      {
        "accuracy": 0.67,
        "f1": 0.6201721001221001,
        "hf_subset": "war-eng",
        "languages": [
          "war-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6201721001221001,
        "precision": 0.6015080086580088,
        "recall": 0.67
      },
      {
        "accuracy": 0.964,
        "f1": 0.9528333333333333,
        "hf_subset": "jpn-eng",
        "languages": [
          "jpn-Jpan",
          "eng-Latn"
        ],
        "main_score": 0.9528333333333333,
        "precision": 0.9473333333333332,
        "recall": 0.964
      },
      {
        "accuracy": 0.958,
        "f1": 0.9448333333333333,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9448333333333333,
        "precision": 0.9383333333333334,
        "recall": 0.958
      },
      {
        "accuracy": 0.884,
        "f1": 0.8563333333333333,
        "hf_subset": "mkd-eng",
        "languages": [
          "mkd-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.8563333333333333,
        "precision": 0.8440833333333334,
        "recall": 0.884
      },
      {
        "accuracy": 0.977,
        "f1": 0.97,
        "hf_subset": "vie-eng",
        "languages": [
          "vie-Latn",
          "eng-Latn"
        ],
        "main_score": 0.97,
        "precision": 0.9665,
        "recall": 0.977
      },
      {
        "accuracy": 0.943502824858757,
        "f1": 0.9286252354048965,
        "hf_subset": "bos-eng",
        "languages": [
          "bos-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9286252354048965,
        "precision": 0.922080979284369,
        "recall": 0.943502824858757
      },
      {
        "accuracy": 0.48737650933040616,
        "f1": 0.41555736920720454,
        "hf_subset": "arq-eng",
        "languages": [
          "arq-Arab",
          "eng-Latn"
        ],
        "main_score": 0.41555736920720454,
        "precision": 0.39068745317373194,
        "recall": 0.48737650933040616
      },
      {
        "accuracy": 0.32116788321167883,
        "f1": 0.2716175358577362,
        "hf_subset": "cha-eng",
        "languages": [
          "cha-Latn",
          "eng-Latn"
        ],
        "main_score": 0.2716175358577362,
        "precision": 0.261432626578612,
        "recall": 0.32116788321167883
      },
      {
        "accuracy": 0.953,
        "f1": 0.9388333333333333,
        "hf_subset": "ell-eng",
        "languages": [
          "ell-Grek",
          "eng-Latn"
        ],
        "main_score": 0.9388333333333333,
        "precision": 0.9318333333333333,
        "recall": 0.953
      },
      {
        "accuracy": 0.9285714285714286,
        "f1": 0.9092093441150045,
        "hf_subset": "hye-eng",
        "languages": [
          "hye-Armn",
          "eng-Latn"
        ],
        "main_score": 0.9092093441150045,
        "precision": 0.9000449236298292,
        "recall": 0.9285714285714286
      },
      {
        "accuracy": 0.8382608695652174,
        "f1": 0.7966956521739131,
        "hf_subset": "kaz-eng",
        "languages": [
          "kaz-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.7966956521739131,
        "precision": 0.7791884057971014,
        "recall": 0.8382608695652174
      },
      {
        "accuracy": 0.8380281690140845,
        "f1": 0.8086854460093896,
        "hf_subset": "xho-eng",
        "languages": [
          "xho-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8086854460093896,
        "precision": 0.7960093896713615,
        "recall": 0.8380281690140845
      },
      {
        "accuracy": 0.790356394129979,
        "f1": 0.747349505840072,
        "hf_subset": "arz-eng",
        "languages": [
          "arz-Arab",
          "eng-Latn"
        ],
        "main_score": 0.747349505840072,
        "precision": 0.7290356394129979,
        "recall": 0.790356394129979
      },
      {
        "accuracy": 0.964,
        "f1": 0.9544,
        "hf_subset": "fin-eng",
        "languages": [
          "fin-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9544,
        "precision": 0.9499166666666666,
        "recall": 0.964
      },
      {
        "accuracy": 0.762,
        "f1": 0.7148147546897547,
        "hf_subset": "gle-eng",
        "languages": [
          "gle-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7148147546897547,
        "precision": 0.6965409090909092,
        "recall": 0.762
      },
      {
        "accuracy": 0.828,
        "f1": 0.7916339285714284,
        "hf_subset": "ile-eng",
        "languages": [
          "ile-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7916339285714284,
        "precision": 0.7762822580645161,
        "recall": 0.828
      },
      {
        "accuracy": 0.449,
        "f1": 0.3889962621607783,
        "hf_subset": "ber-eng",
        "languages": [
          "ber-Tfng",
          "eng-Latn"
        ],
        "main_score": 0.3889962621607783,
        "precision": 0.36971031746031746,
        "recall": 0.449
      },
      {
        "accuracy": 0.9022727272727272,
        "f1": 0.8753030303030302,
        "hf_subset": "mon-eng",
        "languages": [
          "mon-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.8753030303030302,
        "precision": 0.8637121212121212,
        "recall": 0.9022727272727272
      },
      {
        "accuracy": 0.902,
        "f1": 0.8760666666666667,
        "hf_subset": "aze-eng",
        "languages": [
          "aze-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8760666666666667,
        "precision": 0.8645277777777778,
        "recall": 0.902
      },
      {
        "accuracy": 0.947,
        "f1": 0.931,
        "hf_subset": "srp-eng",
        "languages": [
          "srp-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.931,
        "precision": 0.9235,
        "recall": 0.947
      },
      {
        "accuracy": 0.5769230769230769,
        "f1": 0.5316239316239316,
        "hf_subset": "tzl-eng",
        "languages": [
          "tzl-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5316239316239316,
        "precision": 0.5170673076923077,
        "recall": 0.5769230769230769
      },
      {
        "accuracy": 0.5281837160751566,
        "f1": 0.48435977731384827,
        "hf_subset": "dsb-eng",
        "languages": [
          "dsb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.48435977731384827,
        "precision": 0.4711291973845539,
        "recall": 0.5281837160751566
      },
      {
        "accuracy": 0.974,
        "f1": 0.966,
        "hf_subset": "pol-eng",
        "languages": [
          "pol-Latn",
          "eng-Latn"
        ],
        "main_score": 0.966,
        "precision": 0.962,
        "recall": 0.974
      },
      {
        "accuracy": 0.81,
        "f1": 0.7782323809523809,
        "hf_subset": "eus-eng",
        "languages": [
          "eus-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7782323809523809,
        "precision": 0.7660194444444445,
        "recall": 0.81
      },
      {
        "accuracy": 0.7626459143968871,
        "f1": 0.71621271076524,
        "hf_subset": "nov-eng",
        "languages": [
          "nov-Latn",
          "eng-Latn"
        ],
        "main_score": 0.71621271076524,
        "precision": 0.6981934408004445,
        "recall": 0.7626459143968871
      },
      {
        "accuracy": 0.3793103448275862,
        "f1": 0.3315192743764172,
        "hf_subset": "tuk-eng",
        "languages": [
          "tuk-Latn",
          "eng-Latn"
        ],
        "main_score": 0.3315192743764172,
        "precision": 0.3157456528146183,
        "recall": 0.3793103448275862
      },
      {
        "accuracy": 0.948,
        "f1": 0.9331666666666666,
        "hf_subset": "ukr-eng",
        "languages": [
          "ukr-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.9331666666666666,
        "precision": 0.926,
        "recall": 0.948
      },
      {
        "accuracy": 0.875,
        "f1": 0.8502690476190475,
        "hf_subset": "est-eng",
        "languages": [
          "est-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8502690476190475,
        "precision": 0.8396261904761905,
        "recall": 0.875
      },
      {
        "accuracy": 0.993,
        "f1": 0.9906666666666666,
        "hf_subset": "deu-eng",
        "languages": [
          "deu-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9906666666666666,
        "precision": 0.9895,
        "recall": 0.993
      },
      {
        "accuracy": 0.864,
        "f1": 0.8352061289587605,
        "hf_subset": "ido-eng",
        "languages": [
          "ido-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8352061289587605,
        "precision": 0.8234222222222222,
        "recall": 0.864
      },
      {
        "accuracy": 0.8104347826086956,
        "f1": 0.762128364389234,
        "hf_subset": "cym-eng",
        "languages": [
          "cym-Latn",
          "eng-Latn"
        ],
        "main_score": 0.762128364389234,
        "precision": 0.742,
        "recall": 0.8104347826086956
      },
      {
        "accuracy": 0.88,
        "f1": 0.8547666666666667,
        "hf_subset": "ara-eng",
        "languages": [
          "ara-Arab",
          "eng-Latn"
        ],
        "main_score": 0.8547666666666667,
        "precision": 0.8444099567099568,
        "recall": 0.88
      },
      {
        "accuracy": 0.42292490118577075,
        "f1": 0.3697502980111676,
        "hf_subset": "csb-eng",
        "languages": [
          "csb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.3697502980111676,
        "precision": 0.35412284568876057,
        "recall": 0.42292490118577075
      },
      {
        "accuracy": 0.742,
        "f1": 0.6925817460317459,
        "hf_subset": "cbk-eng",
        "languages": [
          "cbk-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6925817460317459,
        "precision": 0.6735992063492062,
        "recall": 0.742
      },
      {
        "accuracy": 0.867,
        "f1": 0.8301523809523809,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.8301523809523809,
        "precision": 0.8137833333333333,
        "recall": 0.867
      },
      {
        "accuracy": 0.946,
        "f1": 0.9313333333333332,
        "hf_subset": "slk-eng",
        "languages": [
          "slk-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9313333333333332,
        "precision": 0.9245333333333333,
        "recall": 0.946
      },
      {
        "accuracy": 0.7709923664122137,
        "f1": 0.7261541257724463,
        "hf_subset": "fao-eng",
        "languages": [
          "fao-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7261541257724463,
        "precision": 0.7089983807541059,
        "recall": 0.7709923664122137
      },
      {
        "accuracy": 0.112,
        "f1": 0.09316381801735853,
        "hf_subset": "pam-eng",
        "languages": [
          "pam-Latn",
          "eng-Latn"
        ],
        "main_score": 0.09316381801735853,
        "precision": 0.08823174603174602,
        "recall": 0.112
      },
      {
        "accuracy": 0.6376811594202898,
        "f1": 0.5890398076733481,
        "hf_subset": "hsb-eng",
        "languages": [
          "hsb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5890398076733481,
        "precision": 0.5709903641090304,
        "recall": 0.6376811594202898
      },
      {
        "accuracy": 0.593,
        "f1": 0.5336589138830518,
        "hf_subset": "lat-eng",
        "languages": [
          "lat-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5336589138830518,
        "precision": 0.5138842105263157,
        "recall": 0.593
      },
      {
        "accuracy": 0.932,
        "f1": 0.9139746031746032,
        "hf_subset": "nno-eng",
        "languages": [
          "nno-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9139746031746032,
        "precision": 0.906125,
        "recall": 0.932
      },
      {
        "accuracy": 0.5811965811965812,
        "f1": 0.5165242165242165,
        "hf_subset": "gsw-eng",
        "languages": [
          "gsw-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5165242165242165,
        "precision": 0.4941768108434775,
        "recall": 0.5811965811965812
      },
      {
        "accuracy": 0.927,
        "f1": 0.91025,
        "hf_subset": "cat-eng",
        "languages": [
          "cat-Latn",
          "eng-Latn"
        ],
        "main_score": 0.91025,
        "precision": 0.9030428571428571,
        "recall": 0.927
      },
      {
        "accuracy": 0.916,
        "f1": 0.8921190476190475,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.8921190476190475,
        "precision": 0.8808666666666668,
        "recall": 0.916
      },
      {
        "accuracy": 0.092,
        "f1": 0.07912334382274959,
        "hf_subset": "kzj-eng",
        "languages": [
          "kzj-Latn",
          "eng-Latn"
        ],
        "main_score": 0.07912334382274959,
        "precision": 0.07631672921795872,
        "recall": 0.092
      },
      {
        "accuracy": 0.927,
        "f1": 0.9065,
        "hf_subset": "kor-eng",
        "languages": [
          "kor-Hang",
          "eng-Latn"
        ],
        "main_score": 0.9065,
        "precision": 0.8968333333333333,
        "recall": 0.927
      },
      {
        "accuracy": 0.9161603888213852,
        "f1": 0.8957067638720131,
        "hf_subset": "slv-eng",
        "languages": [
          "slv-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8957067638720131,
        "precision": 0.8866950182260024,
        "recall": 0.9161603888213852
      },
      {
        "accuracy": 0.8503937007874016,
        "f1": 0.8175853018372703,
        "hf_subset": "ast-eng",
        "languages": [
          "ast-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8175853018372703,
        "precision": 0.8034120734908137,
        "recall": 0.8503937007874016
      },
      {
        "accuracy": 0.964,
        "f1": 0.9528333333333333,
        "hf_subset": "cmn-eng",
        "languages": [
          "cmn-Hans",
          "eng-Latn"
        ],
        "main_score": 0.9528333333333333,
        "precision": 0.9475,
        "recall": 0.964
      },
      {
        "accuracy": 0.085,
        "f1": 0.06282429263935621,
        "hf_subset": "cor-eng",
        "languages": [
          "cor-Latn",
          "eng-Latn"
        ],
        "main_score": 0.06282429263935621,
        "precision": 0.05783274240739785,
        "recall": 0.085
      },
      {
        "accuracy": 0.9316239316239316,
        "f1": 0.9133903133903133,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.9133903133903133,
        "precision": 0.9056267806267806,
        "recall": 0.9316239316239316
      },
      {
        "accuracy": 0.411,
        "f1": 0.3654373737373737,
        "hf_subset": "kab-eng",
        "languages": [
          "kab-Latn",
          "eng-Latn"
        ],
        "main_score": 0.3654373737373737,
        "precision": 0.35054137806637814,
        "recall": 0.411
      },
      {
        "accuracy": 0.91,
        "f1": 0.8870857142857141,
        "hf_subset": "yue-eng",
        "languages": [
          "yue-Hant",
          "eng-Latn"
        ],
        "main_score": 0.8870857142857141,
        "precision": 0.877,
        "recall": 0.91
      },
      {
        "accuracy": 0.964,
        "f1": 0.953,
        "hf_subset": "swe-eng",
        "languages": [
          "swe-Latn",
          "eng-Latn"
        ],
        "main_score": 0.953,
        "precision": 0.9476666666666667,
        "recall": 0.964
      },
      {
        "accuracy": 0.938,
        "f1": 0.9214,
        "hf_subset": "pes-eng",
        "languages": [
          "pes-Arab",
          "eng-Latn"
        ],
        "main_score": 0.9214,
        "precision": 0.9135833333333333,
        "recall": 0.938
      },
      {
        "accuracy": 0.952,
        "f1": 0.9401333333333335,
        "hf_subset": "hun-eng",
        "languages": [
          "hun-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9401333333333335,
        "precision": 0.9346666666666668,
        "recall": 0.952
      },
      {
        "accuracy": 0.937,
        "f1": 0.9199666666666667,
        "hf_subset": "tgl-eng",
        "languages": [
          "tgl-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9199666666666667,
        "precision": 0.9126666666666667,
        "recall": 0.937
      },
      {
        "accuracy": 0.6476190476190476,
        "f1": 0.5985441225441225,
        "hf_subset": "pms-eng",
        "languages": [
          "pms-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5985441225441225,
        "precision": 0.5806437811700969,
        "recall": 0.6476190476190476
      },
      {
        "accuracy": 0.921,
        "f1": 0.9006333333333333,
        "hf_subset": "lvs-eng",
        "languages": [
          "lvs-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9006333333333333,
        "precision": 0.8917,
        "recall": 0.921
      },
      {
        "accuracy": 0.7666666666666667,
        "f1": 0.7160705960705961,
        "hf_subset": "swh-eng",
        "languages": [
          "swh-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7160705960705961,
        "precision": 0.6960683760683761,
        "recall": 0.7666666666666667
      },
      {
        "accuracy": 0.7663551401869159,
        "f1": 0.7235202492211837,
        "hf_subset": "uzb-eng",
        "languages": [
          "uzb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7235202492211837,
        "precision": 0.7060358255451713,
        "recall": 0.7663551401869159
      },
      {
        "accuracy": 0.949,
        "f1": 0.9363333333333334,
        "hf_subset": "por-eng",
        "languages": [
          "por-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9363333333333334,
        "precision": 0.9301666666666666,
        "recall": 0.949
      },
      {
        "accuracy": 0.96,
        "f1": 0.9486666666666667,
        "hf_subset": "ron-eng",
        "languages": [
          "ron-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9486666666666667,
        "precision": 0.9431666666666667,
        "recall": 0.96
      },
      {
        "accuracy": 0.741,
        "f1": 0.6927722222222222,
        "hf_subset": "nds-eng",
        "languages": [
          "nds-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6927722222222222,
        "precision": 0.6749916056166055,
        "recall": 0.741
      },
      {
        "accuracy": 0.6820809248554913,
        "f1": 0.6343104872006606,
        "hf_subset": "fry-eng",
        "languages": [
          "fry-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6343104872006606,
        "precision": 0.6169143958161298,
        "recall": 0.6820809248554913
      },
      {
        "accuracy": 0.6537396121883656,
        "f1": 0.5996449456975772,
        "hf_subset": "khm-eng",
        "languages": [
          "khm-Khmr",
          "eng-Latn"
        ],
        "main_score": 0.5996449456975772,
        "precision": 0.5809691801968689,
        "recall": 0.6537396121883656
      },
      {
        "accuracy": 0.974,
        "f1": 0.9663333333333333,
        "hf_subset": "nld-eng",
        "languages": [
          "nld-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9663333333333333,
        "precision": 0.9626666666666668,
        "recall": 0.974
      },
      {
        "accuracy": 0.904,
        "f1": 0.8848111111111111,
        "hf_subset": "lit-eng",
        "languages": [
          "lit-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8848111111111111,
        "precision": 0.877452380952381,
        "recall": 0.904
      },
      {
        "accuracy": 0.7748917748917749,
        "f1": 0.7227375798804371,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.7227375798804371,
        "precision": 0.7014430014430013,
        "recall": 0.7748917748917749
      },
      {
        "accuracy": 0.8511904761904762,
        "f1": 0.8069444444444445,
        "hf_subset": "amh-eng",
        "languages": [
          "amh-Ethi",
          "eng-Latn"
        ],
        "main_score": 0.8069444444444445,
        "precision": 0.7872023809523809,
        "recall": 0.8511904761904762
      },
      {
        "accuracy": 0.8048780487804879,
        "f1": 0.7545644599303137,
        "hf_subset": "jav-eng",
        "languages": [
          "jav-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7545644599303137,
        "precision": 0.7337398373983739,
        "recall": 0.8048780487804879
      },
      {
        "accuracy": 0.912,
        "f1": 0.8857999999999999,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.8857999999999999,
        "precision": 0.8733333333333334,
        "recall": 0.912
      },
      {
        "accuracy": 0.978,
        "f1": 0.971,
        "hf_subset": "spa-eng",
        "languages": [
          "spa-Latn",
          "eng-Latn"
        ],
        "main_score": 0.971,
        "precision": 0.9676666666666668,
        "recall": 0.978
      },
      {
        "accuracy": 0.671,
        "f1": 0.6290722580722581,
        "hf_subset": "lfn-eng",
        "languages": [
          "lfn-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6290722580722581,
        "precision": 0.6148897759103641,
        "recall": 0.671
      },
      {
        "accuracy": 0.6033333333333334,
        "f1": 0.5531203703703703,
        "hf_subset": "ceb-eng",
        "languages": [
          "ceb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5531203703703703,
        "precision": 0.5339971108326371,
        "recall": 0.6033333333333334
      },
      {
        "accuracy": 0.946,
        "f1": 0.9293333333333333,
        "hf_subset": "bul-eng",
        "languages": [
          "bul-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.9293333333333333,
        "precision": 0.9213333333333332,
        "recall": 0.946
      },
      {
        "accuracy": 0.778,
        "f1": 0.7351500000000001,
        "hf_subset": "tat-eng",
        "languages": [
          "tat-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.7351500000000001,
        "precision": 0.7175282106782106,
        "recall": 0.778
      },
      {
        "accuracy": 0.7121951219512195,
        "f1": 0.6682926829268293,
        "hf_subset": "kur-eng",
        "languages": [
          "kur-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6682926829268293,
        "precision": 0.651260162601626,
        "recall": 0.7121951219512195
      },
      {
        "accuracy": 0.085,
        "f1": 0.06787319277832474,
        "hf_subset": "mhr-eng",
        "languages": [
          "mhr-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.06787319277832474,
        "precision": 0.06345209443334443,
        "recall": 0.085
      },
      {
        "accuracy": 0.968,
        "f1": 0.9601222222222222,
        "hf_subset": "epo-eng",
        "languages": [
          "epo-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9601222222222222,
        "precision": 0.9567083333333333,
        "recall": 0.968
      },
      {
        "accuracy": 0.871313672922252,
        "f1": 0.8409070598748882,
        "hf_subset": "kat-eng",
        "languages": [
          "kat-Geor",
          "eng-Latn"
        ],
        "main_score": 0.8409070598748882,
        "precision": 0.8279171454104429,
        "recall": 0.871313672922252
      },
      {
        "accuracy": 0.949,
        "f1": 0.9347333333333333,
        "hf_subset": "ina-eng",
        "languages": [
          "ina-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9347333333333333,
        "precision": 0.92825,
        "recall": 0.949
      },
      {
        "accuracy": 0.9055374592833876,
        "f1": 0.8822553125484721,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.8822553125484721,
        "precision": 0.8726927252985883,
        "recall": 0.9055374592833876
      },
      {
        "accuracy": 0.946,
        "f1": 0.9329,
        "hf_subset": "ita-eng",
        "languages": [
          "ita-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9329,
        "precision": 0.92675,
        "recall": 0.946
      },
      {
        "accuracy": 0.97,
        "f1": 0.9615,
        "hf_subset": "hrv-eng",
        "languages": [
          "hrv-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9615,
        "precision": 0.9576666666666668,
        "recall": 0.97
      },
      {
        "accuracy": 0.949,
        "f1": 0.9342333333333334,
        "hf_subset": "fra-eng",
        "languages": [
          "fra-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9342333333333334,
        "precision": 0.9275833333333332,
        "recall": 0.949
      },
      {
        "accuracy": 0.891,
        "f1": 0.863652380952381,
        "hf_subset": "wuu-eng",
        "languages": [
          "wuu-Hans",
          "eng-Latn"
        ],
        "main_score": 0.863652380952381,
        "precision": 0.8518499999999999,
        "recall": 0.891
      },
      {
        "accuracy": 0.961,
        "f1": 0.9508,
        "hf_subset": "dan-eng",
        "languages": [
          "dan-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9508,
        "precision": 0.9461666666666667,
        "recall": 0.961
      },
      {
        "accuracy": 0.6901408450704225,
        "f1": 0.6340571205007824,
        "hf_subset": "max-eng",
        "languages": [
          "max-Deva",
          "eng-Latn"
        ],
        "main_score": 0.6340571205007824,
        "precision": 0.6133649116923765,
        "recall": 0.6901408450704225
      },
      {
        "accuracy": 0.4701492537313433,
        "f1": 0.4017886756692727,
        "hf_subset": "ang-eng",
        "languages": [
          "ang-Latn",
          "eng-Latn"
        ],
        "main_score": 0.4017886756692727,
        "precision": 0.3817929582854956,
        "recall": 0.4701492537313433
      },
      {
        "accuracy": 0.931,
        "f1": 0.9108333333333333,
        "hf_subset": "bel-eng",
        "languages": [
          "bel-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.9108333333333333,
        "precision": 0.9013333333333333,
        "recall": 0.931
      },
      {
        "accuracy": 0.625,
        "f1": 0.5564200680272109,
        "hf_subset": "swg-eng",
        "languages": [
          "swg-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5564200680272109,
        "precision": 0.5322916666666666,
        "recall": 0.625
      }
    ]
  },
  "task_name": "Tatoeba"
}