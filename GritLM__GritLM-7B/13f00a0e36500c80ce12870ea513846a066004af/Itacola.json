{
  "dataset_revision": "f8f98e5c4d3059cf1a00c8eb3d70aa271423f636",
  "evaluation_time": 29.33650302886963,
  "kg_co2_emissions": 0.005637548200528852,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.5203076923076924,
        "ap": 0.8474618638402734,
        "ap_weighted": 0.8474618638402734,
        "f1": 0.4414587218905771,
        "f1_weighted": 0.5708503679626591,
        "hf_subset": "default",
        "languages": [
          "ita-Latn"
        ],
        "main_score": 0.5203076923076924,
        "scores_per_experiment": [
          {
            "accuracy": 0.7005128205128205,
            "ap": 0.8463861425248536,
            "ap_weighted": 0.8463861425248536,
            "f1": 0.5109726268344967,
            "f1_weighted": 0.7192482597561474
          },
          {
            "accuracy": 0.5620512820512821,
            "ap": 0.8484261528857027,
            "ap_weighted": 0.8484261528857027,
            "f1": 0.4712722359028154,
            "f1_weighted": 0.6211475422914966
          },
          {
            "accuracy": 0.321025641025641,
            "ap": 0.8418484625611913,
            "ap_weighted": 0.8418484625611913,
            "f1": 0.3165386114123069,
            "f1_weighted": 0.3544227728312168
          },
          {
            "accuracy": 0.5446153846153846,
            "ap": 0.849931286663383,
            "ap_weighted": 0.849931286663383,
            "f1": 0.46555555555555556,
            "f1_weighted": 0.606176638176638
          },
          {
            "accuracy": 0.6892307692307692,
            "ap": 0.8620338618389775,
            "ap_weighted": 0.8620338618389775,
            "f1": 0.5457243422863478,
            "f1_weighted": 0.7203936612132257
          },
          {
            "accuracy": 0.3558974358974359,
            "ap": 0.8417434524576122,
            "ap_weighted": 0.8417434524576122,
            "f1": 0.34442131770993933,
            "f1_weighted": 0.40375907609025097
          },
          {
            "accuracy": 0.49333333333333335,
            "ap": 0.8417002326580494,
            "ap_weighted": 0.8417002326580494,
            "f1": 0.43,
            "f1_weighted": 0.5599794871794872
          },
          {
            "accuracy": 0.48923076923076925,
            "ap": 0.8578082899641974,
            "ap_weighted": 0.8578082899641974,
            "f1": 0.4450857142857143,
            "f1_weighted": 0.5521575384615385
          },
          {
            "accuracy": 0.556923076923077,
            "ap": 0.8356879904090166,
            "ap_weighted": 0.8356879904090166,
            "f1": 0.44806692385105673,
            "f1_weighted": 0.6157506515624597
          },
          {
            "accuracy": 0.49025641025641026,
            "ap": 0.8490527664397503,
            "ap_weighted": 0.8490527664397503,
            "f1": 0.4369498910675381,
            "f1_weighted": 0.5554680520641305
          }
        ]
      }
    ]
  },
  "task_name": "Itacola"
}