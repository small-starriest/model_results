{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 55.17401075363159,
  "kg_co2_emissions": 0.013009941368581041,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.404150390625,
        "f1": 0.35623509642715695,
        "f1_weighted": 0.3562357832323708,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.404150390625,
        "scores_per_experiment": [
          {
            "accuracy": 0.41162109375,
            "f1": 0.3683299752791693,
            "f1_weighted": 0.36830120425545504
          },
          {
            "accuracy": 0.41455078125,
            "f1": 0.37134635454545006,
            "f1_weighted": 0.37139387177539984
          },
          {
            "accuracy": 0.40478515625,
            "f1": 0.3390365897128091,
            "f1_weighted": 0.3390387685328983
          },
          {
            "accuracy": 0.40185546875,
            "f1": 0.36262856008013833,
            "f1_weighted": 0.3626084634074719
          },
          {
            "accuracy": 0.40625,
            "f1": 0.35575561831291946,
            "f1_weighted": 0.3557229042651944
          },
          {
            "accuracy": 0.40869140625,
            "f1": 0.3355125761792656,
            "f1_weighted": 0.3355238127457565
          },
          {
            "accuracy": 0.41748046875,
            "f1": 0.3736686931615302,
            "f1_weighted": 0.37365054114404606
          },
          {
            "accuracy": 0.3828125,
            "f1": 0.33739900724176836,
            "f1_weighted": 0.33742930702164453
          },
          {
            "accuracy": 0.4169921875,
            "f1": 0.39202942086741976,
            "f1_weighted": 0.3920603116492493
          },
          {
            "accuracy": 0.37646484375,
            "f1": 0.3266441688910995,
            "f1_weighted": 0.3266286475265925
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.406591796875,
        "f1": 0.35965752926292816,
        "f1_weighted": 0.3596541299633177,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.406591796875,
        "scores_per_experiment": [
          {
            "accuracy": 0.42138671875,
            "f1": 0.3846207563227457,
            "f1_weighted": 0.3845961573283516
          },
          {
            "accuracy": 0.4033203125,
            "f1": 0.3576805395603747,
            "f1_weighted": 0.35772244778573464
          },
          {
            "accuracy": 0.4169921875,
            "f1": 0.3565214590065917,
            "f1_weighted": 0.35651939324752735
          },
          {
            "accuracy": 0.41552734375,
            "f1": 0.37934868451131576,
            "f1_weighted": 0.3793214545967376
          },
          {
            "accuracy": 0.40087890625,
            "f1": 0.3512632705227126,
            "f1_weighted": 0.3512418486315655
          },
          {
            "accuracy": 0.396484375,
            "f1": 0.3206555416042408,
            "f1_weighted": 0.3206669330855061
          },
          {
            "accuracy": 0.39990234375,
            "f1": 0.3547619333091899,
            "f1_weighted": 0.35474096276751854
          },
          {
            "accuracy": 0.4013671875,
            "f1": 0.36124496986121585,
            "f1_weighted": 0.36125891814437866
          },
          {
            "accuracy": 0.408203125,
            "f1": 0.3812786484747959,
            "f1_weighted": 0.38129335052597624
          },
          {
            "accuracy": 0.40185546875,
            "f1": 0.349199489456098,
            "f1_weighted": 0.34917983351988086
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}