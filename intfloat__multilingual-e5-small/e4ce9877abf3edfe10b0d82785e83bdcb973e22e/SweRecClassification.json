{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "evaluation_time": 10.366897344589233,
  "kg_co2_emissions": 0.0015477785545242513,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.751806640625,
        "f1": 0.6465631652779462,
        "f1_weighted": 0.7544707173534559,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ],
        "main_score": 0.751806640625,
        "scores_per_experiment": [
          {
            "accuracy": 0.75,
            "f1": 0.6310394233009545,
            "f1_weighted": 0.744064916854021
          },
          {
            "accuracy": 0.76904296875,
            "f1": 0.6760394726945096,
            "f1_weighted": 0.776457559764442
          },
          {
            "accuracy": 0.771484375,
            "f1": 0.660952200540211,
            "f1_weighted": 0.7689583358289929
          },
          {
            "accuracy": 0.7578125,
            "f1": 0.632412618621012,
            "f1_weighted": 0.7559406150112568
          },
          {
            "accuracy": 0.71484375,
            "f1": 0.5937188746979031,
            "f1_weighted": 0.7111451201964374
          },
          {
            "accuracy": 0.77490234375,
            "f1": 0.662677652968305,
            "f1_weighted": 0.7755153609548853
          },
          {
            "accuracy": 0.75390625,
            "f1": 0.6515944739055417,
            "f1_weighted": 0.756500013241218
          },
          {
            "accuracy": 0.76806640625,
            "f1": 0.6862070863547434,
            "f1_weighted": 0.7810497430878223
          },
          {
            "accuracy": 0.68505859375,
            "f1": 0.6126632804349627,
            "f1_weighted": 0.7047932949707273
          },
          {
            "accuracy": 0.77294921875,
            "f1": 0.6583265692613186,
            "f1_weighted": 0.7702822136247567
          }
        ]
      }
    ]
  },
  "task_name": "SweRecClassification"
}