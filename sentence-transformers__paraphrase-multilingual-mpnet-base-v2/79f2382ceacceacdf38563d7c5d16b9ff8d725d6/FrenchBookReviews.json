{
  "dataset_revision": "534725e03fec6f560dbe8166e8ae3825314a6290",
  "evaluation_time": 11.246944665908813,
  "kg_co2_emissions": 0.0017071508947729965,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.39345703125,
        "f1": 0.33715889190423454,
        "f1_weighted": 0.43714015619710567,
        "hf_subset": "default",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.39345703125,
        "scores_per_experiment": [
          {
            "accuracy": 0.32421875,
            "f1": 0.298727541410207,
            "f1_weighted": 0.37673888520160975
          },
          {
            "accuracy": 0.47216796875,
            "f1": 0.38737885597660865,
            "f1_weighted": 0.5079459414553268
          },
          {
            "accuracy": 0.47265625,
            "f1": 0.35994143995048505,
            "f1_weighted": 0.5017073121636808
          },
          {
            "accuracy": 0.37451171875,
            "f1": 0.33667546671758775,
            "f1_weighted": 0.42017489034298394
          },
          {
            "accuracy": 0.3544921875,
            "f1": 0.2982186386565235,
            "f1_weighted": 0.4203426538073386
          },
          {
            "accuracy": 0.38330078125,
            "f1": 0.34165042009545643,
            "f1_weighted": 0.42680386485435295
          },
          {
            "accuracy": 0.3681640625,
            "f1": 0.3257907145889585,
            "f1_weighted": 0.4036548873314648
          },
          {
            "accuracy": 0.39990234375,
            "f1": 0.3482903104519641,
            "f1_weighted": 0.439632993856622
          },
          {
            "accuracy": 0.40576171875,
            "f1": 0.35079195541599567,
            "f1_weighted": 0.4525058854403132
          },
          {
            "accuracy": 0.37939453125,
            "f1": 0.3241235757785587,
            "f1_weighted": 0.42189424751736393
          }
        ]
      }
    ]
  },
  "task_name": "FrenchBookReviews"
}