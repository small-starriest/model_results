{
  "dataset_revision": "f8f98e5c4d3059cf1a00c8eb3d70aa271423f636",
  "evaluation_time": 34.6857476234436,
  "kg_co2_emissions": 0.006583105168647688,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.4741538461538461,
        "ap": 0.8379412705859979,
        "ap_weighted": 0.8379412705859979,
        "f1": 0.4078124119899423,
        "f1_weighted": 0.5321600850545201,
        "hf_subset": "default",
        "languages": [
          "ita-Latn"
        ],
        "main_score": 0.4741538461538461,
        "scores_per_experiment": [
          {
            "accuracy": 0.6502564102564102,
            "ap": 0.839135627657698,
            "ap_weighted": 0.839135627657698,
            "f1": 0.4817136381138258,
            "f1_weighted": 0.6839043377884728
          },
          {
            "accuracy": 0.4707692307692308,
            "ap": 0.8451778810107243,
            "ap_weighted": 0.8451778810107243,
            "f1": 0.42208018967624544,
            "f1_weighted": 0.5368349261038822
          },
          {
            "accuracy": 0.4123076923076923,
            "ap": 0.8287529745631442,
            "ap_weighted": 0.8287529745631442,
            "f1": 0.3716547016660275,
            "f1_weighted": 0.4809915756901826
          },
          {
            "accuracy": 0.4307692307692308,
            "ap": 0.8444650059707677,
            "ap_weighted": 0.8444650059707677,
            "f1": 0.39784073951667015,
            "f1_weighted": 0.49417101875113467
          },
          {
            "accuracy": 0.597948717948718,
            "ap": 0.8470437133906067,
            "ap_weighted": 0.8470437133906067,
            "f1": 0.4834744250916285,
            "f1_weighted": 0.6498237787641309
          },
          {
            "accuracy": 0.3302564102564103,
            "ap": 0.8398213560698337,
            "ap_weighted": 0.8398213560698337,
            "f1": 0.32342105011471584,
            "f1_weighted": 0.3699433482219626
          },
          {
            "accuracy": 0.41846153846153844,
            "ap": 0.8356151035322776,
            "ap_weighted": 0.8356151035322776,
            "f1": 0.3822694314297559,
            "f1_weighted": 0.48455814071026004
          },
          {
            "accuracy": 0.41333333333333333,
            "ap": 0.8445196483915407,
            "ap_weighted": 0.8445196483915407,
            "f1": 0.3867492984508739,
            "f1_weighted": 0.4740968416360976
          },
          {
            "accuracy": 0.5487179487179488,
            "ap": 0.8283784580794601,
            "ap_weighted": 0.8283784580794601,
            "f1": 0.4313180695038018,
            "f1_weighted": 0.6080804158600908
          },
          {
            "accuracy": 0.4687179487179487,
            "ap": 0.8265029371939245,
            "ap_weighted": 0.8265029371939245,
            "f1": 0.3976025763358778,
            "f1_weighted": 0.5391964670189862
          }
        ]
      }
    ]
  },
  "task_name": "Itacola"
}