{
  "dataset_revision": "de7ba3406ee55ea2cc52a0a41408fa6aede6d3c6",
  "evaluation_time": 8.91795563697815,
  "kg_co2_emissions": 0.0013379449119628802,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.6013333333333333,
        "f1": 0.5981127279745087,
        "f1_weighted": 0.6044257449930275,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ],
        "main_score": 0.6013333333333333,
        "scores_per_experiment": [
          {
            "accuracy": 0.6133333333333333,
            "f1": 0.6062102241407699,
            "f1_weighted": 0.6225433212350292
          },
          {
            "accuracy": 0.6,
            "f1": 0.6007703371690942,
            "f1_weighted": 0.5991682790760298
          },
          {
            "accuracy": 0.62,
            "f1": 0.6234810735204127,
            "f1_weighted": 0.6186344960223796
          },
          {
            "accuracy": 0.5466666666666666,
            "f1": 0.5499644365214417,
            "f1_weighted": 0.5396962248692255
          },
          {
            "accuracy": 0.6333333333333333,
            "f1": 0.6255718069098352,
            "f1_weighted": 0.6388765498976767
          },
          {
            "accuracy": 0.6533333333333333,
            "f1": 0.6426065222377777,
            "f1_weighted": 0.6574251002538974
          },
          {
            "accuracy": 0.5933333333333334,
            "f1": 0.5918650832899246,
            "f1_weighted": 0.5992018497638343
          },
          {
            "accuracy": 0.6,
            "f1": 0.601242838679095,
            "f1_weighted": 0.5980671710606718
          },
          {
            "accuracy": 0.6133333333333333,
            "f1": 0.6072721001338781,
            "f1_weighted": 0.6264539815610549
          },
          {
            "accuracy": 0.54,
            "f1": 0.5321428571428571,
            "f1_weighted": 0.5441904761904762
          }
        ]
      }
    ]
  },
  "task_name": "LccSentimentClassification"
}