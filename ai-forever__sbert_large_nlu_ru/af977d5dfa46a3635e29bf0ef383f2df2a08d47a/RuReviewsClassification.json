{
  "dataset_revision": "f6d2c31f4dc6b88f468552750bfec05b4b41b05a",
  "evaluation_time": 7.4587438106536865,
  "kg_co2_emissions": 0.0012218045307712904,
  "mteb_version": "1.14.12",
  "scores": {
    "test": [
      {
        "accuracy": 0.58271484375,
        "f1": 0.5767616096186102,
        "f1_weighted": 0.5767613881087228,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.58271484375,
        "scores_per_experiment": [
          {
            "accuracy": 0.6103515625,
            "f1": 0.5953240726424808,
            "f1_weighted": 0.5953142209317482
          },
          {
            "accuracy": 0.50830078125,
            "f1": 0.4877142504766317,
            "f1_weighted": 0.4877031907619698
          },
          {
            "accuracy": 0.587890625,
            "f1": 0.5874312830631013,
            "f1_weighted": 0.5874231797610631
          },
          {
            "accuracy": 0.60107421875,
            "f1": 0.5964089769582251,
            "f1_weighted": 0.5964221037069821
          },
          {
            "accuracy": 0.63427734375,
            "f1": 0.6389469409313736,
            "f1_weighted": 0.6389531272442001
          },
          {
            "accuracy": 0.57373046875,
            "f1": 0.5579957047274458,
            "f1_weighted": 0.5580077955224617
          },
          {
            "accuracy": 0.59326171875,
            "f1": 0.5938242781692371,
            "f1_weighted": 0.5938403512955011
          },
          {
            "accuracy": 0.564453125,
            "f1": 0.561241987876628,
            "f1_weighted": 0.5612378111527556
          },
          {
            "accuracy": 0.52587890625,
            "f1": 0.5251113017520338,
            "f1_weighted": 0.5250764482723863
          },
          {
            "accuracy": 0.6279296875,
            "f1": 0.6236172995889439,
            "f1_weighted": 0.6236356524381594
          }
        ]
      }
    ]
  },
  "task_name": "RuReviewsClassification"
}