{
  "dataset_revision": "0ded8ff72cc68cbb7bb5c01b0a9157982b73ddaf",
  "evaluation_time": 11.435149669647217,
  "kg_co2_emissions": 0.0017136760335241265,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.509375,
        "f1": 0.4512457371209077,
        "f1_weighted": 0.45558423257133474,
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.509375,
        "scores_per_experiment": [
          {
            "accuracy": 0.5107421875,
            "f1": 0.4500955575542277,
            "f1_weighted": 0.4542711483511276
          },
          {
            "accuracy": 0.53564453125,
            "f1": 0.4694540690774575,
            "f1_weighted": 0.47247499695876505
          },
          {
            "accuracy": 0.51171875,
            "f1": 0.47150263697565187,
            "f1_weighted": 0.4775497500671968
          },
          {
            "accuracy": 0.50927734375,
            "f1": 0.4502418296944053,
            "f1_weighted": 0.45509299208016507
          },
          {
            "accuracy": 0.4970703125,
            "f1": 0.44748062991232673,
            "f1_weighted": 0.45068446564747466
          },
          {
            "accuracy": 0.52978515625,
            "f1": 0.4695182552977494,
            "f1_weighted": 0.4717659098432708
          },
          {
            "accuracy": 0.4794921875,
            "f1": 0.4149824020537698,
            "f1_weighted": 0.4193474151331436
          },
          {
            "accuracy": 0.47998046875,
            "f1": 0.42892152038204634,
            "f1_weighted": 0.429194501268795
          },
          {
            "accuracy": 0.53369140625,
            "f1": 0.46645960323901997,
            "f1_weighted": 0.4749157462712327
          },
          {
            "accuracy": 0.50634765625,
            "f1": 0.4438008670224228,
            "f1_weighted": 0.45054540009217625
          }
        ]
      }
    ]
  },
  "task_name": "TweetEmotionClassification"
}