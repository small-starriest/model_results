{
  "dataset_revision": "f8f98e5c4d3059cf1a00c8eb3d70aa271423f636",
  "evaluation_time": 9.212101221084595,
  "kg_co2_emissions": 0.0013543714089141636,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.47610256410256413,
        "ap": 0.838917319311731,
        "ap_weighted": 0.838917319311731,
        "f1": 0.4089033640095776,
        "f1_weighted": 0.5315199457553915,
        "hf_subset": "default",
        "languages": [
          "ita-Latn"
        ],
        "main_score": 0.47610256410256413,
        "scores_per_experiment": [
          {
            "accuracy": 0.5784615384615385,
            "ap": 0.8390256586960703,
            "ap_weighted": 0.8390256586960703,
            "f1": 0.4617593608673636,
            "f1_weighted": 0.6332138816940478
          },
          {
            "accuracy": 0.4430769230769231,
            "ap": 0.8380179591334153,
            "ap_weighted": 0.8380179591334153,
            "f1": 0.399015130617791,
            "f1_weighted": 0.5103379168687042
          },
          {
            "accuracy": 0.3548717948717949,
            "ap": 0.8473093489838153,
            "ap_weighted": 0.8473093489838153,
            "f1": 0.3462451536700989,
            "f1_weighted": 0.397619882969485
          },
          {
            "accuracy": 0.5128205128205128,
            "ap": 0.8303525684402664,
            "ap_weighted": 0.8303525684402664,
            "f1": 0.42227015240217103,
            "f1_weighted": 0.5787392985136578
          },
          {
            "accuracy": 0.6338461538461538,
            "ap": 0.8535986924172442,
            "ap_weighted": 0.8535986924172442,
            "f1": 0.5087578909931537,
            "f1_weighted": 0.6783389302674445
          },
          {
            "accuracy": 0.4451282051282051,
            "ap": 0.8453430775477061,
            "ap_weighted": 0.8453430775477061,
            "f1": 0.4074040370336667,
            "f1_weighted": 0.5096886716639804
          },
          {
            "accuracy": 0.5333333333333333,
            "ap": 0.8300812839744822,
            "ap_weighted": 0.8300812839744822,
            "f1": 0.4294718539447946,
            "f1_weighted": 0.5959997547913218
          },
          {
            "accuracy": 0.28512820512820514,
            "ap": 0.8383688591149006,
            "ap_weighted": 0.8383688591149006,
            "f1": 0.2840406624237105,
            "f1_weighted": 0.303129898842076
          },
          {
            "accuracy": 0.4533333333333333,
            "ap": 0.8341486431620415,
            "ap_weighted": 0.8341486431620415,
            "f1": 0.4002769683505958,
            "f1_weighted": 0.5223066078108921
          },
          {
            "accuracy": 0.521025641025641,
            "ap": 0.8329271016473674,
            "ap_weighted": 0.8329271016473674,
            "f1": 0.4297924297924298,
            "f1_weighted": 0.5858246141323065
          }
        ]
      }
    ]
  },
  "task_name": "Itacola"
}