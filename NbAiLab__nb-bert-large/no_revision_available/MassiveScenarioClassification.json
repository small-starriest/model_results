{
  "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
  "mteb_dataset_name": "MassiveScenarioClassification",
  "mteb_version": "1.0.3.dev0",
  "test": {
    "da": {
      "accuracy": 0.6043039677202422,
      "accuracy_stderr": 0.028896083939013734,
      "f1": 0.5929666103807454,
      "f1_stderr": 0.024498119074388865,
      "main_score": 0.6043039677202422
    },
    "evaluation_time": 276.31,
    "nb": {
      "accuracy": 0.6744451916610626,
      "accuracy_stderr": 0.018218160191781674,
      "f1": 0.6615213677979109,
      "f1_stderr": 0.017939032267758108,
      "main_score": 0.6744451916610626
    },
    "sv": {
      "accuracy": 0.5711835911230666,
      "accuracy_stderr": 0.013201343205764519,
      "f1": 0.5635949260739122,
      "f1_stderr": 0.012873335207392875,
      "main_score": 0.5711835911230666
    }
  },
  "validation": {
    "da": {
      "accuracy": 0.5956222331529759,
      "accuracy_stderr": 0.03342455262822981,
      "f1": 0.5875727820080507,
      "f1_stderr": 0.029940048396168564,
      "main_score": 0.5956222331529759
    },
    "evaluation_time": 224.91,
    "nb": {
      "accuracy": 0.6699458927693065,
      "accuracy_stderr": 0.018119764344706215,
      "f1": 0.6580186602245238,
      "f1_stderr": 0.019366399426336157,
      "main_score": 0.6699458927693065
    },
    "sv": {
      "accuracy": 0.562911952779144,
      "accuracy_stderr": 0.017002684569602463,
      "f1": 0.5578154264635324,
      "f1_stderr": 0.015173431843298071,
      "main_score": 0.562911952779144
    }
  }
}