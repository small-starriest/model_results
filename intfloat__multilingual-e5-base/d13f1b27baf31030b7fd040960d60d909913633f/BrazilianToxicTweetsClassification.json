{
  "dataset_revision": "fb4f11a5bc68b99891852d20f1ec074be6289768",
  "evaluation_time": 5.525326251983643,
  "kg_co2_emissions": 0.0008428089203173472,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.198388671875,
        "f1": 0.1618751168438446,
        "hf_subset": "default",
        "languages": [
          "por-Latn"
        ],
        "lrap": 0.8017876519097232,
        "main_score": 0.198388671875,
        "scores_per_experiment": [
          {
            "accuracy": 0.1669921875,
            "f1": 0.17513509593819318,
            "lrap": 0.8291829427083339
          },
          {
            "accuracy": 0.12255859375,
            "f1": 0.13195079497707823,
            "lrap": 0.7896321614583346
          },
          {
            "accuracy": 0.20166015625,
            "f1": 0.1559225554408731,
            "lrap": 0.8275010850694449
          },
          {
            "accuracy": 0.236328125,
            "f1": 0.1579934275832449,
            "lrap": 0.8171522352430566
          },
          {
            "accuracy": 0.240234375,
            "f1": 0.19525567782790024,
            "lrap": 0.8032769097222232
          },
          {
            "accuracy": 0.220703125,
            "f1": 0.1769216978435694,
            "lrap": 0.8078206380208347
          },
          {
            "accuracy": 0.228515625,
            "f1": 0.1289535388997921,
            "lrap": 0.7500813802083344
          },
          {
            "accuracy": 0.1201171875,
            "f1": 0.16805931804866828,
            "lrap": 0.8206787109375002
          },
          {
            "accuracy": 0.25927734375,
            "f1": 0.17277622672637252,
            "lrap": 0.7762044270833346
          },
          {
            "accuracy": 0.1875,
            "f1": 0.15578283515275385,
            "lrap": 0.7963460286458344
          }
        ]
      }
    ]
  },
  "task_name": "BrazilianToxicTweetsClassification"
}