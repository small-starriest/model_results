{
  "dataset_revision": "f9bd92144ed76200d6eb3ce73a8bd4eba9ffdc85",
  "evaluation_time": 82.62651634216309,
  "kg_co2_emissions": 0.012993665176886647,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.6448400000000001,
        "f1": 0.6211694299003577,
        "f1_weighted": 0.628188044421115,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6448400000000001,
        "scores_per_experiment": [
          {
            "accuracy": 0.652,
            "f1": 0.6352520270813748,
            "f1_weighted": 0.6423108496836817
          },
          {
            "accuracy": 0.6528,
            "f1": 0.6137408212970842,
            "f1_weighted": 0.6255344419987374
          },
          {
            "accuracy": 0.6352,
            "f1": 0.6185420353615947,
            "f1_weighted": 0.6219122454360074
          },
          {
            "accuracy": 0.6576,
            "f1": 0.6304994611401852,
            "f1_weighted": 0.6376857558434658
          },
          {
            "accuracy": 0.6488,
            "f1": 0.6271187118356428,
            "f1_weighted": 0.6352971959329239
          },
          {
            "accuracy": 0.6448,
            "f1": 0.6134399428371862,
            "f1_weighted": 0.6223857693438091
          },
          {
            "accuracy": 0.6528,
            "f1": 0.6310705379317327,
            "f1_weighted": 0.6380331002480077
          },
          {
            "accuracy": 0.648,
            "f1": 0.6232123127888874,
            "f1_weighted": 0.631711316832907
          },
          {
            "accuracy": 0.5972,
            "f1": 0.5699915068395968,
            "f1_weighted": 0.5754303675768975
          },
          {
            "accuracy": 0.6592,
            "f1": 0.6488269418902928,
            "f1_weighted": 0.651579401314712
          }
        ]
      }
    ]
  },
  "task_name": "ArxivClassification"
}