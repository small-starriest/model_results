{
  "dataset_revision": "557bf94ac6177cc442f42d0b09b6e4b76e8f47c9",
  "evaluation_time": 30.53555178642273,
  "kg_co2_emissions": 0.006788784387779578,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.5888625592417062,
        "ap": 0.2157582279253037,
        "ap_weighted": 0.2157582279253037,
        "f1": 0.5231770813133814,
        "f1_weighted": 0.6389901190531202,
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.5888625592417062,
        "scores_per_experiment": [
          {
            "accuracy": 0.5777251184834123,
            "ap": 0.21476102772909747,
            "ap_weighted": 0.21476102772909747,
            "f1": 0.5201288294090035,
            "f1_weighted": 0.6320121816465691
          },
          {
            "accuracy": 0.5962085308056873,
            "ap": 0.2279817185732512,
            "ap_weighted": 0.2279817185732512,
            "f1": 0.5381693861885488,
            "f1_weighted": 0.6483506500339186
          },
          {
            "accuracy": 0.5848341232227489,
            "ap": 0.2435521558557643,
            "ap_weighted": 0.2435521558557643,
            "f1": 0.5398082908004481,
            "f1_weighted": 0.6366820514666102
          },
          {
            "accuracy": 0.6014218009478673,
            "ap": 0.23886424650023388,
            "ap_weighted": 0.23886424650023388,
            "f1": 0.5470576268607991,
            "f1_weighted": 0.652662452036773
          },
          {
            "accuracy": 0.5407582938388625,
            "ap": 0.18550862009753416,
            "ap_weighted": 0.18550862009753416,
            "f1": 0.4792810345156916,
            "f1_weighted": 0.5996916665693506
          },
          {
            "accuracy": 0.5289099526066351,
            "ap": 0.19372664451063729,
            "ap_weighted": 0.19372664451063729,
            "f1": 0.4791800140849407,
            "f1_weighted": 0.5874875489021277
          },
          {
            "accuracy": 0.6293838862559241,
            "ap": 0.1871885509109812,
            "ap_weighted": 0.1871885509109812,
            "f1": 0.5214692398355498,
            "f1_weighted": 0.6744021718883758
          },
          {
            "accuracy": 0.5834123222748815,
            "ap": 0.19285531382091003,
            "ap_weighted": 0.19285531382091003,
            "f1": 0.5069331994165154,
            "f1_weighted": 0.6376195465391146
          },
          {
            "accuracy": 0.7549763033175355,
            "ap": 0.26552916189988035,
            "ap_weighted": 0.26552916189988035,
            "f1": 0.6358001382855727,
            "f1_weighted": 0.7760073912643526
          },
          {
            "accuracy": 0.4909952606635071,
            "ap": 0.2076148393547471,
            "ap_weighted": 0.2076148393547471,
            "f1": 0.4639430537367437,
            "f1_weighted": 0.5449855301840095
          }
        ]
      }
    ]
  },
  "task_name": "TweetSarcasmClassification"
}