{
  "dataset_revision": "b0f431acbf8d3865cb7c7b3effb2a9771a618ebc",
  "evaluation_time": 31.179265022277832,
  "kg_co2_emissions": 0.006394651298449032,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.627880859375,
        "ap": 0.3932941841243197,
        "ap_weighted": 0.3932941841243197,
        "f1": 0.6039029504168625,
        "f1_weighted": 0.638198586236314,
        "hf_subset": "default",
        "languages": [
          "por-Latn"
        ],
        "main_score": 0.627880859375,
        "scores_per_experiment": [
          {
            "accuracy": 0.70068359375,
            "ap": 0.42372504625103646,
            "ap_weighted": 0.42372504625103646,
            "f1": 0.6512578395344489,
            "f1_weighted": 0.6997220032010593
          },
          {
            "accuracy": 0.57763671875,
            "ap": 0.36392758349930154,
            "ap_weighted": 0.36392758349930154,
            "f1": 0.5628224348652268,
            "f1_weighted": 0.5925295930002098
          },
          {
            "accuracy": 0.67236328125,
            "ap": 0.4337314699307827,
            "ap_weighted": 0.4337314699307827,
            "f1": 0.6530667013379147,
            "f1_weighted": 0.6832700438090047
          },
          {
            "accuracy": 0.6171875,
            "ap": 0.36423344869982793,
            "ap_weighted": 0.36423344869982793,
            "f1": 0.579381071687168,
            "f1_weighted": 0.6259310062221859
          },
          {
            "accuracy": 0.67626953125,
            "ap": 0.4024454263533645,
            "ap_weighted": 0.4024454263533645,
            "f1": 0.6290040582599237,
            "f1_weighted": 0.6778859974945306
          },
          {
            "accuracy": 0.56884765625,
            "ap": 0.35432689395326056,
            "ap_weighted": 0.35432689395326056,
            "f1": 0.5514803832937117,
            "f1_weighted": 0.5840601807005452
          },
          {
            "accuracy": 0.5517578125,
            "ap": 0.35543515409322574,
            "ap_weighted": 0.35543515409322574,
            "f1": 0.5423260922697614,
            "f1_weighted": 0.5665790871475178
          },
          {
            "accuracy": 0.60595703125,
            "ap": 0.3858046072151991,
            "ap_weighted": 0.3858046072151991,
            "f1": 0.5918306663903943,
            "f1_weighted": 0.6198609336708718
          },
          {
            "accuracy": 0.6689453125,
            "ap": 0.43346568501635907,
            "ap_weighted": 0.43346568501635907,
            "f1": 0.6513564653562002,
            "f1_weighted": 0.6802633532707929
          },
          {
            "accuracy": 0.63916015625,
            "ap": 0.41584652623083895,
            "ap_weighted": 0.41584652623083895,
            "f1": 0.6265037911738758,
            "f1_weighted": 0.6518836638464219
          }
        ]
      }
    ]
  },
  "task_name": "HateSpeechPortugueseClassification"
}