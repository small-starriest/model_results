{
  "dataset_revision": "31efe3c427b0bae9c22cbb560b8f15491cc6bed7",
  "mteb_dataset_name": "MassiveIntentClassification",
  "mteb_version": "1.0.3.dev0",
  "test": {
    "da": {
      "accuracy": 0.4106254203093477,
      "accuracy_stderr": 0.015077840832839742,
      "f1": 0.3795648689253743,
      "f1_stderr": 0.012582701200982243,
      "main_score": 0.4106254203093477
    },
    "evaluation_time": 185.25,
    "nb": {
      "accuracy": 0.404640215198386,
      "accuracy_stderr": 0.014068009415622156,
      "f1": 0.37341048610165817,
      "f1_stderr": 0.01144099071939448,
      "main_score": 0.404640215198386
    },
    "sv": {
      "accuracy": 0.45117686617350367,
      "accuracy_stderr": 0.014194901450353853,
      "f1": 0.4202125805260968,
      "f1_stderr": 0.010769282318607538,
      "main_score": 0.45117686617350367
    },
    "fr": {
      "accuracy": 0.13581035642232683,
      "accuracy_stderr": 0.016491524857694173,
      "f1": 0.14614446238627393,
      "f1_stderr": 0.013927468848973678,
      "main_score": 0.13581035642232683
    }
  },
  "validation": {
    "da": {
      "accuracy": 0.39822921790457455,
      "accuracy_stderr": 0.01358917785734561,
      "f1": 0.3802486620338696,
      "f1_stderr": 0.0070583125002401225,
      "main_score": 0.39822921790457455
    },
    "evaluation_time": 168.16,
    "nb": {
      "accuracy": 0.40816527299557304,
      "accuracy_stderr": 0.013360421935161806,
      "f1": 0.3833260420650286,
      "f1_stderr": 0.008267938503171176,
      "main_score": 0.40816527299557304
    },
    "sv": {
      "accuracy": 0.4410723069355632,
      "accuracy_stderr": 0.018019407110450938,
      "f1": 0.4107311432887936,
      "f1_stderr": 0.01425923980011036,
      "main_score": 0.4410723069355632
    }
  }
}