{
  "dataset_revision": "7b56c6cb1c9c8523249f407044c838660df3811a",
  "evaluation_time": 9.805984258651733,
  "kg_co2_emissions": 0.0014622299053313314,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.54404296875,
        "f1": 0.45763308273838443,
        "f1_weighted": 0.5919205146156864,
        "hf_subset": "default",
        "languages": [
          "vie-Latn"
        ],
        "main_score": 0.54404296875,
        "scores_per_experiment": [
          {
            "accuracy": 0.57861328125,
            "f1": 0.479319569557268,
            "f1_weighted": 0.6262963018397179
          },
          {
            "accuracy": 0.560546875,
            "f1": 0.46485043863768255,
            "f1_weighted": 0.60347318997201
          },
          {
            "accuracy": 0.59130859375,
            "f1": 0.4766746947378035,
            "f1_weighted": 0.602814917976392
          },
          {
            "accuracy": 0.515625,
            "f1": 0.4554349153595816,
            "f1_weighted": 0.5915507289657631
          },
          {
            "accuracy": 0.4794921875,
            "f1": 0.41498272005388,
            "f1_weighted": 0.535074179033801
          },
          {
            "accuracy": 0.53564453125,
            "f1": 0.46809166167145905,
            "f1_weighted": 0.598137919553015
          },
          {
            "accuracy": 0.60986328125,
            "f1": 0.48613971686658913,
            "f1_weighted": 0.6402605001001201
          },
          {
            "accuracy": 0.4970703125,
            "f1": 0.4202631211480565,
            "f1_weighted": 0.5457716794728245
          },
          {
            "accuracy": 0.55908203125,
            "f1": 0.47353580761435793,
            "f1_weighted": 0.6083765658349749
          },
          {
            "accuracy": 0.51318359375,
            "f1": 0.43703818173716563,
            "f1_weighted": 0.5674491634082464
          }
        ]
      }
    ]
  },
  "task_name": "VieStudentFeedbackClassification"
}