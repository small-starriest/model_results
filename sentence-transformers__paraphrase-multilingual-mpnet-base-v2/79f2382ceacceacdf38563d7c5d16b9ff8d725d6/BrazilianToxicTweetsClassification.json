{
  "dataset_revision": "fb4f11a5bc68b99891852d20f1ec074be6289768",
  "evaluation_time": 5.359951972961426,
  "kg_co2_emissions": 0.0008311889107024178,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.223046875,
        "f1": 0.14935106266305095,
        "hf_subset": "default",
        "languages": [
          "por-Latn"
        ],
        "lrap": 0.7943773057725706,
        "main_score": 0.223046875,
        "scores_per_experiment": [
          {
            "accuracy": 0.16796875,
            "f1": 0.15479871774907428,
            "lrap": 0.8036702473958344
          },
          {
            "accuracy": 0.11962890625,
            "f1": 0.16053893153182378,
            "lrap": 0.8042399088541679
          },
          {
            "accuracy": 0.1494140625,
            "f1": 0.15418303119083543,
            "lrap": 0.8047688802083341
          },
          {
            "accuracy": 0.31591796875,
            "f1": 0.11900563553639083,
            "lrap": 0.8080647786458349
          },
          {
            "accuracy": 0.318359375,
            "f1": 0.1670872044019358,
            "lrap": 0.7630954318576397
          },
          {
            "accuracy": 0.27490234375,
            "f1": 0.16916439051174303,
            "lrap": 0.8149007161458348
          },
          {
            "accuracy": 0.18701171875,
            "f1": 0.13204191466457071,
            "lrap": 0.7850748697916681
          },
          {
            "accuracy": 0.14990234375,
            "f1": 0.14401869330043748,
            "lrap": 0.756157769097223
          },
          {
            "accuracy": 0.2861328125,
            "f1": 0.1366498904489463,
            "lrap": 0.807047526041668
          },
          {
            "accuracy": 0.26123046875,
            "f1": 0.15602221729475188,
            "lrap": 0.7967529296875011
          }
        ]
      }
    ]
  },
  "task_name": "BrazilianToxicTweetsClassification"
}