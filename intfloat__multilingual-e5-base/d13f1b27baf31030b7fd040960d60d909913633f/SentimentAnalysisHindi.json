{
  "dataset_revision": "1beac1b941da76a9c51e3e5b39d230fde9a80983",
  "evaluation_time": 10.041060447692871,
  "kg_co2_emissions": 0.0014949196758187878,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.656689453125,
        "f1": 0.6464383926809989,
        "f1_weighted": 0.6583568445412644,
        "hf_subset": "default",
        "languages": [
          "hin-Deva"
        ],
        "main_score": 0.6464383926809989,
        "scores_per_experiment": [
          {
            "accuracy": 0.68896484375,
            "f1": 0.6807565812059693,
            "f1_weighted": 0.6910895970589306
          },
          {
            "accuracy": 0.67138671875,
            "f1": 0.6577196817716208,
            "f1_weighted": 0.6767860353687635
          },
          {
            "accuracy": 0.62744140625,
            "f1": 0.6105764892135712,
            "f1_weighted": 0.6272111716446582
          },
          {
            "accuracy": 0.61279296875,
            "f1": 0.6096612573101349,
            "f1_weighted": 0.6140780680278906
          },
          {
            "accuracy": 0.69873046875,
            "f1": 0.6814390904821294,
            "f1_weighted": 0.7013779562912621
          },
          {
            "accuracy": 0.64453125,
            "f1": 0.6356415384513979,
            "f1_weighted": 0.6443149025921965
          },
          {
            "accuracy": 0.619140625,
            "f1": 0.6143232312899282,
            "f1_weighted": 0.615649368388393
          },
          {
            "accuracy": 0.646484375,
            "f1": 0.6378325640609641,
            "f1_weighted": 0.6503988682753274
          },
          {
            "accuracy": 0.65234375,
            "f1": 0.6376140722178493,
            "f1_weighted": 0.6574304959748121
          },
          {
            "accuracy": 0.705078125,
            "f1": 0.6988194208064235,
            "f1_weighted": 0.7052319817904105
          }
        ]
      }
    ]
  },
  "task_name": "SentimentAnalysisHindi"
}