{
  "dataset_revision": "b108d2c32ee4e1f4176ea233e1a5ac17bceb9ef9",
  "evaluation_time": 9.861076831817627,
  "kg_co2_emissions": 0.0014627336373021643,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.475390625,
        "f1": 0.46436941806882703,
        "f1_weighted": 0.47856342295445,
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.475390625,
        "scores_per_experiment": [
          {
            "accuracy": 0.453125,
            "f1": 0.4448174651711595,
            "f1_weighted": 0.45669520558239385
          },
          {
            "accuracy": 0.45361328125,
            "f1": 0.45083806991587905,
            "f1_weighted": 0.45558937606276606
          },
          {
            "accuracy": 0.44287109375,
            "f1": 0.4382067477484878,
            "f1_weighted": 0.4507055277462633
          },
          {
            "accuracy": 0.53173828125,
            "f1": 0.4967733049776083,
            "f1_weighted": 0.5298272661368955
          },
          {
            "accuracy": 0.5126953125,
            "f1": 0.5011170024231391,
            "f1_weighted": 0.5083399916226586
          },
          {
            "accuracy": 0.47998046875,
            "f1": 0.47193487094046355,
            "f1_weighted": 0.48473652881921486
          },
          {
            "accuracy": 0.42919921875,
            "f1": 0.4217965132861535,
            "f1_weighted": 0.4331023416175508
          },
          {
            "accuracy": 0.50244140625,
            "f1": 0.49644600596998695,
            "f1_weighted": 0.5100395130319089
          },
          {
            "accuracy": 0.4580078125,
            "f1": 0.44313494968580946,
            "f1_weighted": 0.4604371211505111
          },
          {
            "accuracy": 0.490234375,
            "f1": 0.4786292505695838,
            "f1_weighted": 0.496161357774337
          }
        ]
      }
    ]
  },
  "task_name": "HotelReviewSentimentClassification"
}