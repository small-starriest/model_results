{
  "dataset_revision": "c18a4f81a47ae6fa079fe9d32db288ddde38451d",
  "evaluation_time": 25.07636046409607,
  "kg_co2_emissions": 0.004173204241932315,
  "mteb_version": "1.12.75",
  "scores": {
    "validation": [
      {
        "accuracy": 0.9707207207207207,
        "f1": 0.9651276276276277,
        "hf_subset": "ar-en",
        "languages": [
          "ara-Arab",
          "eng-Latn"
        ],
        "main_score": 0.9651276276276277,
        "precision": 0.9627752752752753,
        "recall": 0.9707207207207207
      },
      {
        "accuracy": 0.9819819819819819,
        "f1": 0.9788083538083537,
        "hf_subset": "de-en",
        "languages": [
          "deu-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9788083538083537,
        "precision": 0.9775900900900901,
        "recall": 0.9819819819819819
      },
      {
        "accuracy": 0.9718468468468469,
        "f1": 0.9654654654654654,
        "hf_subset": "en-ar",
        "languages": [
          "eng-Latn",
          "ara-Arab"
        ],
        "main_score": 0.9654654654654654,
        "precision": 0.9625,
        "recall": 0.9718468468468469
      },
      {
        "accuracy": 0.9808558558558559,
        "f1": 0.9769314769314769,
        "hf_subset": "en-de",
        "languages": [
          "eng-Latn",
          "deu-Latn"
        ],
        "main_score": 0.9769314769314769,
        "precision": 0.9753378378378379,
        "recall": 0.9808558558558559
      },
      {
        "accuracy": 0.9707865168539326,
        "f1": 0.9642492339121552,
        "hf_subset": "en-fr",
        "languages": [
          "eng-Latn",
          "fra-Latn"
        ],
        "main_score": 0.9642492339121552,
        "precision": 0.9613483146067416,
        "recall": 0.9707865168539326
      },
      {
        "accuracy": 0.9623250807319699,
        "f1": 0.9533907427341226,
        "hf_subset": "en-it",
        "languages": [
          "eng-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9533907427341226,
        "precision": 0.9497069728501376,
        "recall": 0.9623250807319699
      },
      {
        "accuracy": 0.8691159586681975,
        "f1": 0.8375857399393564,
        "hf_subset": "en-ja",
        "languages": [
          "eng-Latn",
          "jpn-Jpan"
        ],
        "main_score": 0.8375857399393564,
        "precision": 0.8238723962604559,
        "recall": 0.8691159586681975
      },
      {
        "accuracy": 0.8486916951080774,
        "f1": 0.815092908608267,
        "hf_subset": "en-ko",
        "languages": [
          "eng-Latn",
          "kor-Hang"
        ],
        "main_score": 0.815092908608267,
        "precision": 0.800710173406419,
        "recall": 0.8486916951080774
      },
      {
        "accuracy": 0.9431704885343968,
        "f1": 0.9272848122299767,
        "hf_subset": "en-nl",
        "languages": [
          "eng-Latn",
          "nld-Latn"
        ],
        "main_score": 0.9272848122299767,
        "precision": 0.9200564971751412,
        "recall": 0.9431704885343968
      },
      {
        "accuracy": 0.9748358862144421,
        "f1": 0.9684536834427426,
        "hf_subset": "en-ro",
        "languages": [
          "eng-Latn",
          "ron-Latn"
        ],
        "main_score": 0.9684536834427426,
        "precision": 0.9657913931436908,
        "recall": 0.9748358862144421
      },
      {
        "accuracy": 0.8953356086461889,
        "f1": 0.8697555762402178,
        "hf_subset": "en-zh",
        "languages": [
          "eng-Latn",
          "cmn-Hans"
        ],
        "main_score": 0.8697555762402178,
        "precision": 0.8583238528631021,
        "recall": 0.8953356086461889
      },
      {
        "accuracy": 0.9741573033707865,
        "f1": 0.9674157303370786,
        "hf_subset": "fr-en",
        "languages": [
          "fra-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9674157303370786,
        "precision": 0.9644943820224718,
        "recall": 0.9741573033707865
      },
      {
        "accuracy": 0.9709364908503767,
        "f1": 0.9648726228919986,
        "hf_subset": "it-en",
        "languages": [
          "ita-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9648726228919986,
        "precision": 0.9622652792728142,
        "recall": 0.9709364908503767
      },
      {
        "accuracy": 0.9210789210789211,
        "f1": 0.9012844298558584,
        "hf_subset": "it-nl",
        "languages": [
          "ita-Latn",
          "nld-Latn"
        ],
        "main_score": 0.9012844298558584,
        "precision": 0.8920246420246419,
        "recall": 0.9210789210789211
      },
      {
        "accuracy": 0.949671772428884,
        "f1": 0.9364905699697822,
        "hf_subset": "it-ro",
        "languages": [
          "ita-Latn",
          "ron-Latn"
        ],
        "main_score": 0.9364905699697822,
        "precision": 0.9303428154631657,
        "recall": 0.949671772428884
      },
      {
        "accuracy": 0.894374282433984,
        "f1": 0.8722332393974186,
        "hf_subset": "ja-en",
        "languages": [
          "jpn-Jpan",
          "eng-Latn"
        ],
        "main_score": 0.8722332393974186,
        "precision": 0.8635922949355785,
        "recall": 0.894374282433984
      },
      {
        "accuracy": 0.8395904436860068,
        "f1": 0.8093125304729402,
        "hf_subset": "ko-en",
        "languages": [
          "kor-Hang",
          "eng-Latn"
        ],
        "main_score": 0.8093125304729402,
        "precision": 0.7964949347201906,
        "recall": 0.8395904436860068
      },
      {
        "accuracy": 0.938185443668993,
        "f1": 0.9227871939736346,
        "hf_subset": "nl-en",
        "languages": [
          "nld-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9227871939736346,
        "precision": 0.9162761714855433,
        "recall": 0.938185443668993
      },
      {
        "accuracy": 0.916083916083916,
        "f1": 0.8945578231292517,
        "hf_subset": "nl-it",
        "languages": [
          "nld-Latn",
          "ita-Latn"
        ],
        "main_score": 0.8945578231292517,
        "precision": 0.884981684981685,
        "recall": 0.916083916083916
      },
      {
        "accuracy": 0.91894852135816,
        "f1": 0.8979919678714859,
        "hf_subset": "nl-ro",
        "languages": [
          "nld-Latn",
          "ron-Latn"
        ],
        "main_score": 0.8979919678714859,
        "precision": 0.8887367652427894,
        "recall": 0.91894852135816
      },
      {
        "accuracy": 0.9704595185995624,
        "f1": 0.964138098711403,
        "hf_subset": "ro-en",
        "languages": [
          "ron-Latn",
          "eng-Latn"
        ],
        "main_score": 0.964138098711403,
        "precision": 0.961478847556528,
        "recall": 0.9704595185995624
      },
      {
        "accuracy": 0.9638949671772429,
        "f1": 0.9547775346462436,
        "hf_subset": "ro-it",
        "languages": [
          "ron-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9547775346462436,
        "precision": 0.9506199854121079,
        "recall": 0.9638949671772429
      },
      {
        "accuracy": 0.9178532311062432,
        "f1": 0.8968966776195691,
        "hf_subset": "ro-nl",
        "languages": [
          "ron-Latn",
          "nld-Latn"
        ],
        "main_score": 0.8968966776195691,
        "precision": 0.8876779846659365,
        "recall": 0.9178532311062432
      },
      {
        "accuracy": 0.8953356086461889,
        "f1": 0.870467883778464,
        "hf_subset": "zh-en",
        "languages": [
          "cmn-Hans",
          "eng-Latn"
        ],
        "main_score": 0.870467883778464,
        "precision": 0.860400075843762,
        "recall": 0.8953356086461889
      }
    ]
  },
  "task_name": "IWSLT2017BitextMining"
}