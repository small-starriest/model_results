{
  "dataset_revision": "ba52e9d114a4a145d79b4293afab31304a999a4c",
  "evaluation_time": 151.7572557926178,
  "kg_co2_emissions": 0.02805382707024643,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.871,
        "f1": 0.8483915584415584,
        "hf_subset": "ind-abs",
        "languages": [
          "ind-Latn",
          "abs-Latn"
        ],
        "main_score": 0.8483915584415584,
        "precision": 0.8402010302197802,
        "recall": 0.871
      },
      {
        "accuracy": 0.5796969696969697,
        "f1": 0.5299324915870471,
        "hf_subset": "ind-btk",
        "languages": [
          "ind-Latn",
          "bbc-Latn"
        ],
        "main_score": 0.5299324915870471,
        "precision": 0.5137462348426558,
        "recall": 0.5796969696969697
      },
      {
        "accuracy": 0.7490909090909091,
        "f1": 0.7183693585954276,
        "hf_subset": "ind-bew",
        "languages": [
          "ind-Latn",
          "bew-Latn"
        ],
        "main_score": 0.7183693585954276,
        "precision": 0.7077585128058261,
        "recall": 0.7490909090909091
      },
      {
        "accuracy": 0.669,
        "f1": 0.6252694083694085,
        "hf_subset": "ind-bhp",
        "languages": [
          "ind-Latn",
          "bhp-Latn"
        ],
        "main_score": 0.6252694083694085,
        "precision": 0.6104556633885438,
        "recall": 0.669
      },
      {
        "accuracy": 0.7686363636363637,
        "f1": 0.7438114862074886,
        "hf_subset": "ind-jav",
        "languages": [
          "ind-Latn",
          "jav-Latn"
        ],
        "main_score": 0.7438114862074886,
        "precision": 0.7350607554549298,
        "recall": 0.7686363636363637
      },
      {
        "accuracy": 0.5566666666666666,
        "f1": 0.5169978455839492,
        "hf_subset": "ind-mad",
        "languages": [
          "ind-Latn",
          "mad-Latn"
        ],
        "main_score": 0.5169978455839492,
        "precision": 0.5035541005197337,
        "recall": 0.5566666666666666
      },
      {
        "accuracy": 0.4572727272727273,
        "f1": 0.4114002266159636,
        "hf_subset": "ind-mak",
        "languages": [
          "ind-Latn",
          "mak-Latn"
        ],
        "main_score": 0.4114002266159636,
        "precision": 0.39670376744876384,
        "recall": 0.4572727272727273
      },
      {
        "accuracy": 0.813939393939394,
        "f1": 0.7883317031352385,
        "hf_subset": "ind-min",
        "languages": [
          "ind-Latn",
          "min-Latn"
        ],
        "main_score": 0.7883317031352385,
        "precision": 0.7792498864635917,
        "recall": 0.813939393939394
      },
      {
        "accuracy": 0.882,
        "f1": 0.8633217948717948,
        "hf_subset": "ind-mui",
        "languages": [
          "ind-Latn",
          "mui-Latn"
        ],
        "main_score": 0.8633217948717948,
        "precision": 0.8563691729323308,
        "recall": 0.882
      },
      {
        "accuracy": 0.616,
        "f1": 0.5610206169846413,
        "hf_subset": "ind-rej",
        "languages": [
          "ind-Latn",
          "rej-Latn"
        ],
        "main_score": 0.5610206169846413,
        "precision": 0.5427639028944912,
        "recall": 0.616
      },
      {
        "accuracy": 0.8083333333333333,
        "f1": 0.7854416582667704,
        "hf_subset": "ind-sun",
        "languages": [
          "ind-Latn",
          "sun-Latn"
        ],
        "main_score": 0.7854416582667704,
        "precision": 0.7781469545322047,
        "recall": 0.8083333333333333
      }
    ]
  },
  "task_name": "NusaTranslationBitextMining"
}