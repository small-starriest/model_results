{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 1322.7173640727997,
  "kg_co2_emissions": 0.22294425492525863,
  "mteb_version": "1.12.75",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08838550026752275,
        "map": 0.10485444743161197,
        "mrr": 0.08838550026752275,
        "nAUC_map_diff1": 0.19206515964389806,
        "nAUC_map_max": 0.07766507521581106,
        "nAUC_map_std": 0.2447279182751373,
        "nAUC_mrr_diff1": 0.18613034388468952,
        "nAUC_mrr_max": 0.07829846511528742,
        "nAUC_mrr_std": 0.22331803066906464
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.10977073566068685,
        "map": 0.12671020869171945,
        "mrr": 0.10977073566068685,
        "nAUC_map_diff1": 0.0957443287472777,
        "nAUC_map_max": 0.013632683461023101,
        "nAUC_map_std": 0.18997584635443393,
        "nAUC_mrr_diff1": 0.10247016419820314,
        "nAUC_mrr_max": 0.016404057459693522,
        "nAUC_mrr_std": 0.18724800706434158
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0987504691259907,
        "map": 0.11433487630364239,
        "mrr": 0.0987504691259907,
        "nAUC_map_diff1": -0.029085210284611484,
        "nAUC_map_max": 0.04301202522043939,
        "nAUC_map_std": 0.08379653507362587,
        "nAUC_mrr_diff1": -0.035954167816911936,
        "nAUC_mrr_max": 0.05752693754759389,
        "nAUC_mrr_std": 0.06583937965583928
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09640467801566328,
        "map": 0.11332498686685288,
        "mrr": 0.09640467801566328,
        "nAUC_map_diff1": 0.11533649049849805,
        "nAUC_map_max": -0.01525067635600238,
        "nAUC_map_std": 0.1108706269318614,
        "nAUC_mrr_diff1": 0.12075412077010328,
        "nAUC_mrr_max": -0.006922249856401121,
        "nAUC_mrr_std": 0.09512874581597759
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09852048749949514,
        "map": 0.11446025341209419,
        "mrr": 0.09852048749949514,
        "nAUC_map_diff1": 0.22123905279492767,
        "nAUC_map_max": 0.08744948056831449,
        "nAUC_map_std": -0.04016895728450617,
        "nAUC_mrr_diff1": 0.22663210455954894,
        "nAUC_mrr_max": 0.08880322352084159,
        "nAUC_mrr_std": -0.053891780961410216
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.12424691629149738,
        "map": 0.14011847035195665,
        "mrr": 0.12424691629149738,
        "nAUC_map_diff1": 0.22122187530230925,
        "nAUC_map_max": 0.0058234066593383415,
        "nAUC_map_std": 0.09886427036857791,
        "nAUC_mrr_diff1": 0.22591938184381266,
        "nAUC_mrr_max": 0.011570416817799295,
        "nAUC_mrr_std": 0.09591697870586183
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}