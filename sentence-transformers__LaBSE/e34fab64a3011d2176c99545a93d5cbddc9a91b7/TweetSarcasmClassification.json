{
  "dataset_revision": "557bf94ac6177cc442f42d0b09b6e4b76e8f47c9",
  "evaluation_time": 9.970312356948853,
  "kg_co2_emissions": 0.0014978065553698037,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.5961611374407582,
        "ap": 0.1976491410179389,
        "ap_weighted": 0.1976491410179389,
        "f1": 0.5129804483860225,
        "f1_weighted": 0.6445820143075567,
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.5961611374407582,
        "scores_per_experiment": [
          {
            "accuracy": 0.6218009478672986,
            "ap": 0.2151397699025516,
            "ap_weighted": 0.2151397699025516,
            "f1": 0.5430376874902978,
            "f1_weighted": 0.6707132922110068
          },
          {
            "accuracy": 0.5194312796208531,
            "ap": 0.1921959433193406,
            "ap_weighted": 0.1921959433193406,
            "f1": 0.47272307869747415,
            "f1_weighted": 0.578337163587917
          },
          {
            "accuracy": 0.6962085308056872,
            "ap": 0.2068599392959098,
            "ap_weighted": 0.2068599392959098,
            "f1": 0.5668767739107072,
            "f1_weighted": 0.726157862194204
          },
          {
            "accuracy": 0.5535545023696683,
            "ap": 0.21458274899289792,
            "ap_weighted": 0.21458274899289792,
            "f1": 0.506426130048304,
            "f1_weighted": 0.6090676771285758
          },
          {
            "accuracy": 0.4990521327014218,
            "ap": 0.1949711750619221,
            "ap_weighted": 0.1949711750619221,
            "f1": 0.46239109615309054,
            "f1_weighted": 0.5568714444264669
          },
          {
            "accuracy": 0.5364928909952607,
            "ap": 0.17162561058023834,
            "ap_weighted": 0.17162561058023834,
            "f1": 0.46356155324299364,
            "f1_weighted": 0.5966753059527868
          },
          {
            "accuracy": 0.6407582938388625,
            "ap": 0.17959769748594284,
            "ap_weighted": 0.17959769748594284,
            "f1": 0.5163945331398162,
            "f1_weighted": 0.6814380286469619
          },
          {
            "accuracy": 0.6123222748815166,
            "ap": 0.18405457510860712,
            "ap_weighted": 0.18405457510860712,
            "f1": 0.5111230703866307,
            "f1_weighted": 0.6608135603686494
          },
          {
            "accuracy": 0.6872037914691943,
            "ap": 0.19905796600497633,
            "ap_weighted": 0.19905796600497633,
            "f1": 0.5556755225263512,
            "f1_weighted": 0.7183672837622721
          },
          {
            "accuracy": 0.5947867298578199,
            "ap": 0.21840598442700238,
            "ap_weighted": 0.21840598442700238,
            "f1": 0.5315950382645609,
            "f1_weighted": 0.6473785247967258
          }
        ]
      }
    ]
  },
  "task_name": "TweetSarcasmClassification"
}