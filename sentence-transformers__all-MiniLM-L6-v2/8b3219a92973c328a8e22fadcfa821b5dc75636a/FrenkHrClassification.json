{
  "dataset_revision": "e7fc9f3d8d6c5640a26679d8a50b1666b02cc41f",
  "evaluation_time": 9.897333860397339,
  "kg_co2_emissions": 0.0014512781718727938,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.5748938178386032,
        "ap": 0.6058952857586417,
        "ap_weighted": 0.6058952857586417,
        "f1": 0.5666384000149065,
        "f1_weighted": 0.5701928110416289,
        "hf_subset": "default",
        "languages": [
          "hrv-Latn"
        ],
        "main_score": 0.5748938178386032,
        "scores_per_experiment": [
          {
            "accuracy": 0.5309108069844266,
            "ap": 0.5869629217769496,
            "ap_weighted": 0.5869629217769496,
            "f1": 0.5296200846836867,
            "f1_weighted": 0.5265618867999518
          },
          {
            "accuracy": 0.5908447380840018,
            "ap": 0.6164673989734041,
            "ap_weighted": 0.6164673989734041,
            "f1": 0.5900032247662044,
            "f1_weighted": 0.5923086206264203
          },
          {
            "accuracy": 0.6281264747522416,
            "ap": 0.6191017057722346,
            "ap_weighted": 0.6191017057722346,
            "f1": 0.5969888299977795,
            "f1_weighted": 0.6108923963312659
          },
          {
            "accuracy": 0.6002831524303917,
            "ap": 0.609812642534562,
            "ap_weighted": 0.609812642534562,
            "f1": 0.5862683811961151,
            "f1_weighted": 0.5957193679515376
          },
          {
            "accuracy": 0.5681925436526664,
            "ap": 0.6146910042120148,
            "ap_weighted": 0.6146910042120148,
            "f1": 0.5650540708665179,
            "f1_weighted": 0.5604684134067568
          },
          {
            "accuracy": 0.6007550731477111,
            "ap": 0.6051314882909967,
            "ap_weighted": 0.6051314882909967,
            "f1": 0.5754846464680909,
            "f1_weighted": 0.5883398151658474
          },
          {
            "accuracy": 0.41670599339310993,
            "ap": 0.5327524868345346,
            "ap_weighted": 0.5327524868345346,
            "f1": 0.41537781587335604,
            "f1_weighted": 0.41191929421300666
          },
          {
            "accuracy": 0.6182161396885323,
            "ap": 0.6329349270109619,
            "ap_weighted": 0.6329349270109619,
            "f1": 0.6164448446947873,
            "f1_weighted": 0.6196799181903075
          },
          {
            "accuracy": 0.6059462010382256,
            "ap": 0.6214254520767754,
            "ap_weighted": 0.6214254520767754,
            "f1": 0.6021865110815079,
            "f1_weighted": 0.6069865035990648
          },
          {
            "accuracy": 0.588957055214724,
            "ap": 0.6196728301039824,
            "ap_weighted": 0.6196728301039824,
            "f1": 0.5889555905210189,
            "f1_weighted": 0.5890518941321303
          }
        ]
      }
    ]
  },
  "task_name": "FrenkHrClassification"
}