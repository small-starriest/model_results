{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "evaluation_time": 292.73247146606445,
  "kg_co2_emissions": 0.04388330517572712,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.290771484375,
        "f1": 0.23807858487089506,
        "f1_weighted": 0.29051203821167815,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ],
        "main_score": 0.290771484375,
        "scores_per_experiment": [
          {
            "accuracy": 0.2958984375,
            "f1": 0.2258137028358783,
            "f1_weighted": 0.29787949924311063
          },
          {
            "accuracy": 0.29296875,
            "f1": 0.24897092108525204,
            "f1_weighted": 0.28982881093713503
          },
          {
            "accuracy": 0.29931640625,
            "f1": 0.2524755392287319,
            "f1_weighted": 0.30160354357540525
          },
          {
            "accuracy": 0.294921875,
            "f1": 0.23823852342129423,
            "f1_weighted": 0.2939372203589129
          },
          {
            "accuracy": 0.2841796875,
            "f1": 0.23641567190134083,
            "f1_weighted": 0.28380737465102873
          },
          {
            "accuracy": 0.294921875,
            "f1": 0.2368322783570063,
            "f1_weighted": 0.2976906027265136
          },
          {
            "accuracy": 0.2841796875,
            "f1": 0.22708119860748477,
            "f1_weighted": 0.2823268348029154
          },
          {
            "accuracy": 0.28955078125,
            "f1": 0.23467036400270103,
            "f1_weighted": 0.2875586175565306
          },
          {
            "accuracy": 0.27197265625,
            "f1": 0.23403212284917277,
            "f1_weighted": 0.2764214598011996
          },
          {
            "accuracy": 0.2998046875,
            "f1": 0.2462555264200883,
            "f1_weighted": 0.29406641846402903
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.303857421875,
        "f1": 0.23981599154465724,
        "f1_weighted": 0.3069709216795996,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ],
        "main_score": 0.303857421875,
        "scores_per_experiment": [
          {
            "accuracy": 0.29833984375,
            "f1": 0.23819477223357305,
            "f1_weighted": 0.3053368924648485
          },
          {
            "accuracy": 0.30224609375,
            "f1": 0.24345502410184505,
            "f1_weighted": 0.30393725489836815
          },
          {
            "accuracy": 0.3017578125,
            "f1": 0.23487193746969903,
            "f1_weighted": 0.3058016755566896
          },
          {
            "accuracy": 0.28515625,
            "f1": 0.22661557504043767,
            "f1_weighted": 0.2870067375921567
          },
          {
            "accuracy": 0.3046875,
            "f1": 0.2448130119312216,
            "f1_weighted": 0.30753792510124084
          },
          {
            "accuracy": 0.2919921875,
            "f1": 0.23069812858984867,
            "f1_weighted": 0.29540402700414903
          },
          {
            "accuracy": 0.3193359375,
            "f1": 0.2507222812839695,
            "f1_weighted": 0.3222868349475909
          },
          {
            "accuracy": 0.31005859375,
            "f1": 0.24798159969213793,
            "f1_weighted": 0.31295329415642403
          },
          {
            "accuracy": 0.3017578125,
            "f1": 0.22906899565747077,
            "f1_weighted": 0.30240801515452326
          },
          {
            "accuracy": 0.3232421875,
            "f1": 0.25173858944636907,
            "f1_weighted": 0.3270365599200051
          }
        ]
      }
    ]
  },
  "task_name": "GreekLegalCodeClassification"
}