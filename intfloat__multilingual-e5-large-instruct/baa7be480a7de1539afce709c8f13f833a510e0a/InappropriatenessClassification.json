{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 10.629318475723267,
  "kg_co2_emissions": 0.0017652972758353314,
  "mteb_version": "1.14.12",
  "scores": {
    "test": [
      {
        "accuracy": 0.655322265625,
        "ap": 0.6027459671789649,
        "ap_weighted": 0.6027459671789649,
        "f1": 0.6518714937260511,
        "f1_weighted": 0.6518714937260511,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.655322265625,
        "scores_per_experiment": [
          {
            "accuracy": 0.66259765625,
            "ap": 0.6037284136806131,
            "ap_weighted": 0.6037284136806131,
            "f1": 0.6598820183065097,
            "f1_weighted": 0.6598820183065097
          },
          {
            "accuracy": 0.7060546875,
            "ap": 0.6454031128167641,
            "ap_weighted": 0.6454031128167641,
            "f1": 0.7060544071716377,
            "f1_weighted": 0.7060544071716377
          },
          {
            "accuracy": 0.712890625,
            "ap": 0.6542908344072165,
            "ap_weighted": 0.6542908344072165,
            "f1": 0.7126908794890857,
            "f1_weighted": 0.7126908794890857
          },
          {
            "accuracy": 0.5537109375,
            "ap": 0.529034009725295,
            "ap_weighted": 0.529034009725295,
            "f1": 0.5416661769602946,
            "f1_weighted": 0.5416661769602946
          },
          {
            "accuracy": 0.70703125,
            "ap": 0.6408375850340136,
            "ap_weighted": 0.6408375850340136,
            "f1": 0.7054085155350978,
            "f1_weighted": 0.7054085155350978
          },
          {
            "accuracy": 0.5673828125,
            "ap": 0.5386062202695561,
            "ap_weighted": 0.5386062202695561,
            "f1": 0.566754372979452,
            "f1_weighted": 0.566754372979452
          },
          {
            "accuracy": 0.72119140625,
            "ap": 0.6535998692328326,
            "ap_weighted": 0.6535998692328326,
            "f1": 0.7198635595865583,
            "f1_weighted": 0.7198635595865583
          },
          {
            "accuracy": 0.6669921875,
            "ap": 0.6032441325207469,
            "ap_weighted": 0.6032441325207469,
            "f1": 0.652226222667085,
            "f1_weighted": 0.652226222667085
          },
          {
            "accuracy": 0.65087890625,
            "ap": 0.5959414680903694,
            "ap_weighted": 0.5959414680903694,
            "f1": 0.6498128079760184,
            "f1_weighted": 0.6498128079760184
          },
          {
            "accuracy": 0.6044921875,
            "ap": 0.5627740260122411,
            "ap_weighted": 0.5627740260122411,
            "f1": 0.6043559765887723,
            "f1_weighted": 0.6043559765887723
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}