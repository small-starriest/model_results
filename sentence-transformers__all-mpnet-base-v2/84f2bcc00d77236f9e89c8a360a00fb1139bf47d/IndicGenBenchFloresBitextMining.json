{
  "dataset_revision": "f8650438298df086750ff4973661bb58a201a5ee",
  "evaluation_time": 158.87217044830322,
  "kg_co2_emissions": 0.027070180708944944,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.023715415019762844,
        "f1": 0.017720295396471856,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.017720295396471856,
        "precision": 0.017034833957356096,
        "recall": 0.023715415019762844
      },
      {
        "accuracy": 0.05039525691699605,
        "f1": 0.01918513562521755,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.01918513562521755,
        "precision": 0.015124398105015537,
        "recall": 0.05039525691699605
      },
      {
        "accuracy": 0.03359683794466403,
        "f1": 0.023135447258327856,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.023135447258327856,
        "precision": 0.021419745448601563,
        "recall": 0.03359683794466403
      },
      {
        "accuracy": 0.06126482213438735,
        "f1": 0.022402906887263315,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.022402906887263315,
        "precision": 0.017262468684875968,
        "recall": 0.06126482213438735
      },
      {
        "accuracy": 0.018774703557312252,
        "f1": 0.014565933047086263,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.014565933047086263,
        "precision": 0.01379951628585873,
        "recall": 0.018774703557312252
      },
      {
        "accuracy": 0.07509881422924901,
        "f1": 0.030430146480554043,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.030430146480554043,
        "precision": 0.02333174801155548,
        "recall": 0.07509881422924901
      },
      {
        "accuracy": 0.040513833992094864,
        "f1": 0.027159408953509045,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.027159408953509045,
        "precision": 0.024365499390146508,
        "recall": 0.040513833992094864
      },
      {
        "accuracy": 0.06225296442687747,
        "f1": 0.025524614695876652,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.025524614695876652,
        "precision": 0.020614843861440964,
        "recall": 0.06225296442687747
      },
      {
        "accuracy": 0.03458498023715415,
        "f1": 0.022335036321206012,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.022335036321206012,
        "precision": 0.020043777831167255,
        "recall": 0.03458498023715415
      },
      {
        "accuracy": 0.06620553359683795,
        "f1": 0.028193765457991536,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.028193765457991536,
        "precision": 0.023002818098015725,
        "recall": 0.06620553359683795
      },
      {
        "accuracy": 0.019762845849802372,
        "f1": 0.014836701239863295,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.014836701239863295,
        "precision": 0.01421417012502284,
        "recall": 0.019762845849802372
      },
      {
        "accuracy": 0.06225296442687747,
        "f1": 0.028932792293729772,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.028932792293729772,
        "precision": 0.024261272604149068,
        "recall": 0.06225296442687747
      },
      {
        "accuracy": 0.036561264822134384,
        "f1": 0.028605693804648575,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.028605693804648575,
        "precision": 0.027498092394501517,
        "recall": 0.036561264822134384
      },
      {
        "accuracy": 0.07015810276679842,
        "f1": 0.0312884205034084,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.0312884205034084,
        "precision": 0.025804822111521505,
        "recall": 0.07015810276679842
      },
      {
        "accuracy": 0.08201581027667984,
        "f1": 0.06919861965577165,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.06919861965577165,
        "precision": 0.06611017299293973,
        "recall": 0.08201581027667984
      },
      {
        "accuracy": 0.11166007905138339,
        "f1": 0.0502667982487282,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.0502667982487282,
        "precision": 0.04058423283727341,
        "recall": 0.11166007905138339
      },
      {
        "accuracy": 0.0266798418972332,
        "f1": 0.020056773469497254,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.020056773469497254,
        "precision": 0.01856342513270642,
        "recall": 0.0266798418972332
      },
      {
        "accuracy": 0.05434782608695652,
        "f1": 0.021741258703650408,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.021741258703650408,
        "precision": 0.01707626851389736,
        "recall": 0.05434782608695652
      },
      {
        "accuracy": 0.018774703557312252,
        "f1": 0.013536072295042325,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.013536072295042325,
        "precision": 0.01239535549080138,
        "recall": 0.018774703557312252
      },
      {
        "accuracy": 0.045454545454545456,
        "f1": 0.015987365823801607,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.015987365823801607,
        "precision": 0.012098144386292616,
        "recall": 0.045454545454545456
      },
      {
        "accuracy": 0.02865612648221344,
        "f1": 0.02380109427889014,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.02380109427889014,
        "precision": 0.022957987639573175,
        "recall": 0.02865612648221344
      },
      {
        "accuracy": 0.0741106719367589,
        "f1": 0.030143543849042907,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.030143543849042907,
        "precision": 0.024088906442168507,
        "recall": 0.0741106719367589
      },
      {
        "accuracy": 0.040513833992094864,
        "f1": 0.02995316764865704,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.02995316764865704,
        "precision": 0.028079388981626877,
        "recall": 0.040513833992094864
      },
      {
        "accuracy": 0.08399209486166008,
        "f1": 0.03548266674974114,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.03548266674974114,
        "precision": 0.02835780506210107,
        "recall": 0.08399209486166008
      },
      {
        "accuracy": 0.043478260869565216,
        "f1": 0.031495588591256354,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.031495588591256354,
        "precision": 0.029419781428373426,
        "recall": 0.043478260869565216
      },
      {
        "accuracy": 0.07806324110671936,
        "f1": 0.0339660298378332,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.0339660298378332,
        "precision": 0.026348093694453508,
        "recall": 0.07806324110671936
      },
      {
        "accuracy": 0.043478260869565216,
        "f1": 0.031960028906106504,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.031960028906106504,
        "precision": 0.029988140219381305,
        "recall": 0.043478260869565216
      },
      {
        "accuracy": 0.07312252964426877,
        "f1": 0.03229586144833045,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.03229586144833045,
        "precision": 0.026367364111589586,
        "recall": 0.07312252964426877
      },
      {
        "accuracy": 0.0958498023715415,
        "f1": 0.08926059526580003,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.08926059526580003,
        "precision": 0.08749131487525003,
        "recall": 0.0958498023715415
      },
      {
        "accuracy": 0.13636363636363635,
        "f1": 0.06737562586827923,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.06737562586827923,
        "precision": 0.05370791557008177,
        "recall": 0.13636363636363635
      },
      {
        "accuracy": 0.05138339920948617,
        "f1": 0.04132976013968906,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.04132976013968906,
        "precision": 0.03894751708813785,
        "recall": 0.05138339920948617
      },
      {
        "accuracy": 0.07905138339920949,
        "f1": 0.03166311566528343,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.03166311566528343,
        "precision": 0.024989840782558045,
        "recall": 0.07905138339920949
      },
      {
        "accuracy": 0.017786561264822136,
        "f1": 0.012609685394686758,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.012609685394686758,
        "precision": 0.011831989872322764,
        "recall": 0.017786561264822136
      },
      {
        "accuracy": 0.05632411067193676,
        "f1": 0.02372156281375575,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.02372156281375575,
        "precision": 0.018274754316963387,
        "recall": 0.05632411067193676
      },
      {
        "accuracy": 0.012845849802371542,
        "f1": 0.008711281327689311,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.008711281327689311,
        "precision": 0.008165644386872008,
        "recall": 0.012845849802371542
      },
      {
        "accuracy": 0.044466403162055336,
        "f1": 0.01943084375342511,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.01943084375342511,
        "precision": 0.014938733037516928,
        "recall": 0.044466403162055336
      },
      {
        "accuracy": 0.00691699604743083,
        "f1": 0.005272903442239827,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.005272903442239827,
        "precision": 0.0049421190725538544,
        "recall": 0.00691699604743083
      },
      {
        "accuracy": 0.023715415019762844,
        "f1": 0.005676361097508397,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.005676361097508397,
        "precision": 0.003984357137867412,
        "recall": 0.023715415019762844
      },
      {
        "accuracy": 0.0266798418972332,
        "f1": 0.019937485780700537,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.019937485780700537,
        "precision": 0.018509571369131995,
        "recall": 0.0266798418972332
      },
      {
        "accuracy": 0.07015810276679842,
        "f1": 0.031140487259643045,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.031140487259643045,
        "precision": 0.0253450499536778,
        "recall": 0.07015810276679842
      },
      {
        "accuracy": 0.023715415019762844,
        "f1": 0.018706602268528803,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.018706602268528803,
        "precision": 0.018420315634681567,
        "recall": 0.023715415019762844
      },
      {
        "accuracy": 0.05039525691699605,
        "f1": 0.02088518992038232,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.02088518992038232,
        "precision": 0.017244295065783347,
        "recall": 0.05039525691699605
      },
      {
        "accuracy": 0.039525691699604744,
        "f1": 0.03359276867182005,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.03359276867182005,
        "precision": 0.03213730554977694,
        "recall": 0.039525691699604744
      },
      {
        "accuracy": 0.08893280632411067,
        "f1": 0.039625717955123964,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.039625717955123964,
        "precision": 0.03177220468975533,
        "recall": 0.08893280632411067
      },
      {
        "accuracy": 0.01383399209486166,
        "f1": 0.00982816968995805,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.00982816968995805,
        "precision": 0.009539481192294492,
        "recall": 0.01383399209486166
      },
      {
        "accuracy": 0.045454545454545456,
        "f1": 0.019682285995913224,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.019682285995913224,
        "precision": 0.015498858252847603,
        "recall": 0.045454545454545456
      },
      {
        "accuracy": 0.023715415019762844,
        "f1": 0.019422640138199592,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.019422640138199592,
        "precision": 0.018623465906074603,
        "recall": 0.023715415019762844
      },
      {
        "accuracy": 0.06818181818181818,
        "f1": 0.029168958088986285,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.029168958088986285,
        "precision": 0.023940798271265464,
        "recall": 0.06818181818181818
      },
      {
        "accuracy": 0.03359683794466403,
        "f1": 0.027860101541806268,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.027860101541806268,
        "precision": 0.02694577706741719,
        "recall": 0.03359683794466403
      },
      {
        "accuracy": 0.07509881422924901,
        "f1": 0.031497449631075175,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.031497449631075175,
        "precision": 0.024781861747495932,
        "recall": 0.07509881422924901
      },
      {
        "accuracy": 0.03359683794466403,
        "f1": 0.026915937323546017,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.026915937323546017,
        "precision": 0.02548605735049255,
        "recall": 0.03359683794466403
      },
      {
        "accuracy": 0.07509881422924901,
        "f1": 0.03617803010054385,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.03617803010054385,
        "precision": 0.029894907866935116,
        "recall": 0.07509881422924901
      },
      {
        "accuracy": 0.014822134387351778,
        "f1": 0.012133699633699634,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.012133699633699634,
        "precision": 0.0117090156220591,
        "recall": 0.014822134387351778
      },
      {
        "accuracy": 0.05434782608695652,
        "f1": 0.026796273026947415,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.026796273026947415,
        "precision": 0.02257451124620629,
        "recall": 0.05434782608695652
      },
      {
        "accuracy": 0.023715415019762844,
        "f1": 0.01846354377631605,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01846354377631605,
        "precision": 0.017694684622365852,
        "recall": 0.023715415019762844
      },
      {
        "accuracy": 0.06818181818181818,
        "f1": 0.03071806605225824,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.03071806605225824,
        "precision": 0.0253052096481131,
        "recall": 0.06818181818181818
      },
      {
        "accuracy": 0.014822134387351778,
        "f1": 0.011956301417549055,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.011956301417549055,
        "precision": 0.011414927033950365,
        "recall": 0.014822134387351778
      },
      {
        "accuracy": 0.022727272727272728,
        "f1": 0.0035913746506388445,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.0035913746506388445,
        "precision": 0.002108981972217291,
        "recall": 0.022727272727272728
      }
    ],
    "validation": [
      {
        "accuracy": 0.0160481444332999,
        "f1": 0.010964080855211429,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.010964080855211429,
        "precision": 0.009846357568218805,
        "recall": 0.0160481444332999
      },
      {
        "accuracy": 0.048144433299899696,
        "f1": 0.018412786205683308,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.018412786205683308,
        "precision": 0.013949630519964945,
        "recall": 0.048144433299899696
      },
      {
        "accuracy": 0.028084252758274825,
        "f1": 0.02182637421044951,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.02182637421044951,
        "precision": 0.020434212690983004,
        "recall": 0.028084252758274825
      },
      {
        "accuracy": 0.05115346038114343,
        "f1": 0.01810973589902381,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.01810973589902381,
        "precision": 0.013424890119296692,
        "recall": 0.05115346038114343
      },
      {
        "accuracy": 0.025075225677031094,
        "f1": 0.02027633665611624,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.02027633665611624,
        "precision": 0.01937423898066316,
        "recall": 0.025075225677031094
      },
      {
        "accuracy": 0.062186559679037114,
        "f1": 0.02770530092476698,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.02770530092476698,
        "precision": 0.022947414834073747,
        "recall": 0.062186559679037114
      },
      {
        "accuracy": 0.033099297893681046,
        "f1": 0.025233546030609948,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.025233546030609948,
        "precision": 0.02339147704072265,
        "recall": 0.033099297893681046
      },
      {
        "accuracy": 0.06720160481444333,
        "f1": 0.02887437177690118,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.02887437177690118,
        "precision": 0.023399879078328502,
        "recall": 0.06720160481444333
      },
      {
        "accuracy": 0.044132397191574725,
        "f1": 0.03330516176719544,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.03330516176719544,
        "precision": 0.030159159585746376,
        "recall": 0.044132397191574725
      },
      {
        "accuracy": 0.07522567703109329,
        "f1": 0.03037856333562712,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.03037856333562712,
        "precision": 0.023662925276397594,
        "recall": 0.07522567703109329
      },
      {
        "accuracy": 0.02106318956870612,
        "f1": 0.01889219093120875,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01889219093120875,
        "precision": 0.018389593817843632,
        "recall": 0.02106318956870612
      },
      {
        "accuracy": 0.058174523570712136,
        "f1": 0.021907306564858967,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.021907306564858967,
        "precision": 0.016682089879834373,
        "recall": 0.058174523570712136
      },
      {
        "accuracy": 0.037111334002006016,
        "f1": 0.032293111447370494,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.032293111447370494,
        "precision": 0.031133419429147734,
        "recall": 0.037111334002006016
      },
      {
        "accuracy": 0.06920762286860582,
        "f1": 0.02641347704389959,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.02641347704389959,
        "precision": 0.019790681891088974,
        "recall": 0.06920762286860582
      },
      {
        "accuracy": 0.07823470411233702,
        "f1": 0.0674154352673782,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.0674154352673782,
        "precision": 0.06376610006230143,
        "recall": 0.07823470411233702
      },
      {
        "accuracy": 0.09829488465396188,
        "f1": 0.03983048737986271,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.03983048737986271,
        "precision": 0.030236534383349827,
        "recall": 0.09829488465396188
      },
      {
        "accuracy": 0.033099297893681046,
        "f1": 0.027272496446295678,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.027272496446295678,
        "precision": 0.025923164194025954,
        "recall": 0.033099297893681046
      },
      {
        "accuracy": 0.07422266800401203,
        "f1": 0.030710745539683063,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.030710745539683063,
        "precision": 0.023604456436534093,
        "recall": 0.07422266800401203
      },
      {
        "accuracy": 0.02106318956870612,
        "f1": 0.018116440825745206,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.018116440825745206,
        "precision": 0.017514154485313866,
        "recall": 0.02106318956870612
      },
      {
        "accuracy": 0.055165496489468405,
        "f1": 0.02276174222798906,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.02276174222798906,
        "precision": 0.017627964240166435,
        "recall": 0.055165496489468405
      },
      {
        "accuracy": 0.031093279839518557,
        "f1": 0.025839566893451436,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.025839566893451436,
        "precision": 0.02425501613064301,
        "recall": 0.031093279839518557
      },
      {
        "accuracy": 0.07923771313941826,
        "f1": 0.03587745609317041,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.03587745609317041,
        "precision": 0.02944950953590881,
        "recall": 0.07923771313941826
      },
      {
        "accuracy": 0.03610832497492478,
        "f1": 0.031373690965369225,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.031373690965369225,
        "precision": 0.03012464521993108,
        "recall": 0.03610832497492478
      },
      {
        "accuracy": 0.07823470411233702,
        "f1": 0.03220644832785521,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.03220644832785521,
        "precision": 0.025368757563355,
        "recall": 0.07823470411233702
      },
      {
        "accuracy": 0.05115346038114343,
        "f1": 0.04319625543296556,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.04319625543296556,
        "precision": 0.03970244065529922,
        "recall": 0.05115346038114343
      },
      {
        "accuracy": 0.08124373119358075,
        "f1": 0.033002377750203996,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.033002377750203996,
        "precision": 0.025465964114899643,
        "recall": 0.08124373119358075
      },
      {
        "accuracy": 0.037111334002006016,
        "f1": 0.029847541554572758,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.029847541554572758,
        "precision": 0.02758557362227528,
        "recall": 0.037111334002006016
      },
      {
        "accuracy": 0.06118355065195587,
        "f1": 0.021461861520730596,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.021461861520730596,
        "precision": 0.01626236569201487,
        "recall": 0.06118355065195587
      },
      {
        "accuracy": 0.1464393179538616,
        "f1": 0.13519992229252956,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.13519992229252956,
        "precision": 0.13205391438266534,
        "recall": 0.1464393179538616
      },
      {
        "accuracy": 0.17051153460381144,
        "f1": 0.09081130580104214,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.09081130580104214,
        "precision": 0.07466958720989354,
        "recall": 0.17051153460381144
      },
      {
        "accuracy": 0.048144433299899696,
        "f1": 0.04244628342125847,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.04244628342125847,
        "precision": 0.04068271219696991,
        "recall": 0.048144433299899696
      },
      {
        "accuracy": 0.07221664994984955,
        "f1": 0.02624840329074924,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.02624840329074924,
        "precision": 0.020402314650792126,
        "recall": 0.07221664994984955
      },
      {
        "accuracy": 0.015045135406218655,
        "f1": 0.011198035647356915,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.011198035647356915,
        "precision": 0.010620509444293573,
        "recall": 0.015045135406218655
      },
      {
        "accuracy": 0.05717151454363089,
        "f1": 0.02624456143467234,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.02624456143467234,
        "precision": 0.021373339328085918,
        "recall": 0.05717151454363089
      },
      {
        "accuracy": 0.013039117352056168,
        "f1": 0.010443110971389816,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.010443110971389816,
        "precision": 0.010286773195949154,
        "recall": 0.013039117352056168
      },
      {
        "accuracy": 0.05917753259779338,
        "f1": 0.025059540746577402,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.025059540746577402,
        "precision": 0.020019689146928657,
        "recall": 0.05917753259779338
      },
      {
        "accuracy": 0.009027081243731194,
        "f1": 0.007358162642196562,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.007358162642196562,
        "precision": 0.007022446650295715,
        "recall": 0.009027081243731194
      },
      {
        "accuracy": 0.03811434302908726,
        "f1": 0.011782220711270826,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.011782220711270826,
        "precision": 0.00917457326443684,
        "recall": 0.03811434302908726
      },
      {
        "accuracy": 0.02106318956870612,
        "f1": 0.017281033965232902,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.017281033965232902,
        "precision": 0.016344195757435478,
        "recall": 0.02106318956870612
      },
      {
        "accuracy": 0.0641925777331996,
        "f1": 0.025716295689782263,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.025716295689782263,
        "precision": 0.019796968753258457,
        "recall": 0.0641925777331996
      },
      {
        "accuracy": 0.023069207622868605,
        "f1": 0.01826606430715864,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01826606430715864,
        "precision": 0.017335436638460867,
        "recall": 0.023069207622868605
      },
      {
        "accuracy": 0.05917753259779338,
        "f1": 0.027481633431060658,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.027481633431060658,
        "precision": 0.02182166781718528,
        "recall": 0.05917753259779338
      },
      {
        "accuracy": 0.03009027081243731,
        "f1": 0.023324923847593278,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.023324923847593278,
        "precision": 0.021542815566534056,
        "recall": 0.03009027081243731
      },
      {
        "accuracy": 0.08224674022066199,
        "f1": 0.0399763383750736,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.0399763383750736,
        "precision": 0.03304087043587953,
        "recall": 0.08224674022066199
      },
      {
        "accuracy": 0.013039117352056168,
        "f1": 0.010185624931073303,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.010185624931073303,
        "precision": 0.010113393150191644,
        "recall": 0.013039117352056168
      },
      {
        "accuracy": 0.048144433299899696,
        "f1": 0.020154141515720994,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.020154141515720994,
        "precision": 0.015930726770395146,
        "recall": 0.048144433299899696
      },
      {
        "accuracy": 0.03009027081243731,
        "f1": 0.026088039468177084,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.026088039468177084,
        "precision": 0.025080141272144092,
        "recall": 0.03009027081243731
      },
      {
        "accuracy": 0.06820461384152457,
        "f1": 0.028110574113033825,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.028110574113033825,
        "precision": 0.022672104831386507,
        "recall": 0.06820461384152457
      },
      {
        "accuracy": 0.0320962888665998,
        "f1": 0.02536044080028001,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.02536044080028001,
        "precision": 0.023607506148641003,
        "recall": 0.0320962888665998
      },
      {
        "accuracy": 0.07622868605817452,
        "f1": 0.03404579538839837,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.03404579538839837,
        "precision": 0.027639360653718872,
        "recall": 0.07622868605817452
      },
      {
        "accuracy": 0.03009027081243731,
        "f1": 0.022217312887127673,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.022217312887127673,
        "precision": 0.020671795376494023,
        "recall": 0.03009027081243731
      },
      {
        "accuracy": 0.0712136409227683,
        "f1": 0.030114322710761185,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.030114322710761185,
        "precision": 0.02375276964072763,
        "recall": 0.0712136409227683
      },
      {
        "accuracy": 0.012036108324974924,
        "f1": 0.009257375857175254,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.009257375857175254,
        "precision": 0.008864493022446334,
        "recall": 0.012036108324974924
      },
      {
        "accuracy": 0.05616850551654965,
        "f1": 0.023937227762781788,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.023937227762781788,
        "precision": 0.01942113976097978,
        "recall": 0.05616850551654965
      },
      {
        "accuracy": 0.0160481444332999,
        "f1": 0.013256566235503045,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.013256566235503045,
        "precision": 0.012869180252717728,
        "recall": 0.0160481444332999
      },
      {
        "accuracy": 0.062186559679037114,
        "f1": 0.029801943241472357,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.029801943241472357,
        "precision": 0.025157157522198808,
        "recall": 0.062186559679037114
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.008093992785052264,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.008093992785052264,
        "precision": 0.007774848937055068,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.02106318956870612,
        "f1": 0.002375722766852652,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.002375722766852652,
        "precision": 0.0013204607932692405,
        "recall": 0.02106318956870612
      }
    ]
  },
  "task_name": "IndicGenBenchFloresBitextMining"
}