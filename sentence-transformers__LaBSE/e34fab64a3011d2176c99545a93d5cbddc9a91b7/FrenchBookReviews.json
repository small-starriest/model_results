{
  "dataset_revision": "534725e03fec6f560dbe8166e8ae3825314a6290",
  "evaluation_time": 10.331290006637573,
  "kg_co2_emissions": 0.0015689315527062499,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.387158203125,
        "f1": 0.3308925732274577,
        "f1_weighted": 0.429446369914789,
        "hf_subset": "default",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.387158203125,
        "scores_per_experiment": [
          {
            "accuracy": 0.29541015625,
            "f1": 0.2666210851892818,
            "f1_weighted": 0.3531131251628923
          },
          {
            "accuracy": 0.45458984375,
            "f1": 0.37482943127177926,
            "f1_weighted": 0.4924569371758448
          },
          {
            "accuracy": 0.421875,
            "f1": 0.3426468591118981,
            "f1_weighted": 0.4620351049176617
          },
          {
            "accuracy": 0.40869140625,
            "f1": 0.3467654110179616,
            "f1_weighted": 0.45416078657831876
          },
          {
            "accuracy": 0.4189453125,
            "f1": 0.3312503335266004,
            "f1_weighted": 0.466403026500693
          },
          {
            "accuracy": 0.4189453125,
            "f1": 0.36759696832953054,
            "f1_weighted": 0.45947124864973765
          },
          {
            "accuracy": 0.41845703125,
            "f1": 0.34583714879391714,
            "f1_weighted": 0.45844210702675325
          },
          {
            "accuracy": 0.39208984375,
            "f1": 0.33888398603299913,
            "f1_weighted": 0.43011210130045097
          },
          {
            "accuracy": 0.35009765625,
            "f1": 0.31482057090105714,
            "f1_weighted": 0.39293593007280836
          },
          {
            "accuracy": 0.29248046875,
            "f1": 0.2796739380995516,
            "f1_weighted": 0.3253333317627295
          }
        ]
      }
    ]
  },
  "task_name": "FrenchBookReviews"
}