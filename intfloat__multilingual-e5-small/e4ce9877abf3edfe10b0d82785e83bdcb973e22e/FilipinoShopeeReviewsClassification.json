{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 17.08782196044922,
  "kg_co2_emissions": 0.0025455341468609014,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.33173828125,
        "f1": 0.31508751838408783,
        "f1_weighted": 0.315091035046404,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.33173828125,
        "scores_per_experiment": [
          {
            "accuracy": 0.322265625,
            "f1": 0.2986411811932542,
            "f1_weighted": 0.2986470954501241
          },
          {
            "accuracy": 0.32421875,
            "f1": 0.30759329864743057,
            "f1_weighted": 0.3075819464226733
          },
          {
            "accuracy": 0.375,
            "f1": 0.3523354135778072,
            "f1_weighted": 0.35232251609691495
          },
          {
            "accuracy": 0.35791015625,
            "f1": 0.3467853936725166,
            "f1_weighted": 0.34679023688320876
          },
          {
            "accuracy": 0.3203125,
            "f1": 0.3197596959070668,
            "f1_weighted": 0.3197906686125482
          },
          {
            "accuracy": 0.314453125,
            "f1": 0.2912066022124933,
            "f1_weighted": 0.2911993392793591
          },
          {
            "accuracy": 0.333984375,
            "f1": 0.33043808357055415,
            "f1_weighted": 0.33042264520161363
          },
          {
            "accuracy": 0.3623046875,
            "f1": 0.3486452072712051,
            "f1_weighted": 0.3486398678855798
          },
          {
            "accuracy": 0.3046875,
            "f1": 0.2742109050442612,
            "f1_weighted": 0.27420312699982924
          },
          {
            "accuracy": 0.30224609375,
            "f1": 0.28125940274428896,
            "f1_weighted": 0.281312907632189
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.332470703125,
        "f1": 0.3158377257081423,
        "f1_weighted": 0.31583422262154964,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.332470703125,
        "scores_per_experiment": [
          {
            "accuracy": 0.33447265625,
            "f1": 0.31371499410920434,
            "f1_weighted": 0.31370437049641503
          },
          {
            "accuracy": 0.31103515625,
            "f1": 0.2925959363062369,
            "f1_weighted": 0.29257548176047504
          },
          {
            "accuracy": 0.3662109375,
            "f1": 0.3450990792599338,
            "f1_weighted": 0.3450848073353937
          },
          {
            "accuracy": 0.3603515625,
            "f1": 0.3489249300621175,
            "f1_weighted": 0.3489308832680891
          },
          {
            "accuracy": 0.32470703125,
            "f1": 0.3228351107742813,
            "f1_weighted": 0.3228543667446435
          },
          {
            "accuracy": 0.32373046875,
            "f1": 0.29872420687636614,
            "f1_weighted": 0.2987154462607823
          },
          {
            "accuracy": 0.306640625,
            "f1": 0.30158913539194154,
            "f1_weighted": 0.30156686024106205
          },
          {
            "accuracy": 0.3759765625,
            "f1": 0.3644734576209879,
            "f1_weighted": 0.3644456526828989
          },
          {
            "accuracy": 0.3125,
            "f1": 0.28062400314798325,
            "f1_weighted": 0.28060852344482645
          },
          {
            "accuracy": 0.30908203125,
            "f1": 0.28979640353237063,
            "f1_weighted": 0.2898558339809101
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}