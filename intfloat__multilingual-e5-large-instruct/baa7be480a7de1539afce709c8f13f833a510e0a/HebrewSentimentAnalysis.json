{
  "dataset_revision": "952c9525954c1dac50d5f95945eb5585bb6464e7",
  "evaluation_time": 26.78744626045227,
  "kg_co2_emissions": 0.005195911528965387,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.7279296875,
        "f1": 0.5619518970853105,
        "f1_weighted": 0.7695951052578659,
        "hf_subset": "default",
        "languages": [
          "heb-Hebr"
        ],
        "main_score": 0.7279296875,
        "scores_per_experiment": [
          {
            "accuracy": 0.79345703125,
            "f1": 0.6234567593998199,
            "f1_weighted": 0.7979570691192828
          },
          {
            "accuracy": 0.7119140625,
            "f1": 0.5521601077382011,
            "f1_weighted": 0.7581142819413176
          },
          {
            "accuracy": 0.64697265625,
            "f1": 0.5263733569254558,
            "f1_weighted": 0.7255126354255986
          },
          {
            "accuracy": 0.734375,
            "f1": 0.5645548657689669,
            "f1_weighted": 0.7832493107185936
          },
          {
            "accuracy": 0.75341796875,
            "f1": 0.5724635791990462,
            "f1_weighted": 0.7882880384280502
          },
          {
            "accuracy": 0.6923828125,
            "f1": 0.5482364300007496,
            "f1_weighted": 0.7413937897003636
          },
          {
            "accuracy": 0.7578125,
            "f1": 0.578363605931005,
            "f1_weighted": 0.7967075447176446
          },
          {
            "accuracy": 0.6689453125,
            "f1": 0.5000934266631284,
            "f1_weighted": 0.7190304771508716
          },
          {
            "accuracy": 0.7626953125,
            "f1": 0.592402269043057,
            "f1_weighted": 0.7979926268270492
          },
          {
            "accuracy": 0.75732421875,
            "f1": 0.5614145701836749,
            "f1_weighted": 0.7877052785498866
          }
        ]
      }
    ]
  },
  "task_name": "HebrewSentimentAnalysis"
}