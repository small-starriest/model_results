{
  "dataset_revision": "557bf94ac6177cc442f42d0b09b6e4b76e8f47c9",
  "evaluation_time": 10.35999345779419,
  "kg_co2_emissions": 0.0015476542818353386,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.5728909952606636,
        "ap": 0.17865843697271105,
        "ap_weighted": 0.17865843697271105,
        "f1": 0.480965744106415,
        "f1_weighted": 0.6221790703698681,
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.5728909952606636,
        "scores_per_experiment": [
          {
            "accuracy": 0.5881516587677725,
            "ap": 0.18443552665562096,
            "ap_weighted": 0.18443552665562096,
            "f1": 0.5011913962663689,
            "f1_weighted": 0.6413543619326494
          },
          {
            "accuracy": 0.5151658767772512,
            "ap": 0.16831822107939973,
            "ap_weighted": 0.16831822107939973,
            "f1": 0.44944714651286943,
            "f1_weighted": 0.5774589393460959
          },
          {
            "accuracy": 0.566350710900474,
            "ap": 0.1966095778983113,
            "ap_weighted": 0.1966095778983113,
            "f1": 0.501507754108659,
            "f1_weighted": 0.6225024960855016
          },
          {
            "accuracy": 0.5772511848341232,
            "ap": 0.19066045803046971,
            "ap_weighted": 0.19066045803046971,
            "f1": 0.5020451358573431,
            "f1_weighted": 0.6322800011585963
          },
          {
            "accuracy": 0.5066350710900474,
            "ap": 0.18535836281208767,
            "ap_weighted": 0.18535836281208767,
            "f1": 0.4605039874382616,
            "f1_weighted": 0.566672769910767
          },
          {
            "accuracy": 0.5947867298578199,
            "ap": 0.1661989626811195,
            "ap_weighted": 0.1661989626811195,
            "f1": 0.4801037112476906,
            "f1_weighted": 0.644432557288441
          },
          {
            "accuracy": 0.4933649289099526,
            "ap": 0.17812254682117934,
            "ap_weighted": 0.17812254682117934,
            "f1": 0.44732443909528014,
            "f1_weighted": 0.5546766485153704
          },
          {
            "accuracy": 0.6672985781990521,
            "ap": 0.16970601768695823,
            "ap_weighted": 0.16970601768695823,
            "f1": 0.5082434182132067,
            "f1_weighted": 0.6964586908631238
          },
          {
            "accuracy": 0.7165876777251184,
            "ap": 0.16433667767626306,
            "ap_weighted": 0.16433667767626306,
            "f1": 0.5025789980777753,
            "f1_weighted": 0.7221543774847315
          },
          {
            "accuracy": 0.5033175355450237,
            "ap": 0.1828380183857009,
            "ap_weighted": 0.1828380183857009,
            "f1": 0.45671145424669596,
            "f1_weighted": 0.5637998611134037
          }
        ]
      }
    ]
  },
  "task_name": "TweetSarcasmClassification"
}