{
  "dataset_revision": "fb4f11a5bc68b99891852d20f1ec074be6289768",
  "evaluation_time": 15.179962396621704,
  "kg_co2_emissions": 0.002837179588717525,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.198193359375,
        "f1": 0.18093841428720564,
        "hf_subset": "default",
        "languages": [
          "por-Latn"
        ],
        "lrap": 0.810993787977431,
        "main_score": 0.198193359375,
        "scores_per_experiment": [
          {
            "accuracy": 0.14599609375,
            "f1": 0.18105549466163276,
            "lrap": 0.8339029947916667
          },
          {
            "accuracy": 0.1181640625,
            "f1": 0.18235646886292847,
            "lrap": 0.7953694661458344
          },
          {
            "accuracy": 0.1630859375,
            "f1": 0.1831334594946273,
            "lrap": 0.8361952039930554
          },
          {
            "accuracy": 0.23876953125,
            "f1": 0.17132343269692127,
            "lrap": 0.8294949001736116
          },
          {
            "accuracy": 0.2265625,
            "f1": 0.1726474177023216,
            "lrap": 0.7982516818576397
          },
          {
            "accuracy": 0.22705078125,
            "f1": 0.22146206871498886,
            "lrap": 0.8198649088541676
          },
          {
            "accuracy": 0.25048828125,
            "f1": 0.13901926357854738,
            "lrap": 0.7591959635416674
          },
          {
            "accuracy": 0.1396484375,
            "f1": 0.19168726917692394,
            "lrap": 0.8199869791666666
          },
          {
            "accuracy": 0.203125,
            "f1": 0.20475725566521275,
            "lrap": 0.8214111328125011
          },
          {
            "accuracy": 0.26904296875,
            "f1": 0.16194201231795216,
            "lrap": 0.7962646484375012
          }
        ]
      }
    ]
  },
  "task_name": "BrazilianToxicTweetsClassification"
}