{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 16.454058170318604,
  "kg_co2_emissions": 0.0025059583784027076,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.25380859375,
        "f1": 0.24922105072055842,
        "f1_weighted": 0.24922630944906193,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.25380859375,
        "scores_per_experiment": [
          {
            "accuracy": 0.2568359375,
            "f1": 0.2521411929896199,
            "f1_weighted": 0.2521609981440692
          },
          {
            "accuracy": 0.25439453125,
            "f1": 0.24424424748339907,
            "f1_weighted": 0.24425394955343094
          },
          {
            "accuracy": 0.2314453125,
            "f1": 0.23136372163739577,
            "f1_weighted": 0.23136398281609818
          },
          {
            "accuracy": 0.27294921875,
            "f1": 0.26957815295298043,
            "f1_weighted": 0.2695910310682301
          },
          {
            "accuracy": 0.2548828125,
            "f1": 0.25157519031595654,
            "f1_weighted": 0.2515520830199156
          },
          {
            "accuracy": 0.2373046875,
            "f1": 0.22418158199071453,
            "f1_weighted": 0.22419729483775286
          },
          {
            "accuracy": 0.232421875,
            "f1": 0.23224847185053613,
            "f1_weighted": 0.23221700191659084
          },
          {
            "accuracy": 0.27392578125,
            "f1": 0.26736007053619,
            "f1_weighted": 0.2674034907625533
          },
          {
            "accuracy": 0.28857421875,
            "f1": 0.28368199005023387,
            "f1_weighted": 0.28366832882569787
          },
          {
            "accuracy": 0.2353515625,
            "f1": 0.235835887398558,
            "f1_weighted": 0.2358549335462807
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.2552734375,
        "f1": 0.2510633602366802,
        "f1_weighted": 0.2510631231276468,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.2552734375,
        "scores_per_experiment": [
          {
            "accuracy": 0.2626953125,
            "f1": 0.255428228697211,
            "f1_weighted": 0.25544687279284
          },
          {
            "accuracy": 0.240234375,
            "f1": 0.23268483852224398,
            "f1_weighted": 0.23267660089136566
          },
          {
            "accuracy": 0.244140625,
            "f1": 0.24344964347496106,
            "f1_weighted": 0.24342713260449447
          },
          {
            "accuracy": 0.28857421875,
            "f1": 0.2857437937512494,
            "f1_weighted": 0.28574128624476003
          },
          {
            "accuracy": 0.240234375,
            "f1": 0.2380032621148594,
            "f1_weighted": 0.23798785230114183
          },
          {
            "accuracy": 0.2509765625,
            "f1": 0.24095683098074142,
            "f1_weighted": 0.24097620815330606
          },
          {
            "accuracy": 0.224609375,
            "f1": 0.22534488922081003,
            "f1_weighted": 0.22531153320133485
          },
          {
            "accuracy": 0.27685546875,
            "f1": 0.2701325487932098,
            "f1_weighted": 0.270172792464312
          },
          {
            "accuracy": 0.2802734375,
            "f1": 0.2758885633820435,
            "f1_weighted": 0.2758765159068263
          },
          {
            "accuracy": 0.244140625,
            "f1": 0.24300100342947192,
            "f1_weighted": 0.24301443671608647
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}