{
  "dataset_revision": "f9bd92144ed76200d6eb3ce73a8bd4eba9ffdc85",
  "evaluation_time": 71.53362655639648,
  "kg_co2_emissions": 0.010932734070995476,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.63464,
        "f1": 0.61026976621501,
        "f1_weighted": 0.6166319393715706,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.63464,
        "scores_per_experiment": [
          {
            "accuracy": 0.6224,
            "f1": 0.6036509745426626,
            "f1_weighted": 0.6100530527190469
          },
          {
            "accuracy": 0.6496,
            "f1": 0.6218850744476596,
            "f1_weighted": 0.632354015151628
          },
          {
            "accuracy": 0.6456,
            "f1": 0.6269801286365646,
            "f1_weighted": 0.6303135624053862
          },
          {
            "accuracy": 0.6548,
            "f1": 0.6277066596648244,
            "f1_weighted": 0.6354272870148406
          },
          {
            "accuracy": 0.636,
            "f1": 0.6077965434125207,
            "f1_weighted": 0.615909696972804
          },
          {
            "accuracy": 0.6396,
            "f1": 0.616750576825725,
            "f1_weighted": 0.6230225214922912
          },
          {
            "accuracy": 0.648,
            "f1": 0.6280578917584428,
            "f1_weighted": 0.6331816550120615
          },
          {
            "accuracy": 0.628,
            "f1": 0.5974419660241398,
            "f1_weighted": 0.6047894279967871
          },
          {
            "accuracy": 0.5932,
            "f1": 0.5596986824278092,
            "f1_weighted": 0.5637974821066872
          },
          {
            "accuracy": 0.6292,
            "f1": 0.6127291644097504,
            "f1_weighted": 0.6174706928441739
          }
        ]
      }
    ]
  },
  "task_name": "ArxivClassification"
}