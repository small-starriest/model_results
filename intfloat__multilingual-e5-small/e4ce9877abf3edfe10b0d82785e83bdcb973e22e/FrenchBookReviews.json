{
  "dataset_revision": "534725e03fec6f560dbe8166e8ae3825314a6290",
  "evaluation_time": 10.302908420562744,
  "kg_co2_emissions": 0.0015283548602103366,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.410302734375,
        "f1": 0.33688815239012737,
        "f1_weighted": 0.4490379288924746,
        "hf_subset": "default",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.410302734375,
        "scores_per_experiment": [
          {
            "accuracy": 0.3056640625,
            "f1": 0.27230217016882685,
            "f1_weighted": 0.360343970466258
          },
          {
            "accuracy": 0.4609375,
            "f1": 0.3683557094571761,
            "f1_weighted": 0.4995686865748261
          },
          {
            "accuracy": 0.46630859375,
            "f1": 0.3775286552993516,
            "f1_weighted": 0.5002983502051109
          },
          {
            "accuracy": 0.30712890625,
            "f1": 0.2868509967102756,
            "f1_weighted": 0.34598908037450404
          },
          {
            "accuracy": 0.5029296875,
            "f1": 0.3649920094379504,
            "f1_weighted": 0.5370754107189156
          },
          {
            "accuracy": 0.43603515625,
            "f1": 0.37699940689856876,
            "f1_weighted": 0.47546758945397294
          },
          {
            "accuracy": 0.470703125,
            "f1": 0.3360398370072459,
            "f1_weighted": 0.4960154146263846
          },
          {
            "accuracy": 0.40576171875,
            "f1": 0.34397419259203166,
            "f1_weighted": 0.4452324689362027
          },
          {
            "accuracy": 0.412109375,
            "f1": 0.3342358932295131,
            "f1_weighted": 0.4632845581237144
          },
          {
            "accuracy": 0.33544921875,
            "f1": 0.307602653100334,
            "f1_weighted": 0.36710375944485735
          }
        ]
      }
    ]
  },
  "task_name": "FrenchBookReviews"
}