{
  "dataset_revision": "f8f98e5c4d3059cf1a00c8eb3d70aa271423f636",
  "evaluation_time": 8.942882061004639,
  "kg_co2_emissions": 0.001318022193164451,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.5010256410256411,
        "ap": 0.8407045307719413,
        "ap_weighted": 0.8407045307719413,
        "f1": 0.42629331728566555,
        "f1_weighted": 0.5598484130319625,
        "hf_subset": "default",
        "languages": [
          "ita-Latn"
        ],
        "main_score": 0.5010256410256411,
        "scores_per_experiment": [
          {
            "accuracy": 0.6635897435897435,
            "ap": 0.8391363049999762,
            "ap_weighted": 0.8391363049999762,
            "f1": 0.4841935483870967,
            "f1_weighted": 0.6922931348221671
          },
          {
            "accuracy": 0.39076923076923076,
            "ap": 0.8388665014605058,
            "ap_weighted": 0.8388665014605058,
            "f1": 0.36750264291393275,
            "f1_weighted": 0.4504909536063594
          },
          {
            "accuracy": 0.4246153846153846,
            "ap": 0.8514670791336288,
            "ap_weighted": 0.8514670791336288,
            "f1": 0.39934111187371313,
            "f1_weighted": 0.4836308114671876
          },
          {
            "accuracy": 0.6041025641025641,
            "ap": 0.843059841986593,
            "ap_weighted": 0.843059841986593,
            "f1": 0.47814982029551406,
            "f1_weighted": 0.6535370439266255
          },
          {
            "accuracy": 0.5548717948717948,
            "ap": 0.8353727474312127,
            "ap_weighted": 0.8353727474312127,
            "f1": 0.44676439210107943,
            "f1_weighted": 0.6140674956928827
          },
          {
            "accuracy": 0.4594871794871795,
            "ap": 0.8378055676500902,
            "ap_weighted": 0.8378055676500902,
            "f1": 0.40781808060796537,
            "f1_weighted": 0.5274822783594786
          },
          {
            "accuracy": 0.4635897435897436,
            "ap": 0.8296837709177973,
            "ap_weighted": 0.8296837709177973,
            "f1": 0.3997358449254327,
            "f1_weighted": 0.5336683933817075
          },
          {
            "accuracy": 0.5476923076923077,
            "ap": 0.8497072629979883,
            "ap_weighted": 0.8497072629979883,
            "f1": 0.46668113739960926,
            "f1_weighted": 0.6088770599923193
          },
          {
            "accuracy": 0.4328205128205128,
            "ap": 0.8378080588907763,
            "ap_weighted": 0.8378080588907763,
            "f1": 0.3929064039408867,
            "f1_weighted": 0.4993972464317292
          },
          {
            "accuracy": 0.4687179487179487,
            "ap": 0.8441381722508439,
            "ap_weighted": 0.8441381722508439,
            "f1": 0.41984019041142473,
            "f1_weighted": 0.5350397126391686
          }
        ]
      }
    ]
  },
  "task_name": "Itacola"
}