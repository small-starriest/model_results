{
  "dataset_revision": "566be6449bb30b9b9f2b59173391647fe0ca3224",
  "evaluation_time": 9.761927366256714,
  "kg_co2_emissions": 0.0014843824241830446,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.410888671875,
        "f1": 0.39893306081390245,
        "f1_weighted": 0.41215679991294685,
        "hf_subset": "default",
        "languages": [
          "urd-Latn"
        ],
        "main_score": 0.39893306081390245,
        "scores_per_experiment": [
          {
            "accuracy": 0.4208984375,
            "f1": 0.4033904105714512,
            "f1_weighted": 0.4206685614060697
          },
          {
            "accuracy": 0.38916015625,
            "f1": 0.3831618411644658,
            "f1_weighted": 0.3853960457106417
          },
          {
            "accuracy": 0.45654296875,
            "f1": 0.431190812932386,
            "f1_weighted": 0.45563592942345577
          },
          {
            "accuracy": 0.40625,
            "f1": 0.3979294077742821,
            "f1_weighted": 0.4099824137833692
          },
          {
            "accuracy": 0.34228515625,
            "f1": 0.3402735706585658,
            "f1_weighted": 0.3424535592436784
          },
          {
            "accuracy": 0.435546875,
            "f1": 0.41880063260461475,
            "f1_weighted": 0.4365460380064873
          },
          {
            "accuracy": 0.4248046875,
            "f1": 0.4081090370074148,
            "f1_weighted": 0.4274832481144974
          },
          {
            "accuracy": 0.390625,
            "f1": 0.388945803707837,
            "f1_weighted": 0.39541223916551405
          },
          {
            "accuracy": 0.43798828125,
            "f1": 0.4145369943804278,
            "f1_weighted": 0.4383826839243672
          },
          {
            "accuracy": 0.40478515625,
            "f1": 0.40299209733757907,
            "f1_weighted": 0.40960728035138794
          }
        ]
      }
    ]
  },
  "task_name": "UrduRomanSentimentClassification"
}