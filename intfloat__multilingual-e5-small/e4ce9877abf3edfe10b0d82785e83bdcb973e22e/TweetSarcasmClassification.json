{
  "dataset_revision": "557bf94ac6177cc442f42d0b09b6e4b76e8f47c9",
  "evaluation_time": 9.778178691864014,
  "kg_co2_emissions": 0.0014405857282183217,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.6605687203791469,
        "ap": 0.21970513693163696,
        "ap_weighted": 0.21970513693163696,
        "f1": 0.5625616932329646,
        "f1_weighted": 0.7010294376214994,
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.6605687203791469,
        "scores_per_experiment": [
          {
            "accuracy": 0.6492890995260664,
            "ap": 0.2145594093173688,
            "ap_weighted": 0.2145594093173688,
            "f1": 0.5553087263613579,
            "f1_weighted": 0.6928882417158794
          },
          {
            "accuracy": 0.7241706161137441,
            "ap": 0.2194744537007055,
            "ap_weighted": 0.2194744537007055,
            "f1": 0.5883487152532418,
            "f1_weighted": 0.7474799852383187
          },
          {
            "accuracy": 0.6483412322274882,
            "ap": 0.22234581257625463,
            "ap_weighted": 0.22234581257625463,
            "f1": 0.5608033806711072,
            "f1_weighted": 0.6927606515947388
          },
          {
            "accuracy": 0.6497630331753554,
            "ap": 0.252550188462985,
            "ap_weighted": 0.252550188462985,
            "f1": 0.5805711724957507,
            "f1_weighted": 0.6952181295144657
          },
          {
            "accuracy": 0.6293838862559241,
            "ap": 0.22044159585806827,
            "ap_weighted": 0.22044159585806827,
            "f1": 0.550482149726315,
            "f1_weighted": 0.6772247581788546
          },
          {
            "accuracy": 0.6526066350710901,
            "ap": 0.1952148026634876,
            "ap_weighted": 0.1952148026634876,
            "f1": 0.5391298465891771,
            "f1_weighted": 0.693033418360253
          },
          {
            "accuracy": 0.6815165876777252,
            "ap": 0.2110523158641848,
            "ap_weighted": 0.2110523158641848,
            "f1": 0.5655882352941177,
            "f1_weighted": 0.7166141622525788
          },
          {
            "accuracy": 0.5976303317535545,
            "ap": 0.192840873018083,
            "ap_weighted": 0.192840873018083,
            "f1": 0.5136019262709373,
            "f1_weighted": 0.6496570411914806
          },
          {
            "accuracy": 0.7132701421800948,
            "ap": 0.23475441925306428,
            "ap_weighted": 0.23475441925306428,
            "f1": 0.5976758456434239,
            "f1_weighted": 0.7428075000042308
          },
          {
            "accuracy": 0.6597156398104266,
            "ap": 0.23381749860216788,
            "ap_weighted": 0.23381749860216788,
            "f1": 0.574106934024217,
            "f1_weighted": 0.7026104881641932
          }
        ]
      }
    ]
  },
  "task_name": "TweetSarcasmClassification"
}