{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 17.216701984405518,
  "kg_co2_emissions": 0.0025967462818643644,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.27177734375,
        "f1": 0.2667060811239397,
        "f1_weighted": 0.26669467767319455,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.27177734375,
        "scores_per_experiment": [
          {
            "accuracy": 0.27392578125,
            "f1": 0.27163945779252924,
            "f1_weighted": 0.2716088314611235
          },
          {
            "accuracy": 0.24169921875,
            "f1": 0.23365542212412174,
            "f1_weighted": 0.2336211946281135
          },
          {
            "accuracy": 0.2861328125,
            "f1": 0.2806586879380971,
            "f1_weighted": 0.28066468039163667
          },
          {
            "accuracy": 0.29541015625,
            "f1": 0.2905852090530373,
            "f1_weighted": 0.2905870547629539
          },
          {
            "accuracy": 0.2548828125,
            "f1": 0.24541282175121673,
            "f1_weighted": 0.2453644337837181
          },
          {
            "accuracy": 0.25537109375,
            "f1": 0.252223002933306,
            "f1_weighted": 0.2522331179132115
          },
          {
            "accuracy": 0.24658203125,
            "f1": 0.24583238377905287,
            "f1_weighted": 0.24580683595401343
          },
          {
            "accuracy": 0.298828125,
            "f1": 0.29706264977278407,
            "f1_weighted": 0.29707165276732955
          },
          {
            "accuracy": 0.29833984375,
            "f1": 0.29312091795506845,
            "f1_weighted": 0.29312237724744244
          },
          {
            "accuracy": 0.2666015625,
            "f1": 0.2568702581401833,
            "f1_weighted": 0.2568665978224024
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.27021484375,
        "f1": 0.264889928013106,
        "f1_weighted": 0.2648731072378415,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.27021484375,
        "scores_per_experiment": [
          {
            "accuracy": 0.28369140625,
            "f1": 0.28130466380716745,
            "f1_weighted": 0.28129311526899
          },
          {
            "accuracy": 0.2177734375,
            "f1": 0.209009237773165,
            "f1_weighted": 0.20897374928367246
          },
          {
            "accuracy": 0.28369140625,
            "f1": 0.27753720416546124,
            "f1_weighted": 0.2775182687742346
          },
          {
            "accuracy": 0.29052734375,
            "f1": 0.28339635891998327,
            "f1_weighted": 0.28338656915940696
          },
          {
            "accuracy": 0.2353515625,
            "f1": 0.2274956415705351,
            "f1_weighted": 0.22745410406206767
          },
          {
            "accuracy": 0.2763671875,
            "f1": 0.27445938589362684,
            "f1_weighted": 0.2744709028653045
          },
          {
            "accuracy": 0.22998046875,
            "f1": 0.2284592384446885,
            "f1_weighted": 0.22841958781108385
          },
          {
            "accuracy": 0.31201171875,
            "f1": 0.3112302159831179,
            "f1_weighted": 0.311222683691583
          },
          {
            "accuracy": 0.30517578125,
            "f1": 0.300373188796999,
            "f1_weighted": 0.30036784216589796
          },
          {
            "accuracy": 0.267578125,
            "f1": 0.25563414477631596,
            "f1_weighted": 0.25562424929617394
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}