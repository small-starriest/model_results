{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 3565.2437002658844,
  "kg_co2_emissions": 0.8074420454095772,
  "mteb_version": "1.12.75",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06766976547173176,
        "map": 0.08475589148844641,
        "mrr": 0.06766976547173176,
        "nAUC_map_diff1": -0.016514208407480323,
        "nAUC_map_max": 0.06363886464596247,
        "nAUC_map_std": 0.2951360683887377,
        "nAUC_mrr_diff1": -0.023861019558172894,
        "nAUC_mrr_max": 0.047089857126645165,
        "nAUC_mrr_std": 0.26203133052740524
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09127379660841825,
        "map": 0.11041163147886829,
        "mrr": 0.09127379660841825,
        "nAUC_map_diff1": 0.10779812910110616,
        "nAUC_map_max": -0.06288077127249944,
        "nAUC_map_std": 0.023208935623101574,
        "nAUC_mrr_diff1": 0.1101695651516696,
        "nAUC_mrr_max": -0.05456985220926848,
        "nAUC_mrr_std": 0.020333194287855603
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08214892818508952,
        "map": 0.09933865234677179,
        "mrr": 0.08214892818508952,
        "nAUC_map_diff1": 0.17570913681819314,
        "nAUC_map_max": 0.10602425124151772,
        "nAUC_map_std": 0.16598211209721803,
        "nAUC_mrr_diff1": 0.16032656616692542,
        "nAUC_mrr_max": 0.10294733535217904,
        "nAUC_mrr_std": 0.14360944298931086
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09538999442736702,
        "map": 0.11385457743056793,
        "mrr": 0.09538999442736702,
        "nAUC_map_diff1": 0.16974262584499314,
        "nAUC_map_max": 0.05045301214619524,
        "nAUC_map_std": 0.15544535369738707,
        "nAUC_mrr_diff1": 0.17557519661572835,
        "nAUC_mrr_max": 0.059741761592955966,
        "nAUC_mrr_std": 0.1338425574975047
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07119090532735571,
        "map": 0.08814764896254303,
        "mrr": 0.07119090532735571,
        "nAUC_map_diff1": 0.10258365271955015,
        "nAUC_map_max": 0.08277808187996893,
        "nAUC_map_std": 0.10888553914610459,
        "nAUC_mrr_diff1": 0.10568546800729654,
        "nAUC_mrr_max": 0.08023579878686818,
        "nAUC_mrr_std": 0.09602080946936226
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.11491740175445019,
        "map": 0.1325606242934333,
        "mrr": 0.11491740175445019,
        "nAUC_map_diff1": 0.17896904929609941,
        "nAUC_map_max": -0.007816497352229078,
        "nAUC_map_std": -0.04642433279632792,
        "nAUC_mrr_diff1": 0.18773440097581243,
        "nAUC_mrr_max": -0.007921301967775513,
        "nAUC_mrr_std": -0.05183416578713032
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}