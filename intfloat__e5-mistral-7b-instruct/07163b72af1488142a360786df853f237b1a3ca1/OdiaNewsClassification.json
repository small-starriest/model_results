{
  "dataset_revision": "ffb8a34c9637fb20256e8c7be02504d16af4bd6b",
  "evaluation_time": 32.45182132720947,
  "kg_co2_emissions": 0.007531915214773997,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.566552734375,
        "f1": 0.5615991706834288,
        "f1_weighted": 0.5628477607305455,
        "hf_subset": "default",
        "languages": [
          "ory-Orya"
        ],
        "main_score": 0.5615991706834288,
        "scores_per_experiment": [
          {
            "accuracy": 0.5322265625,
            "f1": 0.5254455264065371,
            "f1_weighted": 0.5257392633240412
          },
          {
            "accuracy": 0.5546875,
            "f1": 0.5614605997150535,
            "f1_weighted": 0.5563149522689099
          },
          {
            "accuracy": 0.60595703125,
            "f1": 0.5937826105144924,
            "f1_weighted": 0.6065106737637671
          },
          {
            "accuracy": 0.5947265625,
            "f1": 0.573856614284103,
            "f1_weighted": 0.5860427593444908
          },
          {
            "accuracy": 0.58251953125,
            "f1": 0.5761934833085666,
            "f1_weighted": 0.5824188089856217
          },
          {
            "accuracy": 0.552734375,
            "f1": 0.5542341248866607,
            "f1_weighted": 0.5507996206293588
          },
          {
            "accuracy": 0.50146484375,
            "f1": 0.5057127690315942,
            "f1_weighted": 0.4844788713874105
          },
          {
            "accuracy": 0.58642578125,
            "f1": 0.5851962660218675,
            "f1_weighted": 0.5850247552772838
          },
          {
            "accuracy": 0.57958984375,
            "f1": 0.579842528888067,
            "f1_weighted": 0.5795788757170426
          },
          {
            "accuracy": 0.5751953125,
            "f1": 0.5602671837773472,
            "f1_weighted": 0.5715690266075291
          }
        ]
      }
    ]
  },
  "task_name": "OdiaNewsClassification"
}