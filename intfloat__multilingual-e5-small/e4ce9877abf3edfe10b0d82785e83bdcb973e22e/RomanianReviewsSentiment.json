{
  "dataset_revision": "358bcc95aeddd5d07a4524ee416f03d993099b23",
  "evaluation_time": 10.720679998397827,
  "kg_co2_emissions": 0.0015953001812859175,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.45791015625,
        "f1": 0.3843808421722515,
        "f1_weighted": 0.48468390189423616,
        "hf_subset": "default",
        "languages": [
          "ron-Latn"
        ],
        "main_score": 0.45791015625,
        "scores_per_experiment": [
          {
            "accuracy": 0.4580078125,
            "f1": 0.38813110972288967,
            "f1_weighted": 0.5029923217406072
          },
          {
            "accuracy": 0.412109375,
            "f1": 0.35220721179315145,
            "f1_weighted": 0.43196350101000147
          },
          {
            "accuracy": 0.466796875,
            "f1": 0.3927688276526734,
            "f1_weighted": 0.49524741793604865
          },
          {
            "accuracy": 0.537109375,
            "f1": 0.44619603050312673,
            "f1_weighted": 0.5700756925509045
          },
          {
            "accuracy": 0.46728515625,
            "f1": 0.3732748185839235,
            "f1_weighted": 0.4723604268176938
          },
          {
            "accuracy": 0.46337890625,
            "f1": 0.3957730691655263,
            "f1_weighted": 0.5029232417001311
          },
          {
            "accuracy": 0.390625,
            "f1": 0.32957598691537304,
            "f1_weighted": 0.411146595945875
          },
          {
            "accuracy": 0.50341796875,
            "f1": 0.42379820684839653,
            "f1_weighted": 0.5314885618900675
          },
          {
            "accuracy": 0.45703125,
            "f1": 0.39663642300751617,
            "f1_weighted": 0.49313337929672635
          },
          {
            "accuracy": 0.42333984375,
            "f1": 0.3454467375299377,
            "f1_weighted": 0.43550788005430574
          }
        ]
      }
    ]
  },
  "task_name": "RomanianReviewsSentiment"
}