{
  "dataset_revision": "2e6fedf42c9c104e83dfd95c3a453721e683e244",
  "evaluation_time": 10.033515691757202,
  "kg_co2_emissions": 0.0014858617860140934,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.513427734375,
        "f1": 0.5089843914012989,
        "f1_weighted": 0.5089446441382584,
        "hf_subset": "default",
        "languages": [
          "ces-Latn"
        ],
        "main_score": 0.513427734375,
        "scores_per_experiment": [
          {
            "accuracy": 0.51904296875,
            "f1": 0.5196052851285899,
            "f1_weighted": 0.5195739514808282
          },
          {
            "accuracy": 0.49951171875,
            "f1": 0.49773989788870737,
            "f1_weighted": 0.4977125022559157
          },
          {
            "accuracy": 0.5322265625,
            "f1": 0.5298227268869359,
            "f1_weighted": 0.5297699515349239
          },
          {
            "accuracy": 0.48828125,
            "f1": 0.47960912261248495,
            "f1_weighted": 0.4795824299876011
          },
          {
            "accuracy": 0.5283203125,
            "f1": 0.5256669139616706,
            "f1_weighted": 0.5256310584025216
          },
          {
            "accuracy": 0.5,
            "f1": 0.4931972175662245,
            "f1_weighted": 0.4931501408220623
          },
          {
            "accuracy": 0.52783203125,
            "f1": 0.5251826841967305,
            "f1_weighted": 0.5251404670840938
          },
          {
            "accuracy": 0.486328125,
            "f1": 0.4883407781091244,
            "f1_weighted": 0.48832577236089336
          },
          {
            "accuracy": 0.5166015625,
            "f1": 0.5071386141479529,
            "f1_weighted": 0.5070819954625188
          },
          {
            "accuracy": 0.5361328125,
            "f1": 0.5235406735145677,
            "f1_weighted": 0.5234781719912247
          }
        ]
      }
    ]
  },
  "task_name": "CzechProductReviewSentimentClassification"
}