{
  "dataset_revision": "673a610d6d3dd91a547a0d57ae1b56f37ebbf6a1",
  "evaluation_time": 46.658061504364014,
  "kg_co2_emissions": 0.00902434254193957,
  "mteb_version": "1.14.12",
  "scores": {
    "test": [
      {
        "accuracy": 0.541943359375,
        "f1": 0.5234775878278427,
        "f1_weighted": 0.5235682040543119,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.541943359375,
        "scores_per_experiment": [
          {
            "accuracy": 0.552734375,
            "f1": 0.5298859836316697,
            "f1_weighted": 0.5300339746128562
          },
          {
            "accuracy": 0.529296875,
            "f1": 0.5143843376028505,
            "f1_weighted": 0.5144943320908703
          },
          {
            "accuracy": 0.5419921875,
            "f1": 0.5200882524013062,
            "f1_weighted": 0.5202430165618488
          },
          {
            "accuracy": 0.56884765625,
            "f1": 0.5517252540077219,
            "f1_weighted": 0.5517981588504722
          },
          {
            "accuracy": 0.52734375,
            "f1": 0.514150464464438,
            "f1_weighted": 0.5141455957093777
          },
          {
            "accuracy": 0.5205078125,
            "f1": 0.49527956287197655,
            "f1_weighted": 0.4953196865529094
          },
          {
            "accuracy": 0.5576171875,
            "f1": 0.5413019589529686,
            "f1_weighted": 0.54146598580155
          },
          {
            "accuracy": 0.5419921875,
            "f1": 0.5238386152433218,
            "f1_weighted": 0.5239493399817611
          },
          {
            "accuracy": 0.54736328125,
            "f1": 0.5315963041223117,
            "f1_weighted": 0.5316634645618643
          },
          {
            "accuracy": 0.53173828125,
            "f1": 0.5125251449798609,
            "f1_weighted": 0.5125684858196081
          }
        ]
      }
    ]
  },
  "task_name": "RuSciBenchGRNTIClassification"
}