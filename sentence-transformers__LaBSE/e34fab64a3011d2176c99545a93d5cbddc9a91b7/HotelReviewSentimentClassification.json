{
  "dataset_revision": "b108d2c32ee4e1f4176ea233e1a5ac17bceb9ef9",
  "evaluation_time": 10.046301364898682,
  "kg_co2_emissions": 0.0015152091668307684,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.4986328125,
        "f1": 0.4872736223691799,
        "f1_weighted": 0.5000605712088916,
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.4986328125,
        "scores_per_experiment": [
          {
            "accuracy": 0.470703125,
            "f1": 0.4691256854862549,
            "f1_weighted": 0.4780663341784539
          },
          {
            "accuracy": 0.48291015625,
            "f1": 0.4725173356486466,
            "f1_weighted": 0.4897904099540932
          },
          {
            "accuracy": 0.50927734375,
            "f1": 0.5051447024800028,
            "f1_weighted": 0.5137664582267446
          },
          {
            "accuracy": 0.52294921875,
            "f1": 0.48735694791410206,
            "f1_weighted": 0.5124816358671027
          },
          {
            "accuracy": 0.5029296875,
            "f1": 0.5014348579586796,
            "f1_weighted": 0.5031318558741464
          },
          {
            "accuracy": 0.51220703125,
            "f1": 0.5009364390609554,
            "f1_weighted": 0.5147116587519412
          },
          {
            "accuracy": 0.4453125,
            "f1": 0.4381896719924164,
            "f1_weighted": 0.4501366274979903
          },
          {
            "accuracy": 0.52783203125,
            "f1": 0.5158350954615437,
            "f1_weighted": 0.5235906582514508
          },
          {
            "accuracy": 0.47509765625,
            "f1": 0.4642658316010949,
            "f1_weighted": 0.47392342255455977
          },
          {
            "accuracy": 0.537109375,
            "f1": 0.5179296560881027,
            "f1_weighted": 0.5410066509324336
          }
        ]
      }
    ]
  },
  "task_name": "HotelReviewSentimentClassification"
}