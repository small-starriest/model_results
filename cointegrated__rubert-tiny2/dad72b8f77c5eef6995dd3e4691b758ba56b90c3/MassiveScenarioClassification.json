{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 4.035830020904541,
  "kg_co2_emissions": 0.000587099088125526,
  "mteb_version": "1.14.12",
  "scores": {
    "test": [
      {
        "accuracy": 0.5914929388029588,
        "f1": 0.5824477270449794,
        "f1_weighted": 0.5822649130658195,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.5914929388029588,
        "scores_per_experiment": [
          {
            "accuracy": 0.6156691324815063,
            "f1": 0.6057126602192426,
            "f1_weighted": 0.6082190670086984
          },
          {
            "accuracy": 0.6314727639542703,
            "f1": 0.6131365281094968,
            "f1_weighted": 0.6231487741929879
          },
          {
            "accuracy": 0.6032279757901816,
            "f1": 0.5863182614163409,
            "f1_weighted": 0.5994598706495807
          },
          {
            "accuracy": 0.6220578345662408,
            "f1": 0.6128213331582992,
            "f1_weighted": 0.6162279797533736
          },
          {
            "accuracy": 0.5904505716207128,
            "f1": 0.5750374879500483,
            "f1_weighted": 0.5724870132616332
          },
          {
            "accuracy": 0.5383322125084062,
            "f1": 0.5251276166163321,
            "f1_weighted": 0.5142619355708181
          },
          {
            "accuracy": 0.5951580363147276,
            "f1": 0.5872230112506944,
            "f1_weighted": 0.5882660095185117
          },
          {
            "accuracy": 0.5911230665770006,
            "f1": 0.5912659030023318,
            "f1_weighted": 0.5806823782812824
          },
          {
            "accuracy": 0.5464021519838601,
            "f1": 0.5458934927907911,
            "f1_weighted": 0.5379198310244121
          },
          {
            "accuracy": 0.5810356422326832,
            "f1": 0.5819409759362163,
            "f1_weighted": 0.5819762713968972
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6,
        "f1": 0.5924225888067995,
        "f1_weighted": 0.5909557754030739,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6,
        "scores_per_experiment": [
          {
            "accuracy": 0.6355140186915887,
            "f1": 0.6309443836432245,
            "f1_weighted": 0.6309188451586382
          },
          {
            "accuracy": 0.6296114117068372,
            "f1": 0.615183564246857,
            "f1_weighted": 0.621699623256637
          },
          {
            "accuracy": 0.6040334481062469,
            "f1": 0.5840633905409499,
            "f1_weighted": 0.6019984653898123
          },
          {
            "accuracy": 0.617806197737334,
            "f1": 0.6077510888288189,
            "f1_weighted": 0.6086807891710506
          },
          {
            "accuracy": 0.6060009837678307,
            "f1": 0.5961984200422582,
            "f1_weighted": 0.5913808122265316
          },
          {
            "accuracy": 0.5405804230201673,
            "f1": 0.5409732192842439,
            "f1_weighted": 0.5186007179246986
          },
          {
            "accuracy": 0.6045253320216429,
            "f1": 0.5980539086572263,
            "f1_weighted": 0.5988265913712387
          },
          {
            "accuracy": 0.602065912444663,
            "f1": 0.5980269249727654,
            "f1_weighted": 0.5885218838609272
          },
          {
            "accuracy": 0.5469749139203148,
            "f1": 0.549192100137241,
            "f1_weighted": 0.5325882288940329
          },
          {
            "accuracy": 0.6128873585833743,
            "f1": 0.60383888771441,
            "f1_weighted": 0.6163417967771718
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}