{
  "dataset_revision": "534725e03fec6f560dbe8166e8ae3825314a6290",
  "evaluation_time": 9.411373615264893,
  "kg_co2_emissions": 0.0013925108316536496,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.329541015625,
        "f1": 0.2892707503571388,
        "f1_weighted": 0.3644643782316184,
        "hf_subset": "default",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.329541015625,
        "scores_per_experiment": [
          {
            "accuracy": 0.3173828125,
            "f1": 0.27790474854014113,
            "f1_weighted": 0.3621278471296444
          },
          {
            "accuracy": 0.35009765625,
            "f1": 0.31309203493021504,
            "f1_weighted": 0.38528171479692735
          },
          {
            "accuracy": 0.39111328125,
            "f1": 0.3035564542565676,
            "f1_weighted": 0.4361730151481422
          },
          {
            "accuracy": 0.2568359375,
            "f1": 0.24718852962427232,
            "f1_weighted": 0.25296193540453865
          },
          {
            "accuracy": 0.3203125,
            "f1": 0.2884404528506641,
            "f1_weighted": 0.3416436701407636
          },
          {
            "accuracy": 0.3544921875,
            "f1": 0.3049017344671362,
            "f1_weighted": 0.40186805573958256
          },
          {
            "accuracy": 0.3759765625,
            "f1": 0.30461873001103074,
            "f1_weighted": 0.42137077506221554
          },
          {
            "accuracy": 0.31494140625,
            "f1": 0.2868847392168062,
            "f1_weighted": 0.34378230123730585
          },
          {
            "accuracy": 0.30810546875,
            "f1": 0.28349676000237767,
            "f1_weighted": 0.35148129557317676
          },
          {
            "accuracy": 0.30615234375,
            "f1": 0.2826233196721768,
            "f1_weighted": 0.34795317208388715
          }
        ]
      }
    ]
  },
  "task_name": "FrenchBookReviews"
}