{
  "dataset_revision": "7b58f24536063837d644aab9a023c62199b2a612",
  "evaluation_time": 0.6771695613861084,
  "kg_co2_emissions": 0.00011293090997426733,
  "mteb_version": "1.12.75",
  "scores": {
    "dev": [
      {
        "cosine_accuracy": 0.6286644951140065,
        "cosine_accuracy_threshold": 0.6105839014053345,
        "cosine_ap": 0.6456662563306175,
        "cosine_f1": 0.6935483870967742,
        "cosine_f1_threshold": 0.5402176380157471,
        "cosine_precision": 0.589041095890411,
        "cosine_recall": 0.8431372549019608,
        "dot_accuracy": 0.6254071661237784,
        "dot_accuracy_threshold": 3.4214258193969727,
        "dot_ap": 0.6305309702444627,
        "dot_f1": 0.6811989100817439,
        "dot_f1_threshold": 2.819490432739258,
        "dot_precision": 0.5841121495327103,
        "dot_recall": 0.8169934640522876,
        "euclidean_accuracy": 0.6026058631921825,
        "euclidean_accuracy_threshold": 1.9719446897506714,
        "euclidean_ap": 0.6156720506879636,
        "euclidean_f1": 0.684863523573201,
        "euclidean_f1_threshold": 2.387511730194092,
        "euclidean_precision": 0.552,
        "euclidean_recall": 0.9019607843137255,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6456662563306175,
        "manhattan_accuracy": 0.6058631921824105,
        "manhattan_accuracy_threshold": 42.25822830200195,
        "manhattan_ap": 0.6142907551186613,
        "manhattan_f1": 0.6763285024154588,
        "manhattan_f1_threshold": 53.34039306640625,
        "manhattan_precision": 0.5363984674329502,
        "manhattan_recall": 0.9150326797385621,
        "max_ap": 0.6456662563306175,
        "max_f1": 0.6935483870967742,
        "max_precision": 0.589041095890411,
        "max_recall": 0.9150326797385621,
        "similarity_accuracy": 0.6286644951140065,
        "similarity_accuracy_threshold": 0.6105839014053345,
        "similarity_ap": 0.6456662563306175,
        "similarity_f1": 0.6935483870967742,
        "similarity_f1_threshold": 0.5402176380157471,
        "similarity_precision": 0.589041095890411,
        "similarity_recall": 0.8431372549019608
      }
    ]
  },
  "task_name": "TERRa"
}