{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 29.525464296340942,
  "kg_co2_emissions": 0.006848618319968377,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.6529296875,
        "ap": 0.6018879744044455,
        "ap_weighted": 0.6018879744044455,
        "f1": 0.6508322137568193,
        "f1_weighted": 0.6508322137568193,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6529296875,
        "scores_per_experiment": [
          {
            "accuracy": 0.69189453125,
            "ap": 0.6335417575398804,
            "ap_weighted": 0.6335417575398804,
            "f1": 0.6918621328355266,
            "f1_weighted": 0.6918621328355266
          },
          {
            "accuracy": 0.6953125,
            "ap": 0.636103592519685,
            "ap_weighted": 0.636103592519685,
            "f1": 0.6953078507667658,
            "f1_weighted": 0.6953078507667658
          },
          {
            "accuracy": 0.6875,
            "ap": 0.6408704188481675,
            "ap_weighted": 0.6408704188481675,
            "f1": 0.6823809025314149,
            "f1_weighted": 0.6823809025314149
          },
          {
            "accuracy": 0.57080078125,
            "ap": 0.5406116156408629,
            "ap_weighted": 0.5406116156408629,
            "f1": 0.5706450822759013,
            "f1_weighted": 0.5706450822759013
          },
          {
            "accuracy": 0.65673828125,
            "ap": 0.6045465538826743,
            "ap_weighted": 0.6045465538826743,
            "f1": 0.6564131507385448,
            "f1_weighted": 0.6564131507385448
          },
          {
            "accuracy": 0.62353515625,
            "ap": 0.576038991152968,
            "ap_weighted": 0.576038991152968,
            "f1": 0.6230821507267508,
            "f1_weighted": 0.6230821507267508
          },
          {
            "accuracy": 0.669921875,
            "ap": 0.6153165907340863,
            "ap_weighted": 0.6153165907340863,
            "f1": 0.669725015768867,
            "f1_weighted": 0.669725015768867
          },
          {
            "accuracy": 0.6640625,
            "ap": 0.601171875,
            "ap_weighted": 0.601171875,
            "f1": 0.6496052966641201,
            "f1_weighted": 0.6496052966641201
          },
          {
            "accuracy": 0.6240234375,
            "ap": 0.5776999424178286,
            "ap_weighted": 0.5776999424178286,
            "f1": 0.6239875781610642,
            "f1_weighted": 0.6239875781610642
          },
          {
            "accuracy": 0.6455078125,
            "ap": 0.5929784063083022,
            "ap_weighted": 0.5929784063083022,
            "f1": 0.6453129770992366,
            "f1_weighted": 0.6453129770992366
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}