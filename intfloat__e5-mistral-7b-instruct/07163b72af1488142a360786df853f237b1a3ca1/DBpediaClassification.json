{
  "dataset_revision": "9abd46cf7fc8b4c64290f26993c540b92aa145ac",
  "evaluation_time": 32.08492374420166,
  "kg_co2_emissions": 0.007430315720658013,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.90859375,
        "f1": 0.9032777764644196,
        "f1_weighted": 0.9032971822805285,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.90859375,
        "scores_per_experiment": [
          {
            "accuracy": 0.923828125,
            "f1": 0.9208851037689781,
            "f1_weighted": 0.9209020376705205
          },
          {
            "accuracy": 0.9111328125,
            "f1": 0.9046158301843479,
            "f1_weighted": 0.9046253198944963
          },
          {
            "accuracy": 0.90625,
            "f1": 0.9033798890548682,
            "f1_weighted": 0.9033914675230065
          },
          {
            "accuracy": 0.90234375,
            "f1": 0.8939128747369284,
            "f1_weighted": 0.893943719471842
          },
          {
            "accuracy": 0.92138671875,
            "f1": 0.9175791274995204,
            "f1_weighted": 0.9176231780966315
          },
          {
            "accuracy": 0.9189453125,
            "f1": 0.915476185344905,
            "f1_weighted": 0.915501919076057
          },
          {
            "accuracy": 0.9189453125,
            "f1": 0.9147543997964235,
            "f1_weighted": 0.9147567052483605
          },
          {
            "accuracy": 0.90380859375,
            "f1": 0.898867151677887,
            "f1_weighted": 0.8988706662608321
          },
          {
            "accuracy": 0.91162109375,
            "f1": 0.9069938248792183,
            "f1_weighted": 0.9069537702788986
          },
          {
            "accuracy": 0.86767578125,
            "f1": 0.8563133777011185,
            "f1_weighted": 0.8564030392846392
          }
        ]
      }
    ]
  },
  "task_name": "DBpediaClassification"
}