{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "evaluation_time": 9.486567974090576,
  "kg_co2_emissions": 0.001408033318424228,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.5095703125,
        "f1": 0.4606969890595208,
        "f1_weighted": 0.5372866419506753,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ],
        "main_score": 0.5095703125,
        "scores_per_experiment": [
          {
            "accuracy": 0.55126953125,
            "f1": 0.4824277674978505,
            "f1_weighted": 0.5671731781430941
          },
          {
            "accuracy": 0.59423828125,
            "f1": 0.5280117001407735,
            "f1_weighted": 0.6193651019217803
          },
          {
            "accuracy": 0.48583984375,
            "f1": 0.4616238641391813,
            "f1_weighted": 0.5316193672371644
          },
          {
            "accuracy": 0.4013671875,
            "f1": 0.3727780156351585,
            "f1_weighted": 0.43486956217145945
          },
          {
            "accuracy": 0.51513671875,
            "f1": 0.46487864578749477,
            "f1_weighted": 0.547990239906343
          },
          {
            "accuracy": 0.48828125,
            "f1": 0.4233364471978896,
            "f1_weighted": 0.5041177181043497
          },
          {
            "accuracy": 0.54345703125,
            "f1": 0.48252983113664233,
            "f1_weighted": 0.5701175967296808
          },
          {
            "accuracy": 0.57470703125,
            "f1": 0.5127925473135307,
            "f1_weighted": 0.6029844015515784
          },
          {
            "accuracy": 0.42041015625,
            "f1": 0.403409534896644,
            "f1_weighted": 0.44518913194627535
          },
          {
            "accuracy": 0.52099609375,
            "f1": 0.4751815368500423,
            "f1_weighted": 0.5494401217950282
          }
        ]
      }
    ]
  },
  "task_name": "SweRecClassification"
}