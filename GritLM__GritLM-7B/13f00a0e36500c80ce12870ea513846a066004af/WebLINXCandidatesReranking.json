{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 6570.738064527512,
  "kg_co2_emissions": 2.90782983558977,
  "mteb_version": "1.12.75",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07667458088104155,
        "map": 0.09221920611359719,
        "mrr": 0.07667458088104155,
        "nAUC_map_diff1": 0.15222907410328837,
        "nAUC_map_max": 0.12006054944658211,
        "nAUC_map_std": 0.2390559825105845,
        "nAUC_mrr_diff1": 0.1408871963994374,
        "nAUC_mrr_max": 0.10009066285772748,
        "nAUC_mrr_std": 0.20078985045304268
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.10021205458044351,
        "map": 0.1192488862135072,
        "mrr": 0.10021205458044351,
        "nAUC_map_diff1": 0.19827012899590282,
        "nAUC_map_max": 0.07025766162914557,
        "nAUC_map_std": 0.09507759530726478,
        "nAUC_mrr_diff1": 0.2073434494844895,
        "nAUC_mrr_max": 0.07794158070559018,
        "nAUC_mrr_std": 0.08912914538213096
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.11344901428350665,
        "map": 0.12927941089251643,
        "mrr": 0.11344901428350665,
        "nAUC_map_diff1": 0.1007894187895389,
        "nAUC_map_max": 0.056399772700820416,
        "nAUC_map_std": 0.11365451396626146,
        "nAUC_mrr_diff1": 0.10703914625802764,
        "nAUC_mrr_max": 0.06840773583193943,
        "nAUC_mrr_std": 0.09987839722190134
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.1073886821621816,
        "map": 0.12486949406885911,
        "mrr": 0.1073886821621816,
        "nAUC_map_diff1": 0.09373125375773152,
        "nAUC_map_max": 0.07752600989243064,
        "nAUC_map_std": 0.1462866262558576,
        "nAUC_mrr_diff1": 0.10146856012466514,
        "nAUC_mrr_max": 0.08211943720058204,
        "nAUC_mrr_std": 0.12778784016484113
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09378534068419565,
        "map": 0.10893396110452624,
        "mrr": 0.09378534068419565,
        "nAUC_map_diff1": 0.16258246056254338,
        "nAUC_map_max": 0.07728154918568243,
        "nAUC_map_std": 0.11109504135071498,
        "nAUC_mrr_diff1": 0.1705258011862934,
        "nAUC_mrr_max": 0.08241486185463386,
        "nAUC_mrr_std": 0.1012302413072845
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.13675225406585897,
        "map": 0.15242580439748837,
        "mrr": 0.13675225406585897,
        "nAUC_map_diff1": 0.17986369526349397,
        "nAUC_map_max": 0.008119654240451938,
        "nAUC_map_std": -0.05292547282682295,
        "nAUC_mrr_diff1": 0.1794212406667878,
        "nAUC_mrr_max": 0.005073857140183718,
        "nAUC_mrr_std": -0.05701721252153113
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}