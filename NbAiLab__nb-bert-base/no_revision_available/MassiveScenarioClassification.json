{
  "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
  "mteb_dataset_name": "MassiveScenarioClassification",
  "mteb_version": "1.0.3.dev0",
  "test": {
    "da": {
      "accuracy": 0.6192669804976463,
      "accuracy_stderr": 0.020088387922951534,
      "f1": 0.6079566761576601,
      "f1_stderr": 0.018480298509820116,
      "main_score": 0.6192669804976463
    },
    "evaluation_time": 99.03,
    "nb": {
      "accuracy": 0.6731002017484868,
      "accuracy_stderr": 0.016653451974664585,
      "f1": 0.6660561469371957,
      "f1_stderr": 0.0156676777743156,
      "main_score": 0.6731002017484868
    },
    "sv": {
      "accuracy": 0.5537323470073974,
      "accuracy_stderr": 0.014555127617509366,
      "f1": 0.5385609530945743,
      "f1_stderr": 0.013419274514718067,
      "main_score": 0.5537323470073974
    }
  },
  "validation": {
    "da": {
      "accuracy": 0.613182488932612,
      "accuracy_stderr": 0.01927808469826446,
      "f1": 0.6068754154449585,
      "f1_stderr": 0.017148773636872613,
      "main_score": 0.613182488932612
    },
    "evaluation_time": 78.54,
    "nb": {
      "accuracy": 0.6802262666010821,
      "accuracy_stderr": 0.018345951273240263,
      "f1": 0.6779922983543948,
      "f1_stderr": 0.01992550609394551,
      "main_score": 0.6802262666010821
    },
    "sv": {
      "accuracy": 0.55976389572061,
      "accuracy_stderr": 0.014692433401931186,
      "f1": 0.5516808820907213,
      "f1_stderr": 0.016722607876170362,
      "main_score": 0.55976389572061
    }
  }
}