{
  "dataset_revision": "c18a4f81a47ae6fa079fe9d32db288ddde38451d",
  "evaluation_time": 21.05716109275818,
  "kg_co2_emissions": 0.0032881570322340443,
  "mteb_version": "1.12.75",
  "scores": {
    "validation": [
      {
        "accuracy": 0.016891891891891893,
        "f1": 0.010782758913423568,
        "hf_subset": "ar-en",
        "languages": [
          "ara-Arab",
          "eng-Latn"
        ],
        "main_score": 0.010782758913423568,
        "precision": 0.009535265785265784,
        "recall": 0.016891891891891893
      },
      {
        "accuracy": 0.33783783783783783,
        "f1": 0.26325710480122244,
        "hf_subset": "de-en",
        "languages": [
          "deu-Latn",
          "eng-Latn"
        ],
        "main_score": 0.26325710480122244,
        "precision": 0.23833227739477741,
        "recall": 0.33783783783783783
      },
      {
        "accuracy": 0.02927927927927928,
        "f1": 0.01203459436844451,
        "hf_subset": "en-ar",
        "languages": [
          "eng-Latn",
          "ara-Arab"
        ],
        "main_score": 0.01203459436844451,
        "precision": 0.009674391199523794,
        "recall": 0.02927927927927928
      },
      {
        "accuracy": 0.2939189189189189,
        "f1": 0.2157280189538254,
        "hf_subset": "en-de",
        "languages": [
          "eng-Latn",
          "deu-Latn"
        ],
        "main_score": 0.2157280189538254,
        "precision": 0.19214605112631428,
        "recall": 0.2939189189189189
      },
      {
        "accuracy": 0.451685393258427,
        "f1": 0.3679773616881837,
        "hf_subset": "en-fr",
        "languages": [
          "eng-Latn",
          "fra-Latn"
        ],
        "main_score": 0.3679773616881837,
        "precision": 0.3417162632843399,
        "recall": 0.451685393258427
      },
      {
        "accuracy": 0.30570505920344454,
        "f1": 0.23024805748955687,
        "hf_subset": "en-it",
        "languages": [
          "eng-Latn",
          "ita-Latn"
        ],
        "main_score": 0.23024805748955687,
        "precision": 0.20914926972040881,
        "recall": 0.30570505920344454
      },
      {
        "accuracy": 0.060849598163030996,
        "f1": 0.03233677565615163,
        "hf_subset": "en-ja",
        "languages": [
          "eng-Latn",
          "jpn-Jpan"
        ],
        "main_score": 0.03233677565615163,
        "precision": 0.026889262351374864,
        "recall": 0.060849598163030996
      },
      {
        "accuracy": 0.04209328782707622,
        "f1": 0.017696569838586265,
        "hf_subset": "en-ko",
        "languages": [
          "eng-Latn",
          "kor-Hang"
        ],
        "main_score": 0.017696569838586265,
        "precision": 0.014198388955624345,
        "recall": 0.04209328782707622
      },
      {
        "accuracy": 0.27517447657028915,
        "f1": 0.20094074540886542,
        "hf_subset": "en-nl",
        "languages": [
          "eng-Latn",
          "nld-Latn"
        ],
        "main_score": 0.20094074540886542,
        "precision": 0.17889554737237495,
        "recall": 0.27517447657028915
      },
      {
        "accuracy": 0.2647702407002188,
        "f1": 0.19082972945519605,
        "hf_subset": "en-ro",
        "languages": [
          "eng-Latn",
          "ron-Latn"
        ],
        "main_score": 0.19082972945519605,
        "precision": 0.1697607487323594,
        "recall": 0.2647702407002188
      },
      {
        "accuracy": 0.06598407281001138,
        "f1": 0.03474874898141625,
        "hf_subset": "en-zh",
        "languages": [
          "eng-Latn",
          "cmn-Hans"
        ],
        "main_score": 0.03474874898141625,
        "precision": 0.028009249049929442,
        "recall": 0.06598407281001138
      },
      {
        "accuracy": 0.5382022471910113,
        "f1": 0.4665925834465161,
        "hf_subset": "fr-en",
        "languages": [
          "fra-Latn",
          "eng-Latn"
        ],
        "main_score": 0.4665925834465161,
        "precision": 0.4423331955185888,
        "recall": 0.5382022471910113
      },
      {
        "accuracy": 0.3132400430570506,
        "f1": 0.26804010630026154,
        "hf_subset": "it-en",
        "languages": [
          "ita-Latn",
          "eng-Latn"
        ],
        "main_score": 0.26804010630026154,
        "precision": 0.2532143717094614,
        "recall": 0.3132400430570506
      },
      {
        "accuracy": 0.16183816183816183,
        "f1": 0.12500885610634926,
        "hf_subset": "it-nl",
        "languages": [
          "ita-Latn",
          "nld-Latn"
        ],
        "main_score": 0.12500885610634926,
        "precision": 0.11672603475903598,
        "recall": 0.16183816183816183
      },
      {
        "accuracy": 0.3774617067833698,
        "f1": 0.31669779284658933,
        "hf_subset": "it-ro",
        "languages": [
          "ita-Latn",
          "ron-Latn"
        ],
        "main_score": 0.31669779284658933,
        "precision": 0.2963091764787607,
        "recall": 0.3774617067833698
      },
      {
        "accuracy": 0.0574052812858783,
        "f1": 0.03851406334886765,
        "hf_subset": "ja-en",
        "languages": [
          "jpn-Jpan",
          "eng-Latn"
        ],
        "main_score": 0.03851406334886765,
        "precision": 0.03448738011951357,
        "recall": 0.0574052812858783
      },
      {
        "accuracy": 0.03640500568828214,
        "f1": 0.023264953747101698,
        "hf_subset": "ko-en",
        "languages": [
          "kor-Hang",
          "eng-Latn"
        ],
        "main_score": 0.023264953747101698,
        "precision": 0.020513129317242845,
        "recall": 0.03640500568828214
      },
      {
        "accuracy": 0.3120638085742772,
        "f1": 0.26026916616984375,
        "hf_subset": "nl-en",
        "languages": [
          "nld-Latn",
          "eng-Latn"
        ],
        "main_score": 0.26026916616984375,
        "precision": 0.24349226236916108,
        "recall": 0.3120638085742772
      },
      {
        "accuracy": 0.2017982017982018,
        "f1": 0.1634861007937931,
        "hf_subset": "nl-it",
        "languages": [
          "nld-Latn",
          "ita-Latn"
        ],
        "main_score": 0.1634861007937931,
        "precision": 0.1510023923123981,
        "recall": 0.2017982017982018
      },
      {
        "accuracy": 0.19934282584884994,
        "f1": 0.15921546074666296,
        "hf_subset": "nl-ro",
        "languages": [
          "nld-Latn",
          "ron-Latn"
        ],
        "main_score": 0.15921546074666296,
        "precision": 0.14571474504822715,
        "recall": 0.19934282584884994
      },
      {
        "accuracy": 0.2833698030634573,
        "f1": 0.23298610344622045,
        "hf_subset": "ro-en",
        "languages": [
          "ron-Latn",
          "eng-Latn"
        ],
        "main_score": 0.23298610344622045,
        "precision": 0.2169318494537313,
        "recall": 0.2833698030634573
      },
      {
        "accuracy": 0.3730853391684901,
        "f1": 0.31645023618491885,
        "hf_subset": "ro-it",
        "languages": [
          "ron-Latn",
          "ita-Latn"
        ],
        "main_score": 0.31645023618491885,
        "precision": 0.29648836205976037,
        "recall": 0.3730853391684901
      },
      {
        "accuracy": 0.18181818181818182,
        "f1": 0.13883171161624047,
        "hf_subset": "ro-nl",
        "languages": [
          "ron-Latn",
          "nld-Latn"
        ],
        "main_score": 0.13883171161624047,
        "precision": 0.12970927076896407,
        "recall": 0.18181818181818182
      },
      {
        "accuracy": 0.07281001137656427,
        "f1": 0.04945916174585117,
        "hf_subset": "zh-en",
        "languages": [
          "cmn-Hans",
          "eng-Latn"
        ],
        "main_score": 0.04945916174585117,
        "precision": 0.04461653106931832,
        "recall": 0.07281001137656427
      }
    ]
  },
  "task_name": "IWSLT2017BitextMining"
}