{
  "dataset_revision": "0ded8ff72cc68cbb7bb5c01b0a9157982b73ddaf",
  "evaluation_time": 33.607203245162964,
  "kg_co2_emissions": 0.0075043856984031675,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.5421875,
        "f1": 0.4826808687011387,
        "f1_weighted": 0.48742392906235743,
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.5421875,
        "scores_per_experiment": [
          {
            "accuracy": 0.55126953125,
            "f1": 0.49083048248910816,
            "f1_weighted": 0.49490399741526303
          },
          {
            "accuracy": 0.55810546875,
            "f1": 0.4981117555967036,
            "f1_weighted": 0.5011251760197357
          },
          {
            "accuracy": 0.5615234375,
            "f1": 0.5178046162220584,
            "f1_weighted": 0.5225958342213486
          },
          {
            "accuracy": 0.55615234375,
            "f1": 0.4967141163773296,
            "f1_weighted": 0.4998367754340767
          },
          {
            "accuracy": 0.51513671875,
            "f1": 0.4539324783617691,
            "f1_weighted": 0.4600823212533052
          },
          {
            "accuracy": 0.56005859375,
            "f1": 0.5070531514125516,
            "f1_weighted": 0.5108748560138836
          },
          {
            "accuracy": 0.51220703125,
            "f1": 0.4351440480809299,
            "f1_weighted": 0.4378823372051284
          },
          {
            "accuracy": 0.5361328125,
            "f1": 0.4870646427583767,
            "f1_weighted": 0.4919668945822543
          },
          {
            "accuracy": 0.53662109375,
            "f1": 0.46808555258010587,
            "f1_weighted": 0.47419787301428373
          },
          {
            "accuracy": 0.53466796875,
            "f1": 0.47206784313245465,
            "f1_weighted": 0.480773225464295
          }
        ]
      }
    ]
  },
  "task_name": "TweetEmotionClassification"
}