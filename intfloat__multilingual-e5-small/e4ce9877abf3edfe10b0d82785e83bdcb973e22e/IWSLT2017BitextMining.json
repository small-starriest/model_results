{
  "dataset_revision": "c18a4f81a47ae6fa079fe9d32db288ddde38451d",
  "evaluation_time": 20.52382254600525,
  "kg_co2_emissions": 0.003167868299401909,
  "mteb_version": "1.12.75",
  "scores": {
    "validation": [
      {
        "accuracy": 0.9436936936936937,
        "f1": 0.9293043043043042,
        "hf_subset": "ar-en",
        "languages": [
          "ara-Arab",
          "eng-Latn"
        ],
        "main_score": 0.9293043043043042,
        "precision": 0.9228134384384384,
        "recall": 0.9436936936936937
      },
      {
        "accuracy": 0.9819819819819819,
        "f1": 0.9784329784329784,
        "hf_subset": "de-en",
        "languages": [
          "deu-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9784329784329784,
        "precision": 0.977027027027027,
        "recall": 0.9819819819819819
      },
      {
        "accuracy": 0.9583333333333334,
        "f1": 0.9488953238953238,
        "hf_subset": "en-ar",
        "languages": [
          "eng-Latn",
          "ara-Arab"
        ],
        "main_score": 0.9488953238953238,
        "precision": 0.944632132132132,
        "recall": 0.9583333333333334
      },
      {
        "accuracy": 0.9628378378378378,
        "f1": 0.9557057057057058,
        "hf_subset": "en-de",
        "languages": [
          "eng-Latn",
          "deu-Latn"
        ],
        "main_score": 0.9557057057057058,
        "precision": 0.9523273273273273,
        "recall": 0.9628378378378378
      },
      {
        "accuracy": 0.9741573033707865,
        "f1": 0.9676029962546817,
        "hf_subset": "en-fr",
        "languages": [
          "eng-Latn",
          "fra-Latn"
        ],
        "main_score": 0.9676029962546817,
        "precision": 0.9648689138576779,
        "recall": 0.9741573033707865
      },
      {
        "accuracy": 0.9483315392895587,
        "f1": 0.9353426623609616,
        "hf_subset": "en-it",
        "languages": [
          "eng-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9353426623609616,
        "precision": 0.9297033847625882,
        "recall": 0.9483315392895587
      },
      {
        "accuracy": 0.801377726750861,
        "f1": 0.7524885562198994,
        "hf_subset": "en-ja",
        "languages": [
          "eng-Latn",
          "jpn-Jpan"
        ],
        "main_score": 0.7524885562198994,
        "precision": 0.7316029741402876,
        "recall": 0.801377726750861
      },
      {
        "accuracy": 0.7747440273037542,
        "f1": 0.7266265778211171,
        "hf_subset": "en-ko",
        "languages": [
          "eng-Latn",
          "kor-Hang"
        ],
        "main_score": 0.7266265778211171,
        "precision": 0.7077550246492226,
        "recall": 0.7747440273037542
      },
      {
        "accuracy": 0.9292123629112662,
        "f1": 0.9090585386697052,
        "hf_subset": "en-nl",
        "languages": [
          "eng-Latn",
          "nld-Latn"
        ],
        "main_score": 0.9090585386697052,
        "precision": 0.8998338318378198,
        "recall": 0.9292123629112662
      },
      {
        "accuracy": 0.9310722100656456,
        "f1": 0.9147337709700949,
        "hf_subset": "en-ro",
        "languages": [
          "eng-Latn",
          "ron-Latn"
        ],
        "main_score": 0.9147337709700949,
        "precision": 0.9077498176513494,
        "recall": 0.9310722100656456
      },
      {
        "accuracy": 0.8361774744027304,
        "f1": 0.798595166683904,
        "hf_subset": "en-zh",
        "languages": [
          "eng-Latn",
          "cmn-Hans"
        ],
        "main_score": 0.798595166683904,
        "precision": 0.7838371526084836,
        "recall": 0.8361774744027304
      },
      {
        "accuracy": 0.9662921348314607,
        "f1": 0.9572498662386303,
        "hf_subset": "fr-en",
        "languages": [
          "fra-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9572498662386303,
        "precision": 0.9532209737827716,
        "recall": 0.9662921348314607
      },
      {
        "accuracy": 0.9601722282023681,
        "f1": 0.949497667743093,
        "hf_subset": "it-en",
        "languages": [
          "ita-Latn",
          "eng-Latn"
        ],
        "main_score": 0.949497667743093,
        "precision": 0.9445384181659747,
        "recall": 0.9601722282023681
      },
      {
        "accuracy": 0.9070929070929071,
        "f1": 0.8844679130393417,
        "hf_subset": "it-nl",
        "languages": [
          "ita-Latn",
          "nld-Latn"
        ],
        "main_score": 0.8844679130393417,
        "precision": 0.8738761238761239,
        "recall": 0.9070929070929071
      },
      {
        "accuracy": 0.9157549234135668,
        "f1": 0.8951495258935085,
        "hf_subset": "it-ro",
        "languages": [
          "ita-Latn",
          "ron-Latn"
        ],
        "main_score": 0.8951495258935085,
        "precision": 0.8859713104789692,
        "recall": 0.9157549234135668
      },
      {
        "accuracy": 0.801377726750861,
        "f1": 0.7655677655677655,
        "hf_subset": "ja-en",
        "languages": [
          "jpn-Jpan",
          "eng-Latn"
        ],
        "main_score": 0.7655677655677655,
        "precision": 0.7527135512784191,
        "recall": 0.801377726750861
      },
      {
        "accuracy": 0.7940841865756542,
        "f1": 0.7549740867147011,
        "hf_subset": "ko-en",
        "languages": [
          "kor-Hang",
          "eng-Latn"
        ],
        "main_score": 0.7549740867147011,
        "precision": 0.7393534319302237,
        "recall": 0.7940841865756542
      },
      {
        "accuracy": 0.9361914257228315,
        "f1": 0.920759942395037,
        "hf_subset": "nl-en",
        "languages": [
          "nld-Latn",
          "eng-Latn"
        ],
        "main_score": 0.920759942395037,
        "precision": 0.914415088069126,
        "recall": 0.9361914257228315
      },
      {
        "accuracy": 0.8851148851148851,
        "f1": 0.8561438561438561,
        "hf_subset": "nl-it",
        "languages": [
          "nld-Latn",
          "ita-Latn"
        ],
        "main_score": 0.8561438561438561,
        "precision": 0.8432733932733933,
        "recall": 0.8851148851148851
      },
      {
        "accuracy": 0.8674698795180723,
        "f1": 0.8355482536205429,
        "hf_subset": "nl-ro",
        "languages": [
          "nld-Latn",
          "ron-Latn"
        ],
        "main_score": 0.8355482536205429,
        "precision": 0.8214220518437385,
        "recall": 0.8674698795180723
      },
      {
        "accuracy": 0.9540481400437637,
        "f1": 0.9429248723559445,
        "hf_subset": "ro-en",
        "languages": [
          "ron-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9429248723559445,
        "precision": 0.938074398249453,
        "recall": 0.9540481400437637
      },
      {
        "accuracy": 0.9059080962800875,
        "f1": 0.8838439095550693,
        "hf_subset": "ro-it",
        "languages": [
          "ron-Latn",
          "ita-Latn"
        ],
        "main_score": 0.8838439095550693,
        "precision": 0.8736688548504741,
        "recall": 0.9059080962800875
      },
      {
        "accuracy": 0.8521358159912377,
        "f1": 0.8170343712512387,
        "hf_subset": "ro-nl",
        "languages": [
          "ron-Latn",
          "nld-Latn"
        ],
        "main_score": 0.8170343712512387,
        "precision": 0.8012413289521723,
        "recall": 0.8521358159912377
      },
      {
        "accuracy": 0.8714448236632537,
        "f1": 0.8397484515231955,
        "hf_subset": "zh-en",
        "languages": [
          "cmn-Hans",
          "eng-Latn"
        ],
        "main_score": 0.8397484515231955,
        "precision": 0.8265277100601333,
        "recall": 0.8714448236632537
      }
    ]
  },
  "task_name": "IWSLT2017BitextMining"
}