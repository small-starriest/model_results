{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 1118.8686709403992,
  "kg_co2_emissions": 0.18274414235916833,
  "mteb_version": "1.12.75",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06444756554307116,
        "map": 0.08177635963213344,
        "mrr": 0.06444756554307116,
        "nAUC_map_diff1": 0.051922843848073906,
        "nAUC_map_max": 0.0015971806694994619,
        "nAUC_map_std": 0.20502428498178948,
        "nAUC_mrr_diff1": 0.04942434462435888,
        "nAUC_mrr_max": 0.0030625860691231768,
        "nAUC_mrr_std": 0.18667473873046592
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08964516576904698,
        "map": 0.10862611139363836,
        "mrr": 0.08964516576904698,
        "nAUC_map_diff1": 0.11220062757661757,
        "nAUC_map_max": -0.13382150779849158,
        "nAUC_map_std": 0.03399694867690333,
        "nAUC_mrr_diff1": 0.1182319151412509,
        "nAUC_mrr_max": -0.129815082247218,
        "nAUC_mrr_std": 0.026518456174383725
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08088421970549925,
        "map": 0.10056553614576741,
        "mrr": 0.08088421970549925,
        "nAUC_map_diff1": 0.19606466408536605,
        "nAUC_map_max": 0.013243549578855476,
        "nAUC_map_std": 0.10963486474533161,
        "nAUC_mrr_diff1": 0.1981021539543045,
        "nAUC_mrr_max": 0.03066709904138471,
        "nAUC_mrr_std": 0.09832977734576513
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08467690712877575,
        "map": 0.10403519085709986,
        "mrr": 0.08467690712877575,
        "nAUC_map_diff1": 0.05671995024651089,
        "nAUC_map_max": 0.022368777805285435,
        "nAUC_map_std": 0.08307636243983696,
        "nAUC_mrr_diff1": 0.0528981587324306,
        "nAUC_mrr_max": 0.020122405279647787,
        "nAUC_mrr_std": 0.06546892532393524
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.051665303525990545,
        "map": 0.0718716542500926,
        "mrr": 0.051665303525990545,
        "nAUC_map_diff1": 0.06048831395993308,
        "nAUC_map_max": -0.08452798728442251,
        "nAUC_map_std": 0.15210894575930903,
        "nAUC_mrr_diff1": 0.047316957649270006,
        "nAUC_mrr_max": -0.09858157793430801,
        "nAUC_mrr_std": 0.15387863590826173
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.11242542366677648,
        "map": 0.1309929332295911,
        "mrr": 0.11242542366677648,
        "nAUC_map_diff1": 0.22614741980308942,
        "nAUC_map_max": -0.019585106237951174,
        "nAUC_map_std": -0.027388109469129583,
        "nAUC_mrr_diff1": 0.22553566239355183,
        "nAUC_mrr_max": -0.01238500347151459,
        "nAUC_mrr_std": -0.03804973760821171
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}