{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 17.373987674713135,
  "kg_co2_emissions": 0.0026506668871197694,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.342724609375,
        "f1": 0.3255472820410067,
        "f1_weighted": 0.32556016588120895,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.342724609375,
        "scores_per_experiment": [
          {
            "accuracy": 0.33203125,
            "f1": 0.3086650638554821,
            "f1_weighted": 0.30869293300163697
          },
          {
            "accuracy": 0.34326171875,
            "f1": 0.32856105721627493,
            "f1_weighted": 0.32858723936675993
          },
          {
            "accuracy": 0.3740234375,
            "f1": 0.35021012436849924,
            "f1_weighted": 0.35017812600892056
          },
          {
            "accuracy": 0.36474609375,
            "f1": 0.35635256901973233,
            "f1_weighted": 0.3563916245678024
          },
          {
            "accuracy": 0.33447265625,
            "f1": 0.3276107621709082,
            "f1_weighted": 0.32762221284727977
          },
          {
            "accuracy": 0.3359375,
            "f1": 0.3135259560101401,
            "f1_weighted": 0.31352771626091164
          },
          {
            "accuracy": 0.35205078125,
            "f1": 0.3508469091494223,
            "f1_weighted": 0.35083601276632753
          },
          {
            "accuracy": 0.3779296875,
            "f1": 0.3557874997185126,
            "f1_weighted": 0.35580574482717997
          },
          {
            "accuracy": 0.302734375,
            "f1": 0.2748583434751025,
            "f1_weighted": 0.2748639281981893
          },
          {
            "accuracy": 0.31005859375,
            "f1": 0.2890545354259923,
            "f1_weighted": 0.2890961209670817
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.341259765625,
        "f1": 0.3246427143395318,
        "f1_weighted": 0.3246497348806126,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.341259765625,
        "scores_per_experiment": [
          {
            "accuracy": 0.34326171875,
            "f1": 0.32066850270643454,
            "f1_weighted": 0.32069309726562817
          },
          {
            "accuracy": 0.34033203125,
            "f1": 0.3240324689969202,
            "f1_weighted": 0.3240471073718678
          },
          {
            "accuracy": 0.3720703125,
            "f1": 0.350293058640431,
            "f1_weighted": 0.35025187443548766
          },
          {
            "accuracy": 0.36474609375,
            "f1": 0.3566789932134021,
            "f1_weighted": 0.3567135135043267
          },
          {
            "accuracy": 0.33642578125,
            "f1": 0.332139866841832,
            "f1_weighted": 0.33215515917756955
          },
          {
            "accuracy": 0.32958984375,
            "f1": 0.30440066149073075,
            "f1_weighted": 0.3044011308937351
          },
          {
            "accuracy": 0.31787109375,
            "f1": 0.3178339998332979,
            "f1_weighted": 0.31782654256399295
          },
          {
            "accuracy": 0.38720703125,
            "f1": 0.3663204981130683,
            "f1_weighted": 0.3663053100588145
          },
          {
            "accuracy": 0.31298828125,
            "f1": 0.2854784665523527,
            "f1_weighted": 0.2854715780180352
          },
          {
            "accuracy": 0.30810546875,
            "f1": 0.2885806270068487,
            "f1_weighted": 0.2886320355166687
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}