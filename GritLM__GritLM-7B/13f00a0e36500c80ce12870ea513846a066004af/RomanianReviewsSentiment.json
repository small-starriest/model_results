{
  "dataset_revision": "358bcc95aeddd5d07a4524ee416f03d993099b23",
  "evaluation_time": 34.77391076087952,
  "kg_co2_emissions": 0.009084274456078602,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.590185546875,
        "f1": 0.5011065708467548,
        "f1_weighted": 0.6164602650191428,
        "hf_subset": "default",
        "languages": [
          "ron-Latn"
        ],
        "main_score": 0.590185546875,
        "scores_per_experiment": [
          {
            "accuracy": 0.6279296875,
            "f1": 0.5292670855866315,
            "f1_weighted": 0.6464555650455344
          },
          {
            "accuracy": 0.462890625,
            "f1": 0.4271637943082053,
            "f1_weighted": 0.5012623826958085
          },
          {
            "accuracy": 0.4990234375,
            "f1": 0.4436301072114017,
            "f1_weighted": 0.5346533140944723
          },
          {
            "accuracy": 0.677734375,
            "f1": 0.56325226986014,
            "f1_weighted": 0.6949282537355121
          },
          {
            "accuracy": 0.62255859375,
            "f1": 0.5174394937997597,
            "f1_weighted": 0.6457348153013678
          },
          {
            "accuracy": 0.626953125,
            "f1": 0.5155764257771435,
            "f1_weighted": 0.6512322017062027
          },
          {
            "accuracy": 0.548828125,
            "f1": 0.4777369925756455,
            "f1_weighted": 0.5720255036647031
          },
          {
            "accuracy": 0.671875,
            "f1": 0.541050643990523,
            "f1_weighted": 0.689355321989161
          },
          {
            "accuracy": 0.60595703125,
            "f1": 0.5289388849605409,
            "f1_weighted": 0.6382719652837776
          },
          {
            "accuracy": 0.55810546875,
            "f1": 0.46701001039755785,
            "f1_weighted": 0.5906833266748873
          }
        ]
      }
    ]
  },
  "task_name": "RomanianReviewsSentiment"
}