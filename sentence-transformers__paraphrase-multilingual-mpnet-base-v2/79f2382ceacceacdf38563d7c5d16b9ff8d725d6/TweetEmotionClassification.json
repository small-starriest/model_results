{
  "dataset_revision": "0ded8ff72cc68cbb7bb5c01b0a9157982b73ddaf",
  "evaluation_time": 10.584683656692505,
  "kg_co2_emissions": 0.0015882233878097513,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.48603515625,
        "f1": 0.4657972946701313,
        "f1_weighted": 0.471096903175302,
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.48603515625,
        "scores_per_experiment": [
          {
            "accuracy": 0.49169921875,
            "f1": 0.46217753951675644,
            "f1_weighted": 0.46830362190203034
          },
          {
            "accuracy": 0.53369140625,
            "f1": 0.5119856908901527,
            "f1_weighted": 0.517618949427108
          },
          {
            "accuracy": 0.47216796875,
            "f1": 0.45089876311658117,
            "f1_weighted": 0.45696362795943396
          },
          {
            "accuracy": 0.49658203125,
            "f1": 0.4610774490785962,
            "f1_weighted": 0.4694738333680355
          },
          {
            "accuracy": 0.4794921875,
            "f1": 0.45346736825405276,
            "f1_weighted": 0.45775842577327
          },
          {
            "accuracy": 0.51025390625,
            "f1": 0.48862533947741454,
            "f1_weighted": 0.4941097093084678
          },
          {
            "accuracy": 0.46484375,
            "f1": 0.45690330680109636,
            "f1_weighted": 0.4575143097378581
          },
          {
            "accuracy": 0.45703125,
            "f1": 0.44294674320882343,
            "f1_weighted": 0.4480003900752465
          },
          {
            "accuracy": 0.48291015625,
            "f1": 0.4632201121134729,
            "f1_weighted": 0.47010716993342033
          },
          {
            "accuracy": 0.4716796875,
            "f1": 0.4666706342443664,
            "f1_weighted": 0.4711189942681492
          }
        ]
      }
    ]
  },
  "task_name": "TweetEmotionClassification"
}