{
  "dataset_revision": "f8650438298df086750ff4973661bb58a201a5ee",
  "evaluation_time": 85.33044219017029,
  "kg_co2_emissions": 0.013058196403834656,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.00691699604743083,
        "f1": 0.005439626443462761,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.005439626443462761,
        "precision": 0.005272520094654482,
        "recall": 0.00691699604743083
      },
      {
        "accuracy": 0.03458498023715415,
        "f1": 0.014271855201585245,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.014271855201585245,
        "precision": 0.011192833460359455,
        "recall": 0.03458498023715415
      },
      {
        "accuracy": 0.03458498023715415,
        "f1": 0.027049084563126585,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.027049084563126585,
        "precision": 0.025431700434936227,
        "recall": 0.03458498023715415
      },
      {
        "accuracy": 0.06521739130434782,
        "f1": 0.020474432227925908,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.020474432227925908,
        "precision": 0.013934767506060472,
        "recall": 0.06521739130434782
      },
      {
        "accuracy": 0.004940711462450593,
        "f1": 0.004940711462450593,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.004940711462450593,
        "precision": 0.004940711462450593,
        "recall": 0.004940711462450593
      },
      {
        "accuracy": 0.05731225296442688,
        "f1": 0.0254113027173781,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.0254113027173781,
        "precision": 0.01949312229014624,
        "recall": 0.05731225296442688
      },
      {
        "accuracy": 0.04743083003952569,
        "f1": 0.0354212783823241,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.0354212783823241,
        "precision": 0.03298208732788625,
        "recall": 0.04743083003952569
      },
      {
        "accuracy": 0.07905138339920949,
        "f1": 0.03206693041327317,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.03206693041327317,
        "precision": 0.0242989832128221,
        "recall": 0.07905138339920949
      },
      {
        "accuracy": 0.040513833992094864,
        "f1": 0.029729335261516592,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.029729335261516592,
        "precision": 0.027110763797597914,
        "recall": 0.040513833992094864
      },
      {
        "accuracy": 0.06818181818181818,
        "f1": 0.02934498204248979,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.02934498204248979,
        "precision": 0.022888617224807836,
        "recall": 0.06818181818181818
      },
      {
        "accuracy": 0.0029644268774703555,
        "f1": 0.0009968534114947367,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0009968534114947367,
        "precision": 0.0009925078855254749,
        "recall": 0.0029644268774703555
      },
      {
        "accuracy": 0.041501976284584984,
        "f1": 0.01594616071346785,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.01594616071346785,
        "precision": 0.011509618272163385,
        "recall": 0.041501976284584984
      },
      {
        "accuracy": 0.025691699604743084,
        "f1": 0.02063982213438735,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.02063982213438735,
        "precision": 0.019257217483762107,
        "recall": 0.025691699604743084
      },
      {
        "accuracy": 0.05632411067193676,
        "f1": 0.021222204937797712,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.021222204937797712,
        "precision": 0.01580060807318554,
        "recall": 0.05632411067193676
      },
      {
        "accuracy": 0.09486166007905138,
        "f1": 0.07928144444373789,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.07928144444373789,
        "precision": 0.07531555468230115,
        "recall": 0.09486166007905138
      },
      {
        "accuracy": 0.12450592885375494,
        "f1": 0.05758876843505314,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.05758876843505314,
        "precision": 0.04592656534227617,
        "recall": 0.12450592885375494
      },
      {
        "accuracy": 0.003952569169960474,
        "f1": 0.002637166590329494,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.002637166590329494,
        "precision": 0.0024714171085534276,
        "recall": 0.003952569169960474
      },
      {
        "accuracy": 0.03359683794466403,
        "f1": 0.013001980415615821,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.013001980415615821,
        "precision": 0.010269017620810563,
        "recall": 0.03359683794466403
      },
      {
        "accuracy": 0.010869565217391304,
        "f1": 0.009585538509635783,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.009585538509635783,
        "precision": 0.009404388714733543,
        "recall": 0.010869565217391304
      },
      {
        "accuracy": 0.03359683794466403,
        "f1": 0.016877278701959826,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.016877278701959826,
        "precision": 0.014342300139164032,
        "recall": 0.03359683794466403
      },
      {
        "accuracy": 0.008893280632411068,
        "f1": 0.008563899868247694,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.008563899868247694,
        "precision": 0.008399209486166008,
        "recall": 0.008893280632411068
      },
      {
        "accuracy": 0.06324110671936758,
        "f1": 0.025388865717019524,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.025388865717019524,
        "precision": 0.019275143648145504,
        "recall": 0.06324110671936758
      },
      {
        "accuracy": 0.015810276679841896,
        "f1": 0.013183646645751296,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.013183646645751296,
        "precision": 0.013047695400033381,
        "recall": 0.015810276679841896
      },
      {
        "accuracy": 0.06620553359683795,
        "f1": 0.030995803730322652,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.030995803730322652,
        "precision": 0.02592617284438483,
        "recall": 0.06620553359683795
      },
      {
        "accuracy": 0.04940711462450593,
        "f1": 0.036551273812605575,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.036551273812605575,
        "precision": 0.03301325418668965,
        "recall": 0.04940711462450593
      },
      {
        "accuracy": 0.08300395256916997,
        "f1": 0.036646727767274345,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.036646727767274345,
        "precision": 0.028528392414835072,
        "recall": 0.08300395256916997
      },
      {
        "accuracy": 0.05237154150197629,
        "f1": 0.03839118237250747,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.03839118237250747,
        "precision": 0.03518825324292076,
        "recall": 0.05237154150197629
      },
      {
        "accuracy": 0.08300395256916997,
        "f1": 0.03282123285220007,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.03282123285220007,
        "precision": 0.024444195969667775,
        "recall": 0.08300395256916997
      },
      {
        "accuracy": 0.07509881422924901,
        "f1": 0.07147800116550117,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.07147800116550117,
        "precision": 0.07015929186703006,
        "recall": 0.07509881422924901
      },
      {
        "accuracy": 0.12055335968379446,
        "f1": 0.057383932576148315,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.057383932576148315,
        "precision": 0.045038672321495166,
        "recall": 0.12055335968379446
      },
      {
        "accuracy": 0.022727272727272728,
        "f1": 0.018854982739975726,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.018854982739975726,
        "precision": 0.01802574966478626,
        "recall": 0.022727272727272728
      },
      {
        "accuracy": 0.06818181818181818,
        "f1": 0.027981140170135,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.027981140170135,
        "precision": 0.02165549031103919,
        "recall": 0.06818181818181818
      },
      {
        "accuracy": 0.004940711462450593,
        "f1": 0.003626276350461133,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.003626276350461133,
        "precision": 0.0034600444122795627,
        "recall": 0.004940711462450593
      },
      {
        "accuracy": 0.044466403162055336,
        "f1": 0.02028476532306953,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.02028476532306953,
        "precision": 0.016478258872143593,
        "recall": 0.044466403162055336
      },
      {
        "accuracy": 0.001976284584980237,
        "f1": 0.001976284584980237,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.001976284584980237,
        "precision": 0.001976284584980237,
        "recall": 0.001976284584980237
      },
      {
        "accuracy": 0.03458498023715415,
        "f1": 0.016559162616728747,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.016559162616728747,
        "precision": 0.013964219453942455,
        "recall": 0.03458498023715415
      },
      {
        "accuracy": 0.007905138339920948,
        "f1": 0.00692439786235585,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.00692439786235585,
        "precision": 0.006920710868079289,
        "recall": 0.007905138339920948
      },
      {
        "accuracy": 0.02766798418972332,
        "f1": 0.009859738509647487,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.009859738509647487,
        "precision": 0.008056015375018514,
        "recall": 0.02766798418972332
      },
      {
        "accuracy": 0.00691699604743083,
        "f1": 0.006211180124223602,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.006211180124223602,
        "precision": 0.006093544137022398,
        "recall": 0.00691699604743083
      },
      {
        "accuracy": 0.05039525691699605,
        "f1": 0.023272355444853246,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.023272355444853246,
        "precision": 0.01921312172386164,
        "recall": 0.05039525691699605
      },
      {
        "accuracy": 0.005928853754940711,
        "f1": 0.004614565206609611,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.004614565206609611,
        "precision": 0.004448260221603059,
        "recall": 0.005928853754940711
      },
      {
        "accuracy": 0.0533596837944664,
        "f1": 0.02186475655491429,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.02186475655491429,
        "precision": 0.016452532148876382,
        "recall": 0.0533596837944664
      },
      {
        "accuracy": 0.009881422924901186,
        "f1": 0.007975967586951568,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.007975967586951568,
        "precision": 0.00766058553637754,
        "recall": 0.009881422924901186
      },
      {
        "accuracy": 0.07509881422924901,
        "f1": 0.03147175904851004,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.03147175904851004,
        "precision": 0.024327583438194002,
        "recall": 0.07509881422924901
      },
      {
        "accuracy": 0.004940711462450593,
        "f1": 0.0029726209321070986,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0029726209321070986,
        "precision": 0.0029685324797924374,
        "recall": 0.004940711462450593
      },
      {
        "accuracy": 0.03557312252964427,
        "f1": 0.012606934925662316,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.012606934925662316,
        "precision": 0.009076812941294424,
        "recall": 0.03557312252964427
      },
      {
        "accuracy": 0.004940711462450593,
        "f1": 0.004611330698287219,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.004611330698287219,
        "precision": 0.004446640316205534,
        "recall": 0.004940711462450593
      },
      {
        "accuracy": 0.05632411067193676,
        "f1": 0.026609670694977917,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.026609670694977917,
        "precision": 0.022079875028892547,
        "recall": 0.05632411067193676
      },
      {
        "accuracy": 0.008893280632411068,
        "f1": 0.007579261626440164,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.007579261626440164,
        "precision": 0.007412822330962727,
        "recall": 0.008893280632411068
      },
      {
        "accuracy": 0.05138339920948617,
        "f1": 0.020621719819264066,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.020621719819264066,
        "precision": 0.015142580721806484,
        "recall": 0.05138339920948617
      },
      {
        "accuracy": 0.010869565217391304,
        "f1": 0.008128894892791425,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.008128894892791425,
        "precision": 0.007536674075992947,
        "recall": 0.010869565217391304
      },
      {
        "accuracy": 0.02865612648221344,
        "f1": 0.008737000165024893,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.008737000165024893,
        "precision": 0.006694257724288658,
        "recall": 0.02865612648221344
      },
      {
        "accuracy": 0.004940711462450593,
        "f1": 0.003957839262187088,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.003957839262187088,
        "precision": 0.003955211261651625,
        "recall": 0.004940711462450593
      },
      {
        "accuracy": 0.040513833992094864,
        "f1": 0.01470139791506519,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.01470139791506519,
        "precision": 0.01031778730576651,
        "recall": 0.040513833992094864
      },
      {
        "accuracy": 0.004940711462450593,
        "f1": 0.004611330698287219,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.004611330698287219,
        "precision": 0.004446640316205534,
        "recall": 0.004940711462450593
      },
      {
        "accuracy": 0.05632411067193676,
        "f1": 0.024803600168090384,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.024803600168090384,
        "precision": 0.019346179119407992,
        "recall": 0.05632411067193676
      },
      {
        "accuracy": 0.018774703557312252,
        "f1": 0.013025967263998187,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.013025967263998187,
        "precision": 0.01183425270381792,
        "recall": 0.018774703557312252
      },
      {
        "accuracy": 0.022727272727272728,
        "f1": 0.0029084987494506333,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.0029084987494506333,
        "precision": 0.0016014192043025334,
        "recall": 0.022727272727272728
      }
    ],
    "validation": [
      {
        "accuracy": 0.012036108324974924,
        "f1": 0.01006144955547909,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.01006144955547909,
        "precision": 0.010045893984474433,
        "recall": 0.012036108324974924
      },
      {
        "accuracy": 0.03009027081243731,
        "f1": 0.010752249498335694,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.010752249498335694,
        "precision": 0.008484087114504253,
        "recall": 0.03009027081243731
      },
      {
        "accuracy": 0.044132397191574725,
        "f1": 0.03286316523772336,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.03286316523772336,
        "precision": 0.030187324696519573,
        "recall": 0.044132397191574725
      },
      {
        "accuracy": 0.06820461384152457,
        "f1": 0.02465934012240489,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.02465934012240489,
        "precision": 0.017974721831700194,
        "recall": 0.06820461384152457
      },
      {
        "accuracy": 0.007021063189568706,
        "f1": 0.005687083622231347,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.005687083622231347,
        "precision": 0.00551823537840412,
        "recall": 0.007021063189568706
      },
      {
        "accuracy": 0.04613841524573721,
        "f1": 0.01819056794651062,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.01819056794651062,
        "precision": 0.014129119765257719,
        "recall": 0.04613841524573721
      },
      {
        "accuracy": 0.04914744232698094,
        "f1": 0.037050442922618367,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.037050442922618367,
        "precision": 0.033996662079880106,
        "recall": 0.04914744232698094
      },
      {
        "accuracy": 0.07422266800401203,
        "f1": 0.030429828040399905,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.030429828040399905,
        "precision": 0.02312658008659031,
        "recall": 0.07422266800401203
      },
      {
        "accuracy": 0.05215646940822467,
        "f1": 0.037991156854261396,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.037991156854261396,
        "precision": 0.03508972526726939,
        "recall": 0.05215646940822467
      },
      {
        "accuracy": 0.07823470411233702,
        "f1": 0.033198931943150496,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.033198931943150496,
        "precision": 0.025577372071272277,
        "recall": 0.07823470411233702
      },
      {
        "accuracy": 0.012036108324974924,
        "f1": 0.010087870062108944,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.010087870062108944,
        "precision": 0.010059736346247123,
        "recall": 0.012036108324974924
      },
      {
        "accuracy": 0.05115346038114343,
        "f1": 0.020650337997379118,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.020650337997379118,
        "precision": 0.016098653636392832,
        "recall": 0.05115346038114343
      },
      {
        "accuracy": 0.03911735205616851,
        "f1": 0.03140330081152548,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.03140330081152548,
        "precision": 0.029985834820906537,
        "recall": 0.03911735205616851
      },
      {
        "accuracy": 0.06820461384152457,
        "f1": 0.03040820220963592,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.03040820220963592,
        "precision": 0.02432512399853326,
        "recall": 0.06820461384152457
      },
      {
        "accuracy": 0.09628886659979939,
        "f1": 0.08301933048211356,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.08301933048211356,
        "precision": 0.07982311842740344,
        "recall": 0.09628886659979939
      },
      {
        "accuracy": 0.10732196589769308,
        "f1": 0.04674846500369959,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.04674846500369959,
        "precision": 0.035467821972704525,
        "recall": 0.10732196589769308
      },
      {
        "accuracy": 0.013039117352056168,
        "f1": 0.010603122301987717,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.010603122301987717,
        "precision": 0.01040142698814366,
        "recall": 0.013039117352056168
      },
      {
        "accuracy": 0.044132397191574725,
        "f1": 0.017491635650521403,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.017491635650521403,
        "precision": 0.013231491653717075,
        "recall": 0.044132397191574725
      },
      {
        "accuracy": 0.011033099297893681,
        "f1": 0.007832327450839205,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.007832327450839205,
        "precision": 0.007595785521347196,
        "recall": 0.011033099297893681
      },
      {
        "accuracy": 0.033099297893681046,
        "f1": 0.012370346391169554,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.012370346391169554,
        "precision": 0.008939667554372347,
        "recall": 0.033099297893681046
      },
      {
        "accuracy": 0.017051153460381142,
        "f1": 0.015345544021226239,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.015345544021226239,
        "precision": 0.015219268917864705,
        "recall": 0.017051153460381142
      },
      {
        "accuracy": 0.05717151454363089,
        "f1": 0.02210134777797325,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.02210134777797325,
        "precision": 0.017012861264110335,
        "recall": 0.05717151454363089
      },
      {
        "accuracy": 0.014042126379137413,
        "f1": 0.013373453694416582,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.013373453694416582,
        "precision": 0.013039117352056168,
        "recall": 0.014042126379137413
      },
      {
        "accuracy": 0.07021063189568706,
        "f1": 0.02961635978352686,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.02961635978352686,
        "precision": 0.02264238696890849,
        "recall": 0.07021063189568706
      },
      {
        "accuracy": 0.05416248746238716,
        "f1": 0.04194980678119959,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.04194980678119959,
        "precision": 0.03882699906863479,
        "recall": 0.05416248746238716
      },
      {
        "accuracy": 0.08425275827482448,
        "f1": 0.03497586985006052,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.03497586985006052,
        "precision": 0.026649813727222423,
        "recall": 0.08425275827482448
      },
      {
        "accuracy": 0.048144433299899696,
        "f1": 0.037416187382709465,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.037416187382709465,
        "precision": 0.0353006434703678,
        "recall": 0.048144433299899696
      },
      {
        "accuracy": 0.08324974924774323,
        "f1": 0.03527690975274037,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.03527690975274037,
        "precision": 0.02758229605935042,
        "recall": 0.08324974924774323
      },
      {
        "accuracy": 0.11434302908726178,
        "f1": 0.10472752178276518,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.10472752178276518,
        "precision": 0.1021239750683939,
        "recall": 0.11434302908726178
      },
      {
        "accuracy": 0.15446339017051153,
        "f1": 0.07838551121828062,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.07838551121828062,
        "precision": 0.06362538982575562,
        "recall": 0.15446339017051153
      },
      {
        "accuracy": 0.026078234704112337,
        "f1": 0.023247219134882122,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.023247219134882122,
        "precision": 0.02240598607416452,
        "recall": 0.026078234704112337
      },
      {
        "accuracy": 0.062186559679037114,
        "f1": 0.026650280842613238,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.026650280842613238,
        "precision": 0.021805162425802542,
        "recall": 0.062186559679037114
      },
      {
        "accuracy": 0.004012036108324975,
        "f1": 0.001040183272232359,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.001040183272232359,
        "precision": 0.0010217426596712204,
        "recall": 0.004012036108324975
      },
      {
        "accuracy": 0.041123370110330994,
        "f1": 0.018659303794208104,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.018659303794208104,
        "precision": 0.014840026839842075,
        "recall": 0.041123370110330994
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.008046919579002979,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.008046919579002979,
        "precision": 0.008035594205869738,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.04613841524573721,
        "f1": 0.01845743129389643,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.01845743129389643,
        "precision": 0.013641033089792313,
        "recall": 0.04613841524573721
      },
      {
        "accuracy": 0.013039117352056168,
        "f1": 0.012537612838515547,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.012537612838515547,
        "precision": 0.012370444667335338,
        "recall": 0.013039117352056168
      },
      {
        "accuracy": 0.029087261785356068,
        "f1": 0.007437092515896133,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.007437092515896133,
        "precision": 0.005339087333908934,
        "recall": 0.029087261785356068
      },
      {
        "accuracy": 0.006018054162487462,
        "f1": 0.004051797718042353,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.004051797718042353,
        "precision": 0.0040322573266231825,
        "recall": 0.006018054162487462
      },
      {
        "accuracy": 0.044132397191574725,
        "f1": 0.01809365172934141,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.01809365172934141,
        "precision": 0.013804023769703477,
        "recall": 0.044132397191574725
      },
      {
        "accuracy": 0.006018054162487462,
        "f1": 0.005018012617734861,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.005018012617734861,
        "precision": 0.005016531074705598,
        "recall": 0.006018054162487462
      },
      {
        "accuracy": 0.04714142427281846,
        "f1": 0.020818940695898788,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.020818940695898788,
        "precision": 0.015930162710229078,
        "recall": 0.04714142427281846
      },
      {
        "accuracy": 0.0160481444332999,
        "f1": 0.014398506875939172,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.014398506875939172,
        "precision": 0.01405327092388276,
        "recall": 0.0160481444332999
      },
      {
        "accuracy": 0.06018054162487462,
        "f1": 0.02393848686819671,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.02393848686819671,
        "precision": 0.01821730685254226,
        "recall": 0.06018054162487462
      },
      {
        "accuracy": 0.004012036108324975,
        "f1": 0.0020236671236602438,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0020236671236602438,
        "precision": 0.0020148952575578145,
        "recall": 0.004012036108324975
      },
      {
        "accuracy": 0.03610832497492478,
        "f1": 0.015325693697969779,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.015325693697969779,
        "precision": 0.011421791985148722,
        "recall": 0.03610832497492478
      },
      {
        "accuracy": 0.00802407221664995,
        "f1": 0.00802407221664995,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.00802407221664995,
        "precision": 0.00802407221664995,
        "recall": 0.00802407221664995
      },
      {
        "accuracy": 0.04714142427281846,
        "f1": 0.019516233886845723,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.019516233886845723,
        "precision": 0.01610431818514759,
        "recall": 0.04714142427281846
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.006012330329104225,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.006012330329104225,
        "precision": 0.005435671011628302,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.05416248746238716,
        "f1": 0.022432912145418985,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.022432912145418985,
        "precision": 0.016775125981027594,
        "recall": 0.05416248746238716
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.008427581595360795,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.008427581595360795,
        "precision": 0.008059893967617136,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.033099297893681046,
        "f1": 0.011879004572925815,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.011879004572925815,
        "precision": 0.009375895709742451,
        "recall": 0.033099297893681046
      },
      {
        "accuracy": 0.0050150451354062184,
        "f1": 0.0030291533480345197,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0030291533480345197,
        "precision": 0.0030191583152819803,
        "recall": 0.0050150451354062184
      },
      {
        "accuracy": 0.03610832497492478,
        "f1": 0.015108736447381152,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.015108736447381152,
        "precision": 0.011580582184176855,
        "recall": 0.03610832497492478
      },
      {
        "accuracy": 0.007021063189568706,
        "f1": 0.005039992918845864,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.005039992918845864,
        "precision": 0.005027597412119399,
        "recall": 0.007021063189568706
      },
      {
        "accuracy": 0.041123370110330994,
        "f1": 0.01822574700159066,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.01822574700159066,
        "precision": 0.014136807614761241,
        "recall": 0.041123370110330994
      },
      {
        "accuracy": 0.014042126379137413,
        "f1": 0.010782904983671597,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.010782904983671597,
        "precision": 0.010289527759713231,
        "recall": 0.014042126379137413
      },
      {
        "accuracy": 0.01905717151454363,
        "f1": 0.0018535156958229373,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.0018535156958229373,
        "precision": 0.0010097127523444242,
        "recall": 0.01905717151454363
      }
    ]
  },
  "task_name": "IndicGenBenchFloresBitextMining"
}