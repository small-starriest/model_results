{
  "dataset_revision": "2e6fedf42c9c104e83dfd95c3a453721e683e244",
  "evaluation_time": 35.51829266548157,
  "kg_co2_emissions": 0.009194963863530019,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.598095703125,
        "f1": 0.5647176728090803,
        "f1_weighted": 0.5646411031524562,
        "hf_subset": "default",
        "languages": [
          "ces-Latn"
        ],
        "main_score": 0.598095703125,
        "scores_per_experiment": [
          {
            "accuracy": 0.6357421875,
            "f1": 0.6005795919285304,
            "f1_weighted": 0.6005165841290729
          },
          {
            "accuracy": 0.6064453125,
            "f1": 0.5716962316080053,
            "f1_weighted": 0.5716214008798459
          },
          {
            "accuracy": 0.5849609375,
            "f1": 0.5381451593933382,
            "f1_weighted": 0.5380475602675467
          },
          {
            "accuracy": 0.5966796875,
            "f1": 0.5542966945724651,
            "f1_weighted": 0.5542189707039434
          },
          {
            "accuracy": 0.5712890625,
            "f1": 0.5289780223590378,
            "f1_weighted": 0.5289009730818717
          },
          {
            "accuracy": 0.5927734375,
            "f1": 0.5615520592790677,
            "f1_weighted": 0.5614756775263988
          },
          {
            "accuracy": 0.58203125,
            "f1": 0.5528623632685387,
            "f1_weighted": 0.5527812681446657
          },
          {
            "accuracy": 0.5888671875,
            "f1": 0.5567171192684318,
            "f1_weighted": 0.5566380329831029
          },
          {
            "accuracy": 0.6064453125,
            "f1": 0.5946035073007144,
            "f1_weighted": 0.5945355676016661
          },
          {
            "accuracy": 0.61572265625,
            "f1": 0.5877459791126723,
            "f1_weighted": 0.5876749962064476
          }
        ]
      }
    ]
  },
  "task_name": "CzechProductReviewSentimentClassification"
}