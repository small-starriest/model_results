{
  "dataset_revision": "358bcc95aeddd5d07a4524ee416f03d993099b23",
  "evaluation_time": 10.088708400726318,
  "kg_co2_emissions": 0.0015078249817813843,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.447900390625,
        "f1": 0.3857192153740018,
        "f1_weighted": 0.4880636951974102,
        "hf_subset": "default",
        "languages": [
          "ron-Latn"
        ],
        "main_score": 0.447900390625,
        "scores_per_experiment": [
          {
            "accuracy": 0.455078125,
            "f1": 0.3935910165689194,
            "f1_weighted": 0.49978308186624854
          },
          {
            "accuracy": 0.435546875,
            "f1": 0.3789731528603518,
            "f1_weighted": 0.4823837604263135
          },
          {
            "accuracy": 0.4375,
            "f1": 0.3782409455738544,
            "f1_weighted": 0.4854947365092831
          },
          {
            "accuracy": 0.466796875,
            "f1": 0.41442082772151256,
            "f1_weighted": 0.5114118364284004
          },
          {
            "accuracy": 0.36669921875,
            "f1": 0.33241854879959243,
            "f1_weighted": 0.41719286251269116
          },
          {
            "accuracy": 0.46630859375,
            "f1": 0.3947472492296371,
            "f1_weighted": 0.5033569459810647
          },
          {
            "accuracy": 0.46533203125,
            "f1": 0.3853279518372547,
            "f1_weighted": 0.4855846607932499
          },
          {
            "accuracy": 0.43310546875,
            "f1": 0.3812282452711719,
            "f1_weighted": 0.4811596557439609
          },
          {
            "accuracy": 0.5166015625,
            "f1": 0.41784210717941894,
            "f1_weighted": 0.5457797224054
          },
          {
            "accuracy": 0.43603515625,
            "f1": 0.3804021086983043,
            "f1_weighted": 0.4684896893074905
          }
        ]
      }
    ]
  },
  "task_name": "RomanianReviewsSentiment"
}