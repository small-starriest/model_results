{
  "dataset_revision": "c18a4f81a47ae6fa079fe9d32db288ddde38451d",
  "evaluation_time": 25.14964461326599,
  "kg_co2_emissions": 0.004142253904556473,
  "mteb_version": "1.12.75",
  "scores": {
    "validation": [
      {
        "accuracy": 0.9448198198198198,
        "f1": 0.9318693693693694,
        "hf_subset": "ar-en",
        "languages": [
          "ara-Arab",
          "eng-Latn"
        ],
        "main_score": 0.9318693693693694,
        "precision": 0.9263138138138137,
        "recall": 0.9448198198198198
      },
      {
        "accuracy": 0.9808558558558559,
        "f1": 0.9771191646191646,
        "hf_subset": "de-en",
        "languages": [
          "deu-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9771191646191646,
        "precision": 0.9757132132132132,
        "recall": 0.9808558558558559
      },
      {
        "accuracy": 0.956081081081081,
        "f1": 0.9453828828828829,
        "hf_subset": "en-ar",
        "languages": [
          "eng-Latn",
          "ara-Arab"
        ],
        "main_score": 0.9453828828828829,
        "precision": 0.9403528528528529,
        "recall": 0.956081081081081
      },
      {
        "accuracy": 0.9797297297297297,
        "f1": 0.976180726180726,
        "hf_subset": "en-de",
        "languages": [
          "eng-Latn",
          "deu-Latn"
        ],
        "main_score": 0.976180726180726,
        "precision": 0.9747747747747748,
        "recall": 0.9797297297297297
      },
      {
        "accuracy": 0.9662921348314607,
        "f1": 0.9590057882192714,
        "hf_subset": "en-fr",
        "languages": [
          "eng-Latn",
          "fra-Latn"
        ],
        "main_score": 0.9590057882192714,
        "precision": 0.9559176029962546,
        "recall": 0.9662921348314607
      },
      {
        "accuracy": 0.9634015069967707,
        "f1": 0.9544475976122909,
        "hf_subset": "en-it",
        "languages": [
          "eng-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9544475976122909,
        "precision": 0.9504126300681738,
        "recall": 0.9634015069967707
      },
      {
        "accuracy": 0.9012629161882894,
        "f1": 0.8789687924016283,
        "hf_subset": "en-ja",
        "languages": [
          "eng-Latn",
          "jpn-Jpan"
        ],
        "main_score": 0.8789687924016283,
        "precision": 0.8691350937619594,
        "recall": 0.9012629161882894
      },
      {
        "accuracy": 0.8646188850967008,
        "f1": 0.8345089116420175,
        "hf_subset": "en-ko",
        "languages": [
          "eng-Latn",
          "kor-Hang"
        ],
        "main_score": 0.8345089116420175,
        "precision": 0.8209897610921502,
        "recall": 0.8646188850967008
      },
      {
        "accuracy": 0.9431704885343968,
        "f1": 0.9278830176138252,
        "hf_subset": "en-nl",
        "languages": [
          "eng-Latn",
          "nld-Latn"
        ],
        "main_score": 0.9278830176138252,
        "precision": 0.9208042539049518,
        "recall": 0.9431704885343968
      },
      {
        "accuracy": 0.9671772428884027,
        "f1": 0.9582629988538084,
        "hf_subset": "en-ro",
        "languages": [
          "eng-Latn",
          "ron-Latn"
        ],
        "main_score": 0.9582629988538084,
        "precision": 0.9543216630196937,
        "recall": 0.9671772428884027
      },
      {
        "accuracy": 0.9135381114903299,
        "f1": 0.8927569207432688,
        "hf_subset": "en-zh",
        "languages": [
          "eng-Latn",
          "cmn-Hans"
        ],
        "main_score": 0.8927569207432688,
        "precision": 0.8833333333333334,
        "recall": 0.9135381114903299
      },
      {
        "accuracy": 0.9651685393258427,
        "f1": 0.9570626003210272,
        "hf_subset": "fr-en",
        "languages": [
          "fra-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9570626003210272,
        "precision": 0.9535955056179776,
        "recall": 0.9651685393258427
      },
      {
        "accuracy": 0.961248654467169,
        "f1": 0.9529960531036957,
        "hf_subset": "it-en",
        "languages": [
          "ita-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9529960531036957,
        "precision": 0.9490491567994259,
        "recall": 0.961248654467169
      },
      {
        "accuracy": 0.9230769230769231,
        "f1": 0.9041149326863612,
        "hf_subset": "it-nl",
        "languages": [
          "ita-Latn",
          "nld-Latn"
        ],
        "main_score": 0.9041149326863612,
        "precision": 0.8955211455211456,
        "recall": 0.9230769230769231
      },
      {
        "accuracy": 0.9606126914660832,
        "f1": 0.9506564551422318,
        "hf_subset": "it-ro",
        "languages": [
          "ita-Latn",
          "ron-Latn"
        ],
        "main_score": 0.9506564551422318,
        "precision": 0.9461524434719183,
        "recall": 0.9606126914660832
      },
      {
        "accuracy": 0.878300803673938,
        "f1": 0.8512146228564139,
        "hf_subset": "ja-en",
        "languages": [
          "jpn-Jpan",
          "eng-Latn"
        ],
        "main_score": 0.8512146228564139,
        "precision": 0.8391216991963261,
        "recall": 0.878300803673938
      },
      {
        "accuracy": 0.8486916951080774,
        "f1": 0.8145430413348502,
        "hf_subset": "ko-en",
        "languages": [
          "kor-Hang",
          "eng-Latn"
        ],
        "main_score": 0.8145430413348502,
        "precision": 0.7989300612167507,
        "recall": 0.8486916951080774
      },
      {
        "accuracy": 0.9242273180458624,
        "f1": 0.9041209704220672,
        "hf_subset": "nl-en",
        "languages": [
          "nld-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9041209704220672,
        "precision": 0.8948155533399801,
        "recall": 0.9242273180458624
      },
      {
        "accuracy": 0.9140859140859141,
        "f1": 0.8938546302182666,
        "hf_subset": "nl-it",
        "languages": [
          "nld-Latn",
          "ita-Latn"
        ],
        "main_score": 0.8938546302182666,
        "precision": 0.8846320346320347,
        "recall": 0.9140859140859141
      },
      {
        "accuracy": 0.9309967141292442,
        "f1": 0.912449799196787,
        "hf_subset": "nl-ro",
        "languages": [
          "nld-Latn",
          "ron-Latn"
        ],
        "main_score": 0.912449799196787,
        "precision": 0.9038882803943045,
        "recall": 0.9309967141292442
      },
      {
        "accuracy": 0.9704595185995624,
        "f1": 0.96347817026154,
        "hf_subset": "ro-en",
        "languages": [
          "ron-Latn",
          "eng-Latn"
        ],
        "main_score": 0.96347817026154,
        "precision": 0.9604303428154631,
        "recall": 0.9704595185995624
      },
      {
        "accuracy": 0.9606126914660832,
        "f1": 0.9507658643326039,
        "hf_subset": "ro-it",
        "languages": [
          "ron-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9507658643326039,
        "precision": 0.9464624361779723,
        "recall": 0.9606126914660832
      },
      {
        "accuracy": 0.9452354874041621,
        "f1": 0.9321650237312887,
        "hf_subset": "ro-nl",
        "languages": [
          "ron-Latn",
          "nld-Latn"
        ],
        "main_score": 0.9321650237312887,
        "precision": 0.9261956918583424,
        "recall": 0.9452354874041621
      },
      {
        "accuracy": 0.9021615472127418,
        "f1": 0.8761660978384528,
        "hf_subset": "zh-en",
        "languages": [
          "cmn-Hans",
          "eng-Latn"
        ],
        "main_score": 0.8761660978384528,
        "precision": 0.8643073839319573,
        "recall": 0.9021615472127418
      }
    ]
  },
  "task_name": "IWSLT2017BitextMining"
}