{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 16.867545127868652,
  "kg_co2_emissions": 0.0025053453677398273,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.260986328125,
        "f1": 0.2573672641570868,
        "f1_weighted": 0.25735596451183407,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.260986328125,
        "scores_per_experiment": [
          {
            "accuracy": 0.2578125,
            "f1": 0.25433592981462994,
            "f1_weighted": 0.25430552450932575
          },
          {
            "accuracy": 0.2431640625,
            "f1": 0.23812176255471745,
            "f1_weighted": 0.2381016369969023
          },
          {
            "accuracy": 0.27490234375,
            "f1": 0.26760468802183046,
            "f1_weighted": 0.26758485412091443
          },
          {
            "accuracy": 0.25830078125,
            "f1": 0.25607136630248517,
            "f1_weighted": 0.25603623616225474
          },
          {
            "accuracy": 0.25634765625,
            "f1": 0.2541099538653557,
            "f1_weighted": 0.2541079821893931
          },
          {
            "accuracy": 0.234375,
            "f1": 0.23235337910296358,
            "f1_weighted": 0.23237698859711003
          },
          {
            "accuracy": 0.2333984375,
            "f1": 0.23017311105361626,
            "f1_weighted": 0.23013453281328006
          },
          {
            "accuracy": 0.291015625,
            "f1": 0.29046214182312047,
            "f1_weighted": 0.2904804953922852
          },
          {
            "accuracy": 0.2802734375,
            "f1": 0.27639992202692093,
            "f1_weighted": 0.27638003026848224
          },
          {
            "accuracy": 0.2802734375,
            "f1": 0.2740403870052273,
            "f1_weighted": 0.27405136406839264
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.26005859375,
        "f1": 0.2560173894318752,
        "f1_weighted": 0.25600252465924234,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.26005859375,
        "scores_per_experiment": [
          {
            "accuracy": 0.27099609375,
            "f1": 0.2682752603993679,
            "f1_weighted": 0.26824507460979546
          },
          {
            "accuracy": 0.232421875,
            "f1": 0.2262941592241446,
            "f1_weighted": 0.22627298621222533
          },
          {
            "accuracy": 0.275390625,
            "f1": 0.2668314530073417,
            "f1_weighted": 0.26679338061126034
          },
          {
            "accuracy": 0.2685546875,
            "f1": 0.2653991641282223,
            "f1_weighted": 0.265377134696542
          },
          {
            "accuracy": 0.23486328125,
            "f1": 0.23139782929717573,
            "f1_weighted": 0.23139165170891982
          },
          {
            "accuracy": 0.2392578125,
            "f1": 0.2398062978333262,
            "f1_weighted": 0.2398073723486661
          },
          {
            "accuracy": 0.236328125,
            "f1": 0.2351619486918234,
            "f1_weighted": 0.23512797048069897
          },
          {
            "accuracy": 0.28271484375,
            "f1": 0.28090459279443103,
            "f1_weighted": 0.2809190635189721
          },
          {
            "accuracy": 0.30078125,
            "f1": 0.2962328764030731,
            "f1_weighted": 0.2962259575118533
          },
          {
            "accuracy": 0.25927734375,
            "f1": 0.24987031253984596,
            "f1_weighted": 0.24986465489349016
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}