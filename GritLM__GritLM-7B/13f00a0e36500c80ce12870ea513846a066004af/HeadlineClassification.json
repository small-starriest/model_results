{
  "dataset_revision": "2fe05ee6b5832cda29f2ef7aaad7b7fe6a3609eb",
  "evaluation_time": 34.218684673309326,
  "kg_co2_emissions": 0.006841106163696172,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.856640625,
        "f1": 0.85655177870273,
        "f1_weighted": 0.8565417396210966,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.856640625,
        "scores_per_experiment": [
          {
            "accuracy": 0.8505859375,
            "f1": 0.8510884884418957,
            "f1_weighted": 0.8510734107100054
          },
          {
            "accuracy": 0.86279296875,
            "f1": 0.862634949677982,
            "f1_weighted": 0.8626311146821234
          },
          {
            "accuracy": 0.84912109375,
            "f1": 0.8484616454497608,
            "f1_weighted": 0.8484457335227277
          },
          {
            "accuracy": 0.865234375,
            "f1": 0.8657064565723097,
            "f1_weighted": 0.8656960231578561
          },
          {
            "accuracy": 0.86767578125,
            "f1": 0.8682408735039889,
            "f1_weighted": 0.8682317971849683
          },
          {
            "accuracy": 0.853515625,
            "f1": 0.8531110222373887,
            "f1_weighted": 0.8530992679505259
          },
          {
            "accuracy": 0.83984375,
            "f1": 0.8397178057061078,
            "f1_weighted": 0.8397160431405494
          },
          {
            "accuracy": 0.86767578125,
            "f1": 0.867507142434622,
            "f1_weighted": 0.8674937258034945
          },
          {
            "accuracy": 0.8447265625,
            "f1": 0.8441363022471196,
            "f1_weighted": 0.8441315211367281
          },
          {
            "accuracy": 0.865234375,
            "f1": 0.8649131007561254,
            "f1_weighted": 0.8648987589219874
          }
        ]
      }
    ]
  },
  "task_name": "HeadlineClassification"
}