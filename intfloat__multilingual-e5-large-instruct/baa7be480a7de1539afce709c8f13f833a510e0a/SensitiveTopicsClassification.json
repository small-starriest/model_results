{
  "dataset_revision": "416b34a802308eac30e4192afc0ff99bb8dcc7f2",
  "evaluation_time": 8.117278575897217,
  "kg_co2_emissions": 0.0014326294367183823,
  "mteb_version": "1.14.12",
  "scores": {
    "test": [
      {
        "accuracy": 0.3228515625,
        "f1": 0.3813611390816865,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "lrap": 0.49556003146700717,
        "main_score": 0.3228515625,
        "scores_per_experiment": [
          {
            "accuracy": 0.31640625,
            "f1": 0.35191467316833114,
            "lrap": 0.4642062717013811
          },
          {
            "accuracy": 0.3076171875,
            "f1": 0.4012411752906404,
            "lrap": 0.5016140407986039
          },
          {
            "accuracy": 0.32568359375,
            "f1": 0.36086785536782107,
            "lrap": 0.482557508680548
          },
          {
            "accuracy": 0.30224609375,
            "f1": 0.35327589593254555,
            "lrap": 0.4758978949652705
          },
          {
            "accuracy": 0.3037109375,
            "f1": 0.37765512818581864,
            "lrap": 0.4768202039930483
          },
          {
            "accuracy": 0.31787109375,
            "f1": 0.37725002690780035,
            "lrap": 0.5002305772569372
          },
          {
            "accuracy": 0.328125,
            "f1": 0.3804759605049043,
            "lrap": 0.5050726996527711
          },
          {
            "accuracy": 0.3203125,
            "f1": 0.38957064221666693,
            "lrap": 0.5115356445312443
          },
          {
            "accuracy": 0.37158203125,
            "f1": 0.4121436422643029,
            "lrap": 0.5314670138888844
          },
          {
            "accuracy": 0.3349609375,
            "f1": 0.4092163909780333,
            "lrap": 0.5061984592013825
          }
        ]
      }
    ]
  },
  "task_name": "SensitiveTopicsClassification"
}