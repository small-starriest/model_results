{
  "dataset_revision": "c18a4f81a47ae6fa079fe9d32db288ddde38451d",
  "evaluation_time": 51.92582654953003,
  "kg_co2_emissions": 0.026536890814046168,
  "mteb_version": "1.12.75",
  "scores": {
    "validation": [
      {
        "accuracy": 0.9763513513513513,
        "f1": 0.971096096096096,
        "hf_subset": "ar-en",
        "languages": [
          "ara-Arab",
          "eng-Latn"
        ],
        "main_score": 0.971096096096096,
        "precision": 0.9689189189189189,
        "recall": 0.9763513513513513
      },
      {
        "accuracy": 0.9887387387387387,
        "f1": 0.9874624624624625,
        "hf_subset": "de-en",
        "languages": [
          "deu-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9874624624624625,
        "precision": 0.9871746746746747,
        "recall": 0.9887387387387387
      },
      {
        "accuracy": 0.9617117117117117,
        "f1": 0.953078078078078,
        "hf_subset": "en-ar",
        "languages": [
          "eng-Latn",
          "ara-Arab"
        ],
        "main_score": 0.953078078078078,
        "precision": 0.9493993993993993,
        "recall": 0.9617117117117117
      },
      {
        "accuracy": 0.9864864864864865,
        "f1": 0.9836711711711712,
        "hf_subset": "en-de",
        "languages": [
          "eng-Latn",
          "deu-Latn"
        ],
        "main_score": 0.9836711711711712,
        "precision": 0.9828078078078079,
        "recall": 0.9864864864864865
      },
      {
        "accuracy": 0.9797752808988764,
        "f1": 0.9749063670411986,
        "hf_subset": "en-fr",
        "languages": [
          "eng-Latn",
          "fra-Latn"
        ],
        "main_score": 0.9749063670411986,
        "precision": 0.9729213483146067,
        "recall": 0.9797752808988764
      },
      {
        "accuracy": 0.9741657696447793,
        "f1": 0.9675278076785074,
        "hf_subset": "en-it",
        "languages": [
          "eng-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9675278076785074,
        "precision": 0.9647290993900252,
        "recall": 0.9741657696447793
      },
      {
        "accuracy": 0.9380022962112514,
        "f1": 0.923438750304422,
        "hf_subset": "en-ja",
        "languages": [
          "eng-Latn",
          "jpn-Jpan"
        ],
        "main_score": 0.923438750304422,
        "precision": 0.9170685036356678,
        "recall": 0.9380022962112514
      },
      {
        "accuracy": 0.8998862343572241,
        "f1": 0.8813993174061433,
        "hf_subset": "en-ko",
        "languages": [
          "eng-Latn",
          "kor-Hang"
        ],
        "main_score": 0.8813993174061433,
        "precision": 0.8733896744135652,
        "recall": 0.8998862343572241
      },
      {
        "accuracy": 0.959122632103689,
        "f1": 0.948654037886341,
        "hf_subset": "en-nl",
        "languages": [
          "eng-Latn",
          "nld-Latn"
        ],
        "main_score": 0.948654037886341,
        "precision": 0.9440677966101695,
        "recall": 0.959122632103689
      },
      {
        "accuracy": 0.9781181619256017,
        "f1": 0.9735073460456392,
        "hf_subset": "en-ro",
        "languages": [
          "eng-Latn",
          "ron-Latn"
        ],
        "main_score": 0.9735073460456392,
        "precision": 0.9717724288840263,
        "recall": 0.9781181619256017
      },
      {
        "accuracy": 0.9306029579067122,
        "f1": 0.9163316900518267,
        "hf_subset": "en-zh",
        "languages": [
          "eng-Latn",
          "cmn-Hans"
        ],
        "main_score": 0.9163316900518267,
        "precision": 0.9104379977246871,
        "recall": 0.9306029579067122
      },
      {
        "accuracy": 0.9775280898876404,
        "f1": 0.9730337078651685,
        "hf_subset": "fr-en",
        "languages": [
          "fra-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9730337078651685,
        "precision": 0.9712359550561798,
        "recall": 0.9775280898876404
      },
      {
        "accuracy": 0.9752421959095802,
        "f1": 0.9690911886821467,
        "hf_subset": "it-en",
        "languages": [
          "ita-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9690911886821467,
        "precision": 0.9664872622891999,
        "recall": 0.9752421959095802
      },
      {
        "accuracy": 0.949050949050949,
        "f1": 0.9361471861471862,
        "hf_subset": "it-nl",
        "languages": [
          "ita-Latn",
          "nld-Latn"
        ],
        "main_score": 0.9361471861471862,
        "precision": 0.9302126444983587,
        "recall": 0.949050949050949
      },
      {
        "accuracy": 0.975929978118162,
        "f1": 0.9712496960855822,
        "hf_subset": "it-ro",
        "languages": [
          "ita-Latn",
          "ron-Latn"
        ],
        "main_score": 0.9712496960855822,
        "precision": 0.9693198395331875,
        "recall": 0.975929978118162
      },
      {
        "accuracy": 0.939150401836969,
        "f1": 0.9248547472428069,
        "hf_subset": "ja-en",
        "languages": [
          "jpn-Jpan",
          "eng-Latn"
        ],
        "main_score": 0.9248547472428069,
        "precision": 0.9188863375430538,
        "recall": 0.939150401836969
      },
      {
        "accuracy": 0.9010238907849829,
        "f1": 0.8837188724560738,
        "hf_subset": "ko-en",
        "languages": [
          "kor-Hang",
          "eng-Latn"
        ],
        "main_score": 0.8837188724560738,
        "precision": 0.8756636329161926,
        "recall": 0.9010238907849829
      },
      {
        "accuracy": 0.9621136590229312,
        "f1": 0.9530574941841143,
        "hf_subset": "nl-en",
        "languages": [
          "nld-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9530574941841143,
        "precision": 0.9489626359018184,
        "recall": 0.9621136590229312
      },
      {
        "accuracy": 0.942057942057942,
        "f1": 0.9283216783216783,
        "hf_subset": "nl-it",
        "languages": [
          "nld-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9283216783216783,
        "precision": 0.9218876361733503,
        "recall": 0.942057942057942
      },
      {
        "accuracy": 0.9572836801752465,
        "f1": 0.9461482292807595,
        "hf_subset": "nl-ro",
        "languages": [
          "nld-Latn",
          "ron-Latn"
        ],
        "main_score": 0.9461482292807595,
        "precision": 0.9410733844468785,
        "recall": 0.9572836801752465
      },
      {
        "accuracy": 0.9781181619256017,
        "f1": 0.9731426487443993,
        "hf_subset": "ro-en",
        "languages": [
          "ron-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9731426487443993,
        "precision": 0.9712253829321663,
        "recall": 0.9781181619256017
      },
      {
        "accuracy": 0.9704595185995624,
        "f1": 0.962800875273523,
        "hf_subset": "ro-it",
        "languages": [
          "ron-Latn",
          "ita-Latn"
        ],
        "main_score": 0.962800875273523,
        "precision": 0.9594091903719912,
        "recall": 0.9704595185995624
      },
      {
        "accuracy": 0.9605695509309967,
        "f1": 0.9495801387367653,
        "hf_subset": "ro-nl",
        "languages": [
          "ron-Latn",
          "nld-Latn"
        ],
        "main_score": 0.9495801387367653,
        "precision": 0.9447243519532675,
        "recall": 0.9605695509309967
      },
      {
        "accuracy": 0.9488054607508533,
        "f1": 0.9363887534535998,
        "hf_subset": "zh-en",
        "languages": [
          "cmn-Hans",
          "eng-Latn"
        ],
        "main_score": 0.9363887534535998,
        "precision": 0.9309821767159651,
        "recall": 0.9488054607508533
      }
    ]
  },
  "task_name": "IWSLT2017BitextMining"
}