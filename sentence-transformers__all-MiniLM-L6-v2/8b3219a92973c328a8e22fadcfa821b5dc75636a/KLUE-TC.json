{
  "dataset_revision": "349481ec73fff722f88e0453ca05c77a447d967c",
  "evaluation_time": 10.381521463394165,
  "kg_co2_emissions": 0.0015409383209598216,
  "mteb_version": "1.12.75",
  "scores": {
    "validation": [
      {
        "accuracy": 0.208642578125,
        "f1": 0.19835263038459883,
        "f1_weighted": 0.20770084878649492,
        "hf_subset": "default",
        "languages": [
          "kor-Hang"
        ],
        "main_score": 0.208642578125,
        "scores_per_experiment": [
          {
            "accuracy": 0.2197265625,
            "f1": 0.2044686697354779,
            "f1_weighted": 0.22436511205470516
          },
          {
            "accuracy": 0.203125,
            "f1": 0.19770882589121813,
            "f1_weighted": 0.2085699170780128
          },
          {
            "accuracy": 0.21142578125,
            "f1": 0.20123800803728834,
            "f1_weighted": 0.2066928530980252
          },
          {
            "accuracy": 0.1884765625,
            "f1": 0.17227250065781072,
            "f1_weighted": 0.16757706610598003
          },
          {
            "accuracy": 0.21826171875,
            "f1": 0.21234194186919447,
            "f1_weighted": 0.22178385269409576
          },
          {
            "accuracy": 0.20703125,
            "f1": 0.2005946655722067,
            "f1_weighted": 0.2022380329554207
          },
          {
            "accuracy": 0.2255859375,
            "f1": 0.19312031870894328,
            "f1_weighted": 0.2308637368777008
          },
          {
            "accuracy": 0.23779296875,
            "f1": 0.21988745736879647,
            "f1_weighted": 0.2582457682369093
          },
          {
            "accuracy": 0.193359375,
            "f1": 0.19322188422329065,
            "f1_weighted": 0.1925904906833297
          },
          {
            "accuracy": 0.181640625,
            "f1": 0.18867203178176162,
            "f1_weighted": 0.16408165808076944
          }
        ]
      }
    ]
  },
  "task_name": "KLUE-TC"
}