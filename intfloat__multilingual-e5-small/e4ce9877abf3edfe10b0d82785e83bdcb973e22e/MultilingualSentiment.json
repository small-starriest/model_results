{
  "dataset_revision": "46958b007a63fdbf239b7672c25d0bea67b5ea1a",
  "evaluation_time": 19.9017071723938,
  "kg_co2_emissions": 0.003024605437885471,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.6633666666666667,
        "f1": 0.6509248592813839,
        "f1_weighted": 0.6509248592813839,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.6633666666666667,
        "scores_per_experiment": [
          {
            "accuracy": 0.6623333333333333,
            "f1": 0.6478535114310976,
            "f1_weighted": 0.6478535114310977
          },
          {
            "accuracy": 0.6396666666666667,
            "f1": 0.6203619514356608,
            "f1_weighted": 0.6203619514356608
          },
          {
            "accuracy": 0.6586666666666666,
            "f1": 0.6492490721530766,
            "f1_weighted": 0.6492490721530766
          },
          {
            "accuracy": 0.665,
            "f1": 0.6520292566063485,
            "f1_weighted": 0.6520292566063486
          },
          {
            "accuracy": 0.673,
            "f1": 0.6607999914826496,
            "f1_weighted": 0.6607999914826496
          },
          {
            "accuracy": 0.677,
            "f1": 0.6705695477790857,
            "f1_weighted": 0.6705695477790857
          },
          {
            "accuracy": 0.6546666666666666,
            "f1": 0.6307283255899917,
            "f1_weighted": 0.6307283255899917
          },
          {
            "accuracy": 0.677,
            "f1": 0.6714635253566622,
            "f1_weighted": 0.6714635253566622
          },
          {
            "accuracy": 0.6683333333333333,
            "f1": 0.6589698076052166,
            "f1_weighted": 0.6589698076052167
          },
          {
            "accuracy": 0.658,
            "f1": 0.6472236033740497,
            "f1_weighted": 0.6472236033740497
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6600333333333334,
        "f1": 0.6468079844401883,
        "f1_weighted": 0.6468079844401883,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.6600333333333334,
        "scores_per_experiment": [
          {
            "accuracy": 0.655,
            "f1": 0.6396857651748934,
            "f1_weighted": 0.6396857651748935
          },
          {
            "accuracy": 0.6416666666666667,
            "f1": 0.6223430132990169,
            "f1_weighted": 0.6223430132990169
          },
          {
            "accuracy": 0.6673333333333333,
            "f1": 0.6591282668539838,
            "f1_weighted": 0.6591282668539838
          },
          {
            "accuracy": 0.6646666666666666,
            "f1": 0.6502701479180404,
            "f1_weighted": 0.6502701479180404
          },
          {
            "accuracy": 0.6643333333333333,
            "f1": 0.6518222813795177,
            "f1_weighted": 0.6518222813795177
          },
          {
            "accuracy": 0.6736666666666666,
            "f1": 0.6647565759581764,
            "f1_weighted": 0.6647565759581764
          },
          {
            "accuracy": 0.6423333333333333,
            "f1": 0.6182683511696538,
            "f1_weighted": 0.6182683511696538
          },
          {
            "accuracy": 0.669,
            "f1": 0.6631605643071671,
            "f1_weighted": 0.6631605643071671
          },
          {
            "accuracy": 0.67,
            "f1": 0.6595688955701805,
            "f1_weighted": 0.6595688955701805
          },
          {
            "accuracy": 0.6523333333333333,
            "f1": 0.6390759827712525,
            "f1_weighted": 0.6390759827712524
          }
        ]
      }
    ]
  },
  "task_name": "MultilingualSentiment"
}