{
  "dataset_revision": "566be6449bb30b9b9f2b59173391647fe0ca3224",
  "evaluation_time": 9.635895252227783,
  "kg_co2_emissions": 0.0014207042455745937,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.4021484375,
        "f1": 0.3892201589975233,
        "f1_weighted": 0.40259715120972883,
        "hf_subset": "default",
        "languages": [
          "urd-Latn"
        ],
        "main_score": 0.3892201589975233,
        "scores_per_experiment": [
          {
            "accuracy": 0.4375,
            "f1": 0.4080477109614877,
            "f1_weighted": 0.4265197674826946
          },
          {
            "accuracy": 0.40576171875,
            "f1": 0.40525869919396085,
            "f1_weighted": 0.4095861823213417
          },
          {
            "accuracy": 0.40234375,
            "f1": 0.38866791170938636,
            "f1_weighted": 0.4054049626508764
          },
          {
            "accuracy": 0.4140625,
            "f1": 0.4032634199594853,
            "f1_weighted": 0.4157358761058919
          },
          {
            "accuracy": 0.32568359375,
            "f1": 0.3124709839041773,
            "f1_weighted": 0.32742999905964904
          },
          {
            "accuracy": 0.44140625,
            "f1": 0.42979653314122307,
            "f1_weighted": 0.4486614332232751
          },
          {
            "accuracy": 0.3974609375,
            "f1": 0.38537388351619306,
            "f1_weighted": 0.39917588440037466
          },
          {
            "accuracy": 0.38916015625,
            "f1": 0.358391781069622,
            "f1_weighted": 0.3785934473831945
          },
          {
            "accuracy": 0.40234375,
            "f1": 0.39606884441317564,
            "f1_weighted": 0.4067038449594263
          },
          {
            "accuracy": 0.40576171875,
            "f1": 0.4048618221065217,
            "f1_weighted": 0.4081601145105641
          }
        ]
      }
    ]
  },
  "task_name": "UrduRomanSentimentClassification"
}