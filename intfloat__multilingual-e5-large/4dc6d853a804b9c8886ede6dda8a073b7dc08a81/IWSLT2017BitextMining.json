{
  "dataset_revision": "c18a4f81a47ae6fa079fe9d32db288ddde38451d",
  "evaluation_time": 52.24266767501831,
  "kg_co2_emissions": 0.009330402488545216,
  "mteb_version": "1.12.75",
  "scores": {
    "validation": [
      {
        "accuracy": 0.9684684684684685,
        "f1": 0.9612987987987988,
        "hf_subset": "ar-en",
        "languages": [
          "ara-Arab",
          "eng-Latn"
        ],
        "main_score": 0.9612987987987988,
        "precision": 0.9583708708708708,
        "recall": 0.9684684684684685
      },
      {
        "accuracy": 0.9853603603603603,
        "f1": 0.9829374829374828,
        "hf_subset": "de-en",
        "languages": [
          "deu-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9829374829374828,
        "precision": 0.9820945945945946,
        "recall": 0.9853603603603603
      },
      {
        "accuracy": 0.9797297297297297,
        "f1": 0.9758258258258258,
        "hf_subset": "en-ar",
        "languages": [
          "eng-Latn",
          "ara-Arab"
        ],
        "main_score": 0.9758258258258258,
        "precision": 0.9742242242242242,
        "recall": 0.9797297297297297
      },
      {
        "accuracy": 0.9853603603603603,
        "f1": 0.9829374829374828,
        "hf_subset": "en-de",
        "languages": [
          "eng-Latn",
          "deu-Latn"
        ],
        "main_score": 0.9829374829374828,
        "precision": 0.9820945945945946,
        "recall": 0.9853603603603603
      },
      {
        "accuracy": 0.9741573033707865,
        "f1": 0.9687436159346271,
        "hf_subset": "en-fr",
        "languages": [
          "eng-Latn",
          "fra-Latn"
        ],
        "main_score": 0.9687436159346271,
        "precision": 0.9664044943820225,
        "recall": 0.9741573033707865
      },
      {
        "accuracy": 0.9666307857911733,
        "f1": 0.9585934696806602,
        "hf_subset": "en-it",
        "languages": [
          "eng-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9585934696806602,
        "precision": 0.955089104174142,
        "recall": 0.9666307857911733
      },
      {
        "accuracy": 0.9173363949483353,
        "f1": 0.8981038861635876,
        "hf_subset": "en-ja",
        "languages": [
          "eng-Latn",
          "jpn-Jpan"
        ],
        "main_score": 0.8981038861635876,
        "precision": 0.88964791427478,
        "recall": 0.9173363949483353
      },
      {
        "accuracy": 0.89419795221843,
        "f1": 0.8707036232633502,
        "hf_subset": "en-ko",
        "languages": [
          "eng-Latn",
          "kor-Hang"
        ],
        "main_score": 0.8707036232633502,
        "precision": 0.8598028062191885,
        "recall": 0.89419795221843
      },
      {
        "accuracy": 0.9571286141575274,
        "f1": 0.9461615154536391,
        "hf_subset": "en-nl",
        "languages": [
          "eng-Latn",
          "nld-Latn"
        ],
        "main_score": 0.9461615154536391,
        "precision": 0.9410767696909272,
        "recall": 0.9571286141575274
      },
      {
        "accuracy": 0.9770240700218819,
        "f1": 0.9711889132020423,
        "hf_subset": "en-ro",
        "languages": [
          "eng-Latn",
          "ron-Latn"
        ],
        "main_score": 0.9711889132020423,
        "precision": 0.9687089715536105,
        "recall": 0.9770240700218819
      },
      {
        "accuracy": 0.919226393629124,
        "f1": 0.9013065811700623,
        "hf_subset": "en-zh",
        "languages": [
          "eng-Latn",
          "cmn-Hans"
        ],
        "main_score": 0.9013065811700623,
        "precision": 0.8936480849450132,
        "recall": 0.919226393629124
      },
      {
        "accuracy": 0.9696629213483146,
        "f1": 0.9621187800963081,
        "hf_subset": "fr-en",
        "languages": [
          "fra-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9621187800963081,
        "precision": 0.9588389513108614,
        "recall": 0.9696629213483146
      },
      {
        "accuracy": 0.9687836383207751,
        "f1": 0.9615177610333692,
        "hf_subset": "it-en",
        "languages": [
          "ita-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9615177610333692,
        "precision": 0.9581731508534522,
        "recall": 0.9687836383207751
      },
      {
        "accuracy": 0.9370629370629371,
        "f1": 0.9224299510013795,
        "hf_subset": "it-nl",
        "languages": [
          "ita-Latn",
          "nld-Latn"
        ],
        "main_score": 0.9224299510013795,
        "precision": 0.9158341658341659,
        "recall": 0.9370629370629371
      },
      {
        "accuracy": 0.9562363238512035,
        "f1": 0.9448786078983015,
        "hf_subset": "it-ro",
        "languages": [
          "ita-Latn",
          "ron-Latn"
        ],
        "main_score": 0.9448786078983015,
        "precision": 0.9394602479941648,
        "recall": 0.9562363238512035
      },
      {
        "accuracy": 0.9150401836969001,
        "f1": 0.8979125352259681,
        "hf_subset": "ja-en",
        "languages": [
          "jpn-Jpan",
          "eng-Latn"
        ],
        "main_score": 0.8979125352259681,
        "precision": 0.8907960199004975,
        "recall": 0.9150401836969001
      },
      {
        "accuracy": 0.875995449374289,
        "f1": 0.8518392112248767,
        "hf_subset": "ko-en",
        "languages": [
          "kor-Hang",
          "eng-Latn"
        ],
        "main_score": 0.8518392112248767,
        "precision": 0.8415560611806345,
        "recall": 0.875995449374289
      },
      {
        "accuracy": 0.9491525423728814,
        "f1": 0.9376370887337987,
        "hf_subset": "nl-en",
        "languages": [
          "nld-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9376370887337987,
        "precision": 0.932761240089256,
        "recall": 0.9491525423728814
      },
      {
        "accuracy": 0.9300699300699301,
        "f1": 0.9126540126540126,
        "hf_subset": "nl-it",
        "languages": [
          "nld-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9126540126540126,
        "precision": 0.9049117549117549,
        "recall": 0.9300699300699301
      },
      {
        "accuracy": 0.940854326396495,
        "f1": 0.9263964950711938,
        "hf_subset": "nl-ro",
        "languages": [
          "nld-Latn",
          "ron-Latn"
        ],
        "main_score": 0.9263964950711938,
        "precision": 0.9200803212851405,
        "recall": 0.940854326396495
      },
      {
        "accuracy": 0.9792122538293216,
        "f1": 0.9748358862144421,
        "hf_subset": "ro-en",
        "languages": [
          "ron-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9748358862144421,
        "precision": 0.9728665207877463,
        "recall": 0.9792122538293216
      },
      {
        "accuracy": 0.9617067833698031,
        "f1": 0.9532275711159738,
        "hf_subset": "ro-it",
        "languages": [
          "ron-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9532275711159738,
        "precision": 0.9492810253204126,
        "recall": 0.9617067833698031
      },
      {
        "accuracy": 0.9496166484118291,
        "f1": 0.9368382621394669,
        "hf_subset": "ro-nl",
        "languages": [
          "ron-Latn",
          "nld-Latn"
        ],
        "main_score": 0.9368382621394669,
        "precision": 0.9310332238043081,
        "recall": 0.9496166484118291
      },
      {
        "accuracy": 0.9249146757679181,
        "f1": 0.9069992957364971,
        "hf_subset": "zh-en",
        "languages": [
          "cmn-Hans",
          "eng-Latn"
        ],
        "main_score": 0.9069992957364971,
        "precision": 0.8995070155479713,
        "recall": 0.9249146757679181
      }
    ]
  },
  "task_name": "IWSLT2017BitextMining"
}