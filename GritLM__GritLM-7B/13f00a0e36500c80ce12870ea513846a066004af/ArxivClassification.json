{
  "dataset_revision": "f9bd92144ed76200d6eb3ce73a8bd4eba9ffdc85",
  "evaluation_time": 80.67138075828552,
  "kg_co2_emissions": 0.018136037853595167,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.7358,
        "f1": 0.7187597987018604,
        "f1_weighted": 0.7265115226911065,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7358,
        "scores_per_experiment": [
          {
            "accuracy": 0.7464,
            "f1": 0.7314943059063357,
            "f1_weighted": 0.7399822476302828
          },
          {
            "accuracy": 0.726,
            "f1": 0.7056498850844352,
            "f1_weighted": 0.7155158451840296
          },
          {
            "accuracy": 0.7348,
            "f1": 0.7164378723530335,
            "f1_weighted": 0.7250212791275791
          },
          {
            "accuracy": 0.7268,
            "f1": 0.7089280708903624,
            "f1_weighted": 0.7173915861486803
          },
          {
            "accuracy": 0.7392,
            "f1": 0.718225018610241,
            "f1_weighted": 0.7272595722178791
          },
          {
            "accuracy": 0.7188,
            "f1": 0.6976328242308371,
            "f1_weighted": 0.7076227457495985
          },
          {
            "accuracy": 0.734,
            "f1": 0.7170935777926654,
            "f1_weighted": 0.7230695119226248
          },
          {
            "accuracy": 0.7572,
            "f1": 0.7424452814038229,
            "f1_weighted": 0.7499000595771634
          },
          {
            "accuracy": 0.726,
            "f1": 0.7117405644358757,
            "f1_weighted": 0.7170993521375012
          },
          {
            "accuracy": 0.7488,
            "f1": 0.7379505863109941,
            "f1_weighted": 0.7422530272157256
          }
        ]
      }
    ]
  },
  "task_name": "ArxivClassification"
}