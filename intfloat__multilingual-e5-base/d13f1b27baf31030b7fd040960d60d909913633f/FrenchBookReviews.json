{
  "dataset_revision": "534725e03fec6f560dbe8166e8ae3825314a6290",
  "evaluation_time": 10.864496231079102,
  "kg_co2_emissions": 0.0016666276626033985,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.435595703125,
        "f1": 0.35233832812282706,
        "f1_weighted": 0.47119312529299934,
        "hf_subset": "default",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.435595703125,
        "scores_per_experiment": [
          {
            "accuracy": 0.38720703125,
            "f1": 0.31793214157196326,
            "f1_weighted": 0.4409827717355477
          },
          {
            "accuracy": 0.5048828125,
            "f1": 0.39854787539013853,
            "f1_weighted": 0.5320317442988325
          },
          {
            "accuracy": 0.50048828125,
            "f1": 0.3971868404093468,
            "f1_weighted": 0.5284959765815644
          },
          {
            "accuracy": 0.3369140625,
            "f1": 0.30523028004163044,
            "f1_weighted": 0.37054456755329473
          },
          {
            "accuracy": 0.54345703125,
            "f1": 0.36872826134812664,
            "f1_weighted": 0.5648805532882463
          },
          {
            "accuracy": 0.41259765625,
            "f1": 0.36211886934528303,
            "f1_weighted": 0.456071171654975
          },
          {
            "accuracy": 0.44677734375,
            "f1": 0.3251172113222353,
            "f1_weighted": 0.4770509395884772
          },
          {
            "accuracy": 0.435546875,
            "f1": 0.3643443342703497,
            "f1_weighted": 0.47132794412627954
          },
          {
            "accuracy": 0.4296875,
            "f1": 0.3527621822818534,
            "f1_weighted": 0.4783932798025635
          },
          {
            "accuracy": 0.3583984375,
            "f1": 0.3314152852473436,
            "f1_weighted": 0.392152304300212
          }
        ]
      }
    ]
  },
  "task_name": "FrenchBookReviews"
}