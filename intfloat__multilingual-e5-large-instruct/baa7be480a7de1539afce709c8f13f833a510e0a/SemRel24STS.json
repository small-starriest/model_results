{
  "dataset_revision": "ef5c383d1b87eb8feccde3dfb7f95e42b1b050dd",
  "evaluation_time": 36.07847046852112,
  "kg_co2_emissions": 0.006954102533503394,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "cosine_pearson": 0.8387954588035434,
        "cosine_spearman": 0.8170049839927372,
        "euclidean_pearson": 0.82125782400208,
        "euclidean_spearman": 0.8170049839927372,
        "hf_subset": "afr",
        "languages": [
          "afr-Latn"
        ],
        "main_score": 0.8170049839927372,
        "manhattan_pearson": 0.8190590464631429,
        "manhattan_spearman": 0.8140769217411346,
        "pearson": 0.8387954588035434,
        "spearman": 0.8170049839927372
      },
      {
        "cosine_pearson": 0.8216492664147119,
        "cosine_spearman": 0.818744938323323,
        "euclidean_pearson": 0.8179910107834588,
        "euclidean_spearman": 0.818744938323323,
        "hf_subset": "amh",
        "languages": [
          "amh-Ethi"
        ],
        "main_score": 0.818744938323323,
        "manhattan_pearson": 0.8165502576739908,
        "manhattan_spearman": 0.8154199273581055,
        "pearson": 0.8216492664147119,
        "spearman": 0.818744938323323
      },
      {
        "cosine_pearson": 0.6403433890774903,
        "cosine_spearman": 0.6436771813372034,
        "euclidean_pearson": 0.6353616468578481,
        "euclidean_spearman": 0.6436771813372034,
        "hf_subset": "arb",
        "languages": [
          "arb-Arab"
        ],
        "main_score": 0.6436771813372034,
        "manhattan_pearson": 0.6292534499435544,
        "manhattan_spearman": 0.6382146292347779,
        "pearson": 0.6403433890774903,
        "spearman": 0.6436771813372034
      },
      {
        "cosine_pearson": 0.5546502084179906,
        "cosine_spearman": 0.506139228045977,
        "euclidean_pearson": 0.5602542588334822,
        "euclidean_spearman": 0.506139228045977,
        "hf_subset": "arq",
        "languages": [
          "arq-Arab"
        ],
        "main_score": 0.506139228045977,
        "manhattan_pearson": 0.5576748064167903,
        "manhattan_spearman": 0.501792396350985,
        "pearson": 0.5546502084179906,
        "spearman": 0.506139228045977
      },
      {
        "cosine_pearson": 0.4706995177268383,
        "cosine_spearman": 0.45399831742658875,
        "euclidean_pearson": 0.4774044537075715,
        "euclidean_spearman": 0.45399831742658875,
        "hf_subset": "ary",
        "languages": [
          "ary-Arab"
        ],
        "main_score": 0.45399831742658875,
        "manhattan_pearson": 0.47422484571775814,
        "manhattan_spearman": 0.4505050222155913,
        "pearson": 0.4706995177268383,
        "spearman": 0.45399831742658875
      },
      {
        "cosine_pearson": 0.8281026335701728,
        "cosine_spearman": 0.8130913959641606,
        "euclidean_pearson": 0.8233036767859208,
        "euclidean_spearman": 0.8130909879052866,
        "hf_subset": "eng",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8130913959641606,
        "manhattan_pearson": 0.8228980844091566,
        "manhattan_spearman": 0.8123305017157079,
        "pearson": 0.8281026335701728,
        "spearman": 0.8130913959641606
      },
      {
        "cosine_pearson": 0.6492919383124852,
        "cosine_spearman": 0.6371212084711662,
        "euclidean_pearson": 0.6390630359515626,
        "euclidean_spearman": 0.6371212084711662,
        "hf_subset": "hau",
        "languages": [
          "hau-Latn"
        ],
        "main_score": 0.6371212084711662,
        "manhattan_pearson": 0.6365459851621801,
        "manhattan_spearman": 0.6352891867376941,
        "pearson": 0.6492919383124852,
        "spearman": 0.6371212084711662
      },
      {
        "cosine_pearson": 0.8025585446321312,
        "cosine_spearman": 0.8109137654184349,
        "euclidean_pearson": 0.7776010551108856,
        "euclidean_spearman": 0.8109137654184349,
        "hf_subset": "hin",
        "languages": [
          "hin-Deva"
        ],
        "main_score": 0.8109137654184349,
        "manhattan_pearson": 0.7760975544026179,
        "manhattan_spearman": 0.8085095121914675,
        "pearson": 0.8025585446321312,
        "spearman": 0.8109137654184349
      },
      {
        "cosine_pearson": 0.4892525440310187,
        "cosine_spearman": 0.48935710089834167,
        "euclidean_pearson": 0.5086032597374244,
        "euclidean_spearman": 0.48935710089834167,
        "hf_subset": "ind",
        "languages": [
          "ind-Latn"
        ],
        "main_score": 0.48935710089834167,
        "manhattan_pearson": 0.5105547457323194,
        "manhattan_spearman": 0.4937524236857825,
        "pearson": 0.4892525440310187,
        "spearman": 0.48935710089834167
      },
      {
        "cosine_pearson": 0.6381874159942972,
        "cosine_spearman": 0.6580216005078864,
        "euclidean_pearson": 0.6481034965162702,
        "euclidean_spearman": 0.6580216005078864,
        "hf_subset": "kin",
        "languages": [
          "kin-Latn"
        ],
        "main_score": 0.6580216005078864,
        "manhattan_pearson": 0.6460214349479817,
        "manhattan_spearman": 0.6553538860410144,
        "pearson": 0.6381874159942972,
        "spearman": 0.6580216005078864
      },
      {
        "cosine_pearson": 0.845457680409904,
        "cosine_spearman": 0.8315010212590723,
        "euclidean_pearson": 0.8306183260056184,
        "euclidean_spearman": 0.8315010212590723,
        "hf_subset": "mar",
        "languages": [
          "mar-Deva"
        ],
        "main_score": 0.8315010212590723,
        "manhattan_pearson": 0.8314653587309693,
        "manhattan_spearman": 0.831204806461058,
        "pearson": 0.845457680409904,
        "spearman": 0.8315010212590723
      },
      {
        "cosine_pearson": 0.8390375452571047,
        "cosine_spearman": 0.8212000082387925,
        "euclidean_pearson": 0.8147374237944839,
        "euclidean_spearman": 0.8212000082387925,
        "hf_subset": "tel",
        "languages": [
          "tel-Telu"
        ],
        "main_score": 0.8212000082387925,
        "manhattan_pearson": 0.8134155675927228,
        "manhattan_spearman": 0.8178038887938246,
        "pearson": 0.8390375452571047,
        "spearman": 0.8212000082387925
      }
    ]
  },
  "task_name": "SemRel24STS"
}