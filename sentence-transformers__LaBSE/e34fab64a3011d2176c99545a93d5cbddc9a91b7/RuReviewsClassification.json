{
  "dataset_revision": "f6d2c31f4dc6b88f468552750bfec05b4b41b05a",
  "evaluation_time": 10.833472967147827,
  "kg_co2_emissions": 0.0016370944803392646,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.580126953125,
        "f1": 0.5727391911295525,
        "f1_weighted": 0.572734471473742,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.580126953125,
        "scores_per_experiment": [
          {
            "accuracy": 0.60546875,
            "f1": 0.5904962605337559,
            "f1_weighted": 0.5904897503286797
          },
          {
            "accuracy": 0.5322265625,
            "f1": 0.518096140087109,
            "f1_weighted": 0.5181102118331817
          },
          {
            "accuracy": 0.5517578125,
            "f1": 0.5560668590744411,
            "f1_weighted": 0.5560400037518488
          },
          {
            "accuracy": 0.634765625,
            "f1": 0.6357204104206892,
            "f1_weighted": 0.6357275577787644
          },
          {
            "accuracy": 0.63037109375,
            "f1": 0.6307989672418625,
            "f1_weighted": 0.6307832559082174
          },
          {
            "accuracy": 0.59228515625,
            "f1": 0.5790052688323716,
            "f1_weighted": 0.5790206032826254
          },
          {
            "accuracy": 0.58837890625,
            "f1": 0.559291695826951,
            "f1_weighted": 0.5592672031313985
          },
          {
            "accuracy": 0.548828125,
            "f1": 0.5439538765868196,
            "f1_weighted": 0.5439421076553149
          },
          {
            "accuracy": 0.4814453125,
            "f1": 0.4793316555398963,
            "f1_weighted": 0.4793121513870821
          },
          {
            "accuracy": 0.6357421875,
            "f1": 0.6346307771516285,
            "f1_weighted": 0.6346518696803068
          }
        ]
      }
    ]
  },
  "task_name": "RuReviewsClassification"
}