{
  "dataset_revision": "f8650438298df086750ff4973661bb58a201a5ee",
  "evaluation_time": 111.26987028121948,
  "kg_co2_emissions": 0.017441058280015197,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.02865612648221344,
        "f1": 0.02316316753075646,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.02316316753075646,
        "precision": 0.02159273898404333,
        "recall": 0.02865612648221344
      },
      {
        "accuracy": 0.05533596837944664,
        "f1": 0.01797989133266919,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.01797989133266919,
        "precision": 0.013148641542172103,
        "recall": 0.05533596837944664
      },
      {
        "accuracy": 0.039525691699604744,
        "f1": 0.033945959875976144,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.033945959875976144,
        "precision": 0.03243988801054019,
        "recall": 0.039525691699604744
      },
      {
        "accuracy": 0.06521739130434782,
        "f1": 0.02460728603216121,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.02460728603216121,
        "precision": 0.017863429278335126,
        "recall": 0.06521739130434782
      },
      {
        "accuracy": 0.025691699604743084,
        "f1": 0.02084425810472741,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.02084425810472741,
        "precision": 0.019810947030787812,
        "recall": 0.025691699604743084
      },
      {
        "accuracy": 0.07608695652173914,
        "f1": 0.0318348410490779,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.0318348410490779,
        "precision": 0.02427578490324761,
        "recall": 0.07608695652173914
      },
      {
        "accuracy": 0.05533596837944664,
        "f1": 0.04319019401381763,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.04319019401381763,
        "precision": 0.040069562080431644,
        "recall": 0.05533596837944664
      },
      {
        "accuracy": 0.07905138339920949,
        "f1": 0.03322627523540976,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.03322627523540976,
        "precision": 0.026216768984086404,
        "recall": 0.07905138339920949
      },
      {
        "accuracy": 0.040513833992094864,
        "f1": 0.03213518236295364,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.03213518236295364,
        "precision": 0.030570260793525598,
        "recall": 0.040513833992094864
      },
      {
        "accuracy": 0.06126482213438735,
        "f1": 0.026794232823987924,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.026794232823987924,
        "precision": 0.021204838235645854,
        "recall": 0.06126482213438735
      },
      {
        "accuracy": 0.021739130434782608,
        "f1": 0.017418308625655327,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.017418308625655327,
        "precision": 0.016911118049758832,
        "recall": 0.021739130434782608
      },
      {
        "accuracy": 0.05632411067193676,
        "f1": 0.022694253116859187,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.022694253116859187,
        "precision": 0.01732678634193672,
        "recall": 0.05632411067193676
      },
      {
        "accuracy": 0.0533596837944664,
        "f1": 0.041139997973105344,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.041139997973105344,
        "precision": 0.03810599338549649,
        "recall": 0.0533596837944664
      },
      {
        "accuracy": 0.07608695652173914,
        "f1": 0.03331031801887341,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.03331031801887341,
        "precision": 0.025857559331206977,
        "recall": 0.07608695652173914
      },
      {
        "accuracy": 0.09980237154150198,
        "f1": 0.08630053700168491,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.08630053700168491,
        "precision": 0.08216299005191667,
        "recall": 0.09980237154150198
      },
      {
        "accuracy": 0.12055335968379446,
        "f1": 0.06004793213151973,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.06004793213151973,
        "precision": 0.04949602119560603,
        "recall": 0.12055335968379446
      },
      {
        "accuracy": 0.019762845849802372,
        "f1": 0.017294557361866806,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.017294557361866806,
        "precision": 0.016634763294221522,
        "recall": 0.019762845849802372
      },
      {
        "accuracy": 0.05138339920948617,
        "f1": 0.019932757716166885,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.019932757716166885,
        "precision": 0.01589473619031244,
        "recall": 0.05138339920948617
      },
      {
        "accuracy": 0.025691699604743084,
        "f1": 0.01924698334235501,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.01924698334235501,
        "precision": 0.017949604743083002,
        "recall": 0.025691699604743084
      },
      {
        "accuracy": 0.04743083003952569,
        "f1": 0.01772222453715753,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.01772222453715753,
        "precision": 0.013621703063274766,
        "recall": 0.04743083003952569
      },
      {
        "accuracy": 0.036561264822134384,
        "f1": 0.03222920146005907,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.03222920146005907,
        "precision": 0.031515121365882484,
        "recall": 0.036561264822134384
      },
      {
        "accuracy": 0.07608695652173914,
        "f1": 0.03269453430990164,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.03269453430990164,
        "precision": 0.024774492300113637,
        "recall": 0.07608695652173914
      },
      {
        "accuracy": 0.05237154150197629,
        "f1": 0.04507716523219428,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.04507716523219428,
        "precision": 0.044015293307951675,
        "recall": 0.05237154150197629
      },
      {
        "accuracy": 0.09090909090909091,
        "f1": 0.043226987943448876,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.043226987943448876,
        "precision": 0.03532195598926271,
        "recall": 0.09090909090909091
      },
      {
        "accuracy": 0.05632411067193676,
        "f1": 0.046383565838635066,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.046383565838635066,
        "precision": 0.04385532830667507,
        "recall": 0.05632411067193676
      },
      {
        "accuracy": 0.08695652173913043,
        "f1": 0.04278848421328537,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.04278848421328537,
        "precision": 0.034602976186823294,
        "recall": 0.08695652173913043
      },
      {
        "accuracy": 0.04841897233201581,
        "f1": 0.03859423765489906,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.03859423765489906,
        "precision": 0.03651558625997355,
        "recall": 0.04841897233201581
      },
      {
        "accuracy": 0.08794466403162056,
        "f1": 0.04346215033404916,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.04346215033404916,
        "precision": 0.03604399447565952,
        "recall": 0.08794466403162056
      },
      {
        "accuracy": 0.10177865612648221,
        "f1": 0.09539107061955601,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.09539107061955601,
        "precision": 0.09349593202735064,
        "recall": 0.10177865612648221
      },
      {
        "accuracy": 0.1373517786561265,
        "f1": 0.06852929235442023,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.06852929235442023,
        "precision": 0.054155061368842294,
        "recall": 0.1373517786561265
      },
      {
        "accuracy": 0.05632411067193676,
        "f1": 0.049659752197837366,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.049659752197837366,
        "precision": 0.047919535189012194,
        "recall": 0.05632411067193676
      },
      {
        "accuracy": 0.08201581027667984,
        "f1": 0.03295071457103601,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.03295071457103601,
        "precision": 0.0256274376488202,
        "recall": 0.08201581027667984
      },
      {
        "accuracy": 0.024703557312252964,
        "f1": 0.020876340714750388,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.020876340714750388,
        "precision": 0.0202519364401377,
        "recall": 0.024703557312252964
      },
      {
        "accuracy": 0.06027667984189723,
        "f1": 0.02628742876650544,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.02628742876650544,
        "precision": 0.02054015171128863,
        "recall": 0.06027667984189723
      },
      {
        "accuracy": 0.01383399209486166,
        "f1": 0.009970716985827542,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.009970716985827542,
        "precision": 0.009630983212345104,
        "recall": 0.01383399209486166
      },
      {
        "accuracy": 0.05928853754940711,
        "f1": 0.027753429015635475,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.027753429015635475,
        "precision": 0.022274591321482348,
        "recall": 0.05928853754940711
      },
      {
        "accuracy": 0.003952569169960474,
        "f1": 0.003952569169960474,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.003952569169960474,
        "precision": 0.003952569169960474,
        "recall": 0.003952569169960474
      },
      {
        "accuracy": 0.030632411067193676,
        "f1": 0.010876565466936394,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.010876565466936394,
        "precision": 0.009057965740139876,
        "recall": 0.030632411067193676
      },
      {
        "accuracy": 0.024703557312252964,
        "f1": 0.021166113359965006,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.021166113359965006,
        "precision": 0.020513948813823995,
        "recall": 0.024703557312252964
      },
      {
        "accuracy": 0.07608695652173914,
        "f1": 0.037927326005847764,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.037927326005847764,
        "precision": 0.031386993731727764,
        "recall": 0.07608695652173914
      },
      {
        "accuracy": 0.02865612648221344,
        "f1": 0.02526255541378415,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.02526255541378415,
        "precision": 0.025037979539051423,
        "recall": 0.02865612648221344
      },
      {
        "accuracy": 0.07114624505928854,
        "f1": 0.031189371329640433,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.031189371329640433,
        "precision": 0.02456985708697415,
        "recall": 0.07114624505928854
      },
      {
        "accuracy": 0.05434782608695652,
        "f1": 0.04654860803900657,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.04654860803900657,
        "precision": 0.044852491391813645,
        "recall": 0.05434782608695652
      },
      {
        "accuracy": 0.08695652173913043,
        "f1": 0.04111236741445842,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.04111236741445842,
        "precision": 0.03306798322533477,
        "recall": 0.08695652173913043
      },
      {
        "accuracy": 0.019762845849802372,
        "f1": 0.015606675459081606,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.015606675459081606,
        "precision": 0.0150996397531945,
        "recall": 0.019762845849802372
      },
      {
        "accuracy": 0.05039525691699605,
        "f1": 0.02086819636342571,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.02086819636342571,
        "precision": 0.015957431570135443,
        "recall": 0.05039525691699605
      },
      {
        "accuracy": 0.03162055335968379,
        "f1": 0.02694904958549708,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.02694904958549708,
        "precision": 0.02607520378702148,
        "recall": 0.03162055335968379
      },
      {
        "accuracy": 0.08102766798418973,
        "f1": 0.03779336738771452,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.03779336738771452,
        "precision": 0.02965255790003554,
        "recall": 0.08102766798418973
      },
      {
        "accuracy": 0.036561264822134384,
        "f1": 0.031099758562277303,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.031099758562277303,
        "precision": 0.029928065224632244,
        "recall": 0.036561264822134384
      },
      {
        "accuracy": 0.08201581027667984,
        "f1": 0.03749429671802399,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.03749429671802399,
        "precision": 0.030350366123717484,
        "recall": 0.08201581027667984
      },
      {
        "accuracy": 0.030632411067193676,
        "f1": 0.02310904104382365,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.02310904104382365,
        "precision": 0.021513212674622573,
        "recall": 0.030632411067193676
      },
      {
        "accuracy": 0.05138339920948617,
        "f1": 0.016195268795454715,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.016195268795454715,
        "precision": 0.011863310916000461,
        "recall": 0.05138339920948617
      },
      {
        "accuracy": 0.023715415019762844,
        "f1": 0.019489602498714148,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.019489602498714148,
        "precision": 0.01900086898181571,
        "recall": 0.023715415019762844
      },
      {
        "accuracy": 0.058300395256917,
        "f1": 0.02529569732722642,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.02529569732722642,
        "precision": 0.0200390730146815,
        "recall": 0.058300395256917
      },
      {
        "accuracy": 0.030632411067193676,
        "f1": 0.02626644714099363,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.02626644714099363,
        "precision": 0.02556839191357427,
        "recall": 0.030632411067193676
      },
      {
        "accuracy": 0.07905138339920949,
        "f1": 0.036027532032343365,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.036027532032343365,
        "precision": 0.02808943232420823,
        "recall": 0.07905138339920949
      },
      {
        "accuracy": 0.012845849802371542,
        "f1": 0.010869565217391304,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.010869565217391304,
        "precision": 0.009881422924901186,
        "recall": 0.012845849802371542
      },
      {
        "accuracy": 0.024703557312252964,
        "f1": 0.0036915382739109636,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.0036915382739109636,
        "precision": 0.0020550150491512047,
        "recall": 0.024703557312252964
      }
    ],
    "validation": [
      {
        "accuracy": 0.022066198595787363,
        "f1": 0.017096931012591035,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.017096931012591035,
        "precision": 0.015963352241598745,
        "recall": 0.022066198595787363
      },
      {
        "accuracy": 0.04513540621865597,
        "f1": 0.013543647428024342,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.013543647428024342,
        "precision": 0.009976044936503873,
        "recall": 0.04513540621865597
      },
      {
        "accuracy": 0.03610832497492478,
        "f1": 0.027349817944057463,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.027349817944057463,
        "precision": 0.025435748675243276,
        "recall": 0.03610832497492478
      },
      {
        "accuracy": 0.06018054162487462,
        "f1": 0.025414352604869878,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.025414352604869878,
        "precision": 0.020845494128501046,
        "recall": 0.06018054162487462
      },
      {
        "accuracy": 0.028084252758274825,
        "f1": 0.022392661962857698,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.022392661962857698,
        "precision": 0.021434417161069707,
        "recall": 0.028084252758274825
      },
      {
        "accuracy": 0.06820461384152457,
        "f1": 0.032037955338549716,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.032037955338549716,
        "precision": 0.025642067230068337,
        "recall": 0.06820461384152457
      },
      {
        "accuracy": 0.04012036108324975,
        "f1": 0.02735262528302909,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.02735262528302909,
        "precision": 0.024850412668781494,
        "recall": 0.04012036108324975
      },
      {
        "accuracy": 0.07622868605817452,
        "f1": 0.03379615616924598,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.03379615616924598,
        "precision": 0.02770926444571253,
        "recall": 0.07622868605817452
      },
      {
        "accuracy": 0.044132397191574725,
        "f1": 0.02995507449887409,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.02995507449887409,
        "precision": 0.026794210188488113,
        "recall": 0.044132397191574725
      },
      {
        "accuracy": 0.08224674022066199,
        "f1": 0.035622458279699516,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.035622458279699516,
        "precision": 0.02681362316054249,
        "recall": 0.08224674022066199
      },
      {
        "accuracy": 0.02708124373119358,
        "f1": 0.023364648556939305,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.023364648556939305,
        "precision": 0.022730515213351656,
        "recall": 0.02708124373119358
      },
      {
        "accuracy": 0.05616850551654965,
        "f1": 0.02362200384901106,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.02362200384901106,
        "precision": 0.01861810841552295,
        "recall": 0.05616850551654965
      },
      {
        "accuracy": 0.05315947843530592,
        "f1": 0.04237464174853224,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.04237464174853224,
        "precision": 0.04063081732937809,
        "recall": 0.05315947843530592
      },
      {
        "accuracy": 0.07823470411233702,
        "f1": 0.036398819358654626,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.036398819358654626,
        "precision": 0.029585588809105157,
        "recall": 0.07823470411233702
      },
      {
        "accuracy": 0.09027081243731194,
        "f1": 0.07654017774690351,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.07654017774690351,
        "precision": 0.07324439640608095,
        "recall": 0.09027081243731194
      },
      {
        "accuracy": 0.11133400200601805,
        "f1": 0.04603094396555983,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.04603094396555983,
        "precision": 0.03389622566629394,
        "recall": 0.11133400200601805
      },
      {
        "accuracy": 0.04312938816449348,
        "f1": 0.03352528402785341,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.03352528402785341,
        "precision": 0.03137309287512233,
        "recall": 0.04312938816449348
      },
      {
        "accuracy": 0.0712136409227683,
        "f1": 0.029716507337692516,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.029716507337692516,
        "precision": 0.02327177082988247,
        "recall": 0.0712136409227683
      },
      {
        "accuracy": 0.026078234704112337,
        "f1": 0.02116741127078776,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.02116741127078776,
        "precision": 0.020301747734044622,
        "recall": 0.026078234704112337
      },
      {
        "accuracy": 0.05015045135406219,
        "f1": 0.01949957222078139,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.01949957222078139,
        "precision": 0.015528953478405274,
        "recall": 0.05015045135406219
      },
      {
        "accuracy": 0.04012036108324975,
        "f1": 0.03451082602841678,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.03451082602841678,
        "precision": 0.03303741671430289,
        "recall": 0.04012036108324975
      },
      {
        "accuracy": 0.08324974924774323,
        "f1": 0.03961704584055156,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.03961704584055156,
        "precision": 0.0321401449053212,
        "recall": 0.08324974924774323
      },
      {
        "accuracy": 0.04714142427281846,
        "f1": 0.042469189442367036,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.042469189442367036,
        "precision": 0.041308150060637025,
        "recall": 0.04714142427281846
      },
      {
        "accuracy": 0.08625877632898696,
        "f1": 0.0375089486065378,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.0375089486065378,
        "precision": 0.029225404128053252,
        "recall": 0.08625877632898696
      },
      {
        "accuracy": 0.0641925777331996,
        "f1": 0.04867250653651519,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.04867250653651519,
        "precision": 0.04505745479923831,
        "recall": 0.0641925777331996
      },
      {
        "accuracy": 0.08926780341023069,
        "f1": 0.04013632770881229,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.04013632770881229,
        "precision": 0.031646376726187524,
        "recall": 0.08926780341023069
      },
      {
        "accuracy": 0.048144433299899696,
        "f1": 0.036709239983172826,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.036709239983172826,
        "precision": 0.034350192402188506,
        "recall": 0.048144433299899696
      },
      {
        "accuracy": 0.07823470411233702,
        "f1": 0.03523436414125726,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.03523436414125726,
        "precision": 0.027761866249877473,
        "recall": 0.07823470411233702
      },
      {
        "accuracy": 0.1514543630892678,
        "f1": 0.1364735269927032,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.1364735269927032,
        "precision": 0.13268963584182836,
        "recall": 0.1514543630892678
      },
      {
        "accuracy": 0.17552657973921765,
        "f1": 0.09682686442788731,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.09682686442788731,
        "precision": 0.0801116183580873,
        "recall": 0.17552657973921765
      },
      {
        "accuracy": 0.05416248746238716,
        "f1": 0.045571453822767116,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.045571453822767116,
        "precision": 0.043509968515198005,
        "recall": 0.05416248746238716
      },
      {
        "accuracy": 0.07823470411233702,
        "f1": 0.03434942678316764,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.03434942678316764,
        "precision": 0.02825064630225956,
        "recall": 0.07823470411233702
      },
      {
        "accuracy": 0.017051153460381142,
        "f1": 0.012302921496083433,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.012302921496083433,
        "precision": 0.011376418215686019,
        "recall": 0.017051153460381142
      },
      {
        "accuracy": 0.05115346038114343,
        "f1": 0.01900535900429544,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.01900535900429544,
        "precision": 0.01406293604151778,
        "recall": 0.05115346038114343
      },
      {
        "accuracy": 0.026078234704112337,
        "f1": 0.020938490845400018,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.020938490845400018,
        "precision": 0.02037125180545952,
        "recall": 0.026078234704112337
      },
      {
        "accuracy": 0.062186559679037114,
        "f1": 0.026741421748239697,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.026741421748239697,
        "precision": 0.0208507265777991,
        "recall": 0.062186559679037114
      },
      {
        "accuracy": 0.014042126379137413,
        "f1": 0.012303577398863256,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.012303577398863256,
        "precision": 0.011896801515658084,
        "recall": 0.014042126379137413
      },
      {
        "accuracy": 0.033099297893681046,
        "f1": 0.01036093082396157,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.01036093082396157,
        "precision": 0.008393121019876286,
        "recall": 0.033099297893681046
      },
      {
        "accuracy": 0.026078234704112337,
        "f1": 0.02022357614531487,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.02022357614531487,
        "precision": 0.019032189060895873,
        "recall": 0.026078234704112337
      },
      {
        "accuracy": 0.06318956870611836,
        "f1": 0.02872122640458237,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.02872122640458237,
        "precision": 0.023113639108155566,
        "recall": 0.06318956870611836
      },
      {
        "accuracy": 0.025075225677031094,
        "f1": 0.019380173213314426,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.019380173213314426,
        "precision": 0.01809024827174684,
        "recall": 0.025075225677031094
      },
      {
        "accuracy": 0.058174523570712136,
        "f1": 0.029266055651608835,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.029266055651608835,
        "precision": 0.0240203645445917,
        "recall": 0.058174523570712136
      },
      {
        "accuracy": 0.04012036108324975,
        "f1": 0.03125954933985795,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.03125954933985795,
        "precision": 0.02887875142361714,
        "recall": 0.04012036108324975
      },
      {
        "accuracy": 0.07622868605817452,
        "f1": 0.03406318112316914,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.03406318112316914,
        "precision": 0.027338650229729974,
        "recall": 0.07622868605817452
      },
      {
        "accuracy": 0.020060180541624874,
        "f1": 0.014659449151159574,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.014659449151159574,
        "precision": 0.013605856457553203,
        "recall": 0.020060180541624874
      },
      {
        "accuracy": 0.048144433299899696,
        "f1": 0.024255681216221136,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.024255681216221136,
        "precision": 0.0196838369593477,
        "recall": 0.048144433299899696
      },
      {
        "accuracy": 0.03009027081243731,
        "f1": 0.02436678137090421,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.02436678137090421,
        "precision": 0.023131233350352245,
        "recall": 0.03009027081243731
      },
      {
        "accuracy": 0.06820461384152457,
        "f1": 0.02876906958906602,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.02876906958906602,
        "precision": 0.0226653543435498,
        "recall": 0.06820461384152457
      },
      {
        "accuracy": 0.034102306920762285,
        "f1": 0.026353434877427907,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.026353434877427907,
        "precision": 0.024345028961409748,
        "recall": 0.034102306920762285
      },
      {
        "accuracy": 0.0712136409227683,
        "f1": 0.033990614628547966,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.033990614628547966,
        "precision": 0.027642256264056198,
        "recall": 0.0712136409227683
      },
      {
        "accuracy": 0.04613841524573721,
        "f1": 0.040099985055011424,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.040099985055011424,
        "precision": 0.038600287598666815,
        "recall": 0.04613841524573721
      },
      {
        "accuracy": 0.05717151454363089,
        "f1": 0.020987022890291814,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.020987022890291814,
        "precision": 0.016536282608796804,
        "recall": 0.05717151454363089
      },
      {
        "accuracy": 0.01805416248746239,
        "f1": 0.012271304333928377,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.012271304333928377,
        "precision": 0.011561496376864475,
        "recall": 0.01805416248746239
      },
      {
        "accuracy": 0.06018054162487462,
        "f1": 0.026401367977376838,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.026401367977376838,
        "precision": 0.020752457493997904,
        "recall": 0.06018054162487462
      },
      {
        "accuracy": 0.020060180541624874,
        "f1": 0.01479178716498986,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01479178716498986,
        "precision": 0.01396839105727286,
        "recall": 0.020060180541624874
      },
      {
        "accuracy": 0.06619859578736209,
        "f1": 0.03308944499522696,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.03308944499522696,
        "precision": 0.027581030554588565,
        "recall": 0.06619859578736209
      },
      {
        "accuracy": 0.012036108324974924,
        "f1": 0.01053159478435306,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.01053159478435306,
        "precision": 0.00986292209963223,
        "recall": 0.012036108324974924
      },
      {
        "accuracy": 0.01905717151454363,
        "f1": 0.0023341295461938136,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.0023341295461938136,
        "precision": 0.0013078570696802385,
        "recall": 0.01905717151454363
      }
    ]
  },
  "task_name": "IndicGenBenchFloresBitextMining"
}