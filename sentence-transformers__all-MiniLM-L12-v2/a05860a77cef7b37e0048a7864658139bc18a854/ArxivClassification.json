{
  "dataset_revision": "f9bd92144ed76200d6eb3ce73a8bd4eba9ffdc85",
  "evaluation_time": 61.400251626968384,
  "kg_co2_emissions": 0.008991937458235932,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.67744,
        "f1": 0.6631333024540965,
        "f1_weighted": 0.6674394604245182,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.67744,
        "scores_per_experiment": [
          {
            "accuracy": 0.6776,
            "f1": 0.6620093890103068,
            "f1_weighted": 0.6691652137872549
          },
          {
            "accuracy": 0.7012,
            "f1": 0.6881268816379627,
            "f1_weighted": 0.6937067110629449
          },
          {
            "accuracy": 0.6888,
            "f1": 0.6710558518856479,
            "f1_weighted": 0.6769522994151437
          },
          {
            "accuracy": 0.67,
            "f1": 0.6496153382764523,
            "f1_weighted": 0.6555680723920719
          },
          {
            "accuracy": 0.678,
            "f1": 0.67168777760763,
            "f1_weighted": 0.675129884308763
          },
          {
            "accuracy": 0.6656,
            "f1": 0.6514886194418185,
            "f1_weighted": 0.6560358231829926
          },
          {
            "accuracy": 0.6904,
            "f1": 0.6778298678191188,
            "f1_weighted": 0.6807924716581916
          },
          {
            "accuracy": 0.674,
            "f1": 0.661889920789179,
            "f1_weighted": 0.6645156170873894
          },
          {
            "accuracy": 0.6508,
            "f1": 0.6366675148090806,
            "f1_weighted": 0.6369479905013572
          },
          {
            "accuracy": 0.678,
            "f1": 0.660961863263768,
            "f1_weighted": 0.6655805208490727
          }
        ]
      }
    ]
  },
  "task_name": "ArxivClassification"
}