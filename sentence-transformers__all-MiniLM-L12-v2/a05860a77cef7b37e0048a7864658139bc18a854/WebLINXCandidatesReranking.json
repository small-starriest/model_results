{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 1215.1819961071014,
  "kg_co2_emissions": 0.19942200614837818,
  "mteb_version": "1.12.75",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0465829543427858,
        "map": 0.0622614646489881,
        "mrr": 0.0465829543427858,
        "nAUC_map_diff1": -0.019809214622371382,
        "nAUC_map_max": -0.12030675752827408,
        "nAUC_map_std": 0.08908685565621549,
        "nAUC_mrr_diff1": -0.03329463780057532,
        "nAUC_mrr_max": -0.12614642300939777,
        "nAUC_mrr_std": 0.08735876353795523
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0589996060805662,
        "map": 0.07605535430416004,
        "mrr": 0.0589996060805662,
        "nAUC_map_diff1": 0.11635622832960466,
        "nAUC_map_max": -0.06253967119373241,
        "nAUC_map_std": -0.00034165916431559484,
        "nAUC_mrr_diff1": 0.12319311313074957,
        "nAUC_mrr_max": -0.06868616986582964,
        "nAUC_mrr_std": -0.004542044009985767
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.04637807139545665,
        "map": 0.06219655110657529,
        "mrr": 0.04637807139545665,
        "nAUC_map_diff1": 0.08240502928203712,
        "nAUC_map_max": 0.016249223901748013,
        "nAUC_map_std": 0.08343616551663202,
        "nAUC_mrr_diff1": 0.09821289959776117,
        "nAUC_mrr_max": 0.023341857771297554,
        "nAUC_mrr_std": 0.07711802238867696
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.04777941061916146,
        "map": 0.06423820601954183,
        "mrr": 0.04777941061916146,
        "nAUC_map_diff1": 0.015658388029973783,
        "nAUC_map_max": 0.01668410001741131,
        "nAUC_map_std": 0.07475718415388524,
        "nAUC_mrr_diff1": 0.011494554983654096,
        "nAUC_mrr_max": 0.021268528923402454,
        "nAUC_mrr_std": 0.0743103703634384
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.04094028938971686,
        "map": 0.05609139084323196,
        "mrr": 0.04094028938971686,
        "nAUC_map_diff1": -0.07805428983354715,
        "nAUC_map_max": -0.08543613689259826,
        "nAUC_map_std": -0.15566319146499386,
        "nAUC_mrr_diff1": -0.060635575560010604,
        "nAUC_mrr_max": -0.10576918330921342,
        "nAUC_mrr_std": -0.14944447412535786
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.05212839939972915,
        "map": 0.07078959796694081,
        "mrr": 0.05212839939972915,
        "nAUC_map_diff1": 0.0941127301316234,
        "nAUC_map_max": -0.021360983493079737,
        "nAUC_map_std": -0.023566134923741214,
        "nAUC_mrr_diff1": 0.11397232278443448,
        "nAUC_mrr_max": -0.008211919607166862,
        "nAUC_mrr_std": -0.04937834412427377
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}