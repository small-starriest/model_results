{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "evaluation_time": 10.986562490463257,
  "kg_co2_emissions": 0.0016791610443188554,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.67353515625,
        "f1": 0.608721162019842,
        "f1_weighted": 0.6945175102850335,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ],
        "main_score": 0.67353515625,
        "scores_per_experiment": [
          {
            "accuracy": 0.60888671875,
            "f1": 0.5490112909687718,
            "f1_weighted": 0.6186565959012037
          },
          {
            "accuracy": 0.69482421875,
            "f1": 0.6357221023530298,
            "f1_weighted": 0.7203750001038394
          },
          {
            "accuracy": 0.6669921875,
            "f1": 0.6174700639022753,
            "f1_weighted": 0.6971758664534777
          },
          {
            "accuracy": 0.71044921875,
            "f1": 0.6309925837119286,
            "f1_weighted": 0.7252006285031236
          },
          {
            "accuracy": 0.62158203125,
            "f1": 0.560951396814946,
            "f1_weighted": 0.6537767319102358
          },
          {
            "accuracy": 0.70654296875,
            "f1": 0.6372188613238202,
            "f1_weighted": 0.7248966577195813
          },
          {
            "accuracy": 0.7265625,
            "f1": 0.6458451111679698,
            "f1_weighted": 0.7391502114097945
          },
          {
            "accuracy": 0.67529296875,
            "f1": 0.6205464631075995,
            "f1_weighted": 0.7062196846165247
          },
          {
            "accuracy": 0.64013671875,
            "f1": 0.5863128177536181,
            "f1_weighted": 0.6653963041820617
          },
          {
            "accuracy": 0.68408203125,
            "f1": 0.603140929094462,
            "f1_weighted": 0.6943274220504918
          }
        ]
      }
    ]
  },
  "task_name": "SweRecClassification"
}