{
  "dataset_revision": "1300d045cf983bac23faadf3aa12a619624769da",
  "evaluation_time": 37.294230461120605,
  "kg_co2_emissions": 0.010248736870195575,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.37490234375,
        "f1": 0.3442855539166922,
        "f1_weighted": 0.38608588241418845,
        "hf_subset": "default",
        "languages": [
          "yue-Hant"
        ],
        "main_score": 0.37490234375,
        "scores_per_experiment": [
          {
            "accuracy": 0.33740234375,
            "f1": 0.32558198942624783,
            "f1_weighted": 0.33627289776150554
          },
          {
            "accuracy": 0.400390625,
            "f1": 0.3597629636508723,
            "f1_weighted": 0.4142343043610378
          },
          {
            "accuracy": 0.3662109375,
            "f1": 0.33058383537394653,
            "f1_weighted": 0.37692051877185473
          },
          {
            "accuracy": 0.39892578125,
            "f1": 0.36464858428640634,
            "f1_weighted": 0.4150051171121444
          },
          {
            "accuracy": 0.37890625,
            "f1": 0.34212141781122835,
            "f1_weighted": 0.3940591934216341
          },
          {
            "accuracy": 0.39013671875,
            "f1": 0.34186233601336424,
            "f1_weighted": 0.408203604291864
          },
          {
            "accuracy": 0.3623046875,
            "f1": 0.33891034225757954,
            "f1_weighted": 0.37268374430238915
          },
          {
            "accuracy": 0.376953125,
            "f1": 0.34480344765241944,
            "f1_weighted": 0.38789817830773055
          },
          {
            "accuracy": 0.40185546875,
            "f1": 0.3633495612863515,
            "f1_weighted": 0.4191906775610113
          },
          {
            "accuracy": 0.3359375,
            "f1": 0.3312310614085057,
            "f1_weighted": 0.33639058825071283
          }
        ]
      }
    ]
  },
  "task_name": "YueOpenriceReviewClassification"
}