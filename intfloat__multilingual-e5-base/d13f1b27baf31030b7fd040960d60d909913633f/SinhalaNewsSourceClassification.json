{
  "dataset_revision": "ac4d14eeb68efbef95e247542d4432ce674faeb1",
  "evaluation_time": 10.637945890426636,
  "kg_co2_emissions": 0.0015932283668215667,
  "mteb_version": "1.12.75",
  "scores": {
    "train": [
      {
        "accuracy": 0.3423828125,
        "f1": 0.3155987104918193,
        "f1_weighted": 0.3282776278532966,
        "hf_subset": "default",
        "languages": [
          "sin-Sinh"
        ],
        "main_score": 0.3423828125,
        "scores_per_experiment": [
          {
            "accuracy": 0.3515625,
            "f1": 0.3247821207963233,
            "f1_weighted": 0.33327157488186304
          },
          {
            "accuracy": 0.3515625,
            "f1": 0.33026888233701013,
            "f1_weighted": 0.3392539811429796
          },
          {
            "accuracy": 0.36865234375,
            "f1": 0.3369938015281025,
            "f1_weighted": 0.35201073422709706
          },
          {
            "accuracy": 0.32666015625,
            "f1": 0.30262149327384147,
            "f1_weighted": 0.31328682900052074
          },
          {
            "accuracy": 0.349609375,
            "f1": 0.3289230456297673,
            "f1_weighted": 0.3435703774737331
          },
          {
            "accuracy": 0.34375,
            "f1": 0.31691876501849053,
            "f1_weighted": 0.3309764747173635
          },
          {
            "accuracy": 0.33349609375,
            "f1": 0.31012737480523633,
            "f1_weighted": 0.32358849102644077
          },
          {
            "accuracy": 0.32861328125,
            "f1": 0.30269779035180583,
            "f1_weighted": 0.315579358436181
          },
          {
            "accuracy": 0.3447265625,
            "f1": 0.3086755926156665,
            "f1_weighted": 0.32160196660577867
          },
          {
            "accuracy": 0.3251953125,
            "f1": 0.29397823856194916,
            "f1_weighted": 0.30963649102100865
          }
        ]
      }
    ]
  },
  "task_name": "SinhalaNewsSourceClassification"
}