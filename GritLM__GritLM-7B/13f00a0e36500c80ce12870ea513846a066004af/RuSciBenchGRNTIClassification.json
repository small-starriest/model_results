{
  "dataset_revision": "673a610d6d3dd91a547a0d57ae1b56f37ebbf6a1",
  "evaluation_time": 42.400203704833984,
  "kg_co2_emissions": 0.01254825877298829,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.64560546875,
        "f1": 0.6347522808293438,
        "f1_weighted": 0.6348384394589267,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.64560546875,
        "scores_per_experiment": [
          {
            "accuracy": 0.66650390625,
            "f1": 0.6571923243946676,
            "f1_weighted": 0.6572589706386203
          },
          {
            "accuracy": 0.6455078125,
            "f1": 0.6344074317690216,
            "f1_weighted": 0.6344956422225593
          },
          {
            "accuracy": 0.6357421875,
            "f1": 0.625020851264137,
            "f1_weighted": 0.6251456093387753
          },
          {
            "accuracy": 0.669921875,
            "f1": 0.6613002308428682,
            "f1_weighted": 0.6613750556186949
          },
          {
            "accuracy": 0.64794921875,
            "f1": 0.640486942956998,
            "f1_weighted": 0.640583384426727
          },
          {
            "accuracy": 0.6123046875,
            "f1": 0.597516108205184,
            "f1_weighted": 0.5975894559897497
          },
          {
            "accuracy": 0.6337890625,
            "f1": 0.6213127813159909,
            "f1_weighted": 0.6214492551643144
          },
          {
            "accuracy": 0.6455078125,
            "f1": 0.6320480041355927,
            "f1_weighted": 0.6320822547455862
          },
          {
            "accuracy": 0.654296875,
            "f1": 0.6446294484578006,
            "f1_weighted": 0.644717374946289
          },
          {
            "accuracy": 0.64453125,
            "f1": 0.6336086849511774,
            "f1_weighted": 0.6336873914979503
          }
        ]
      }
    ]
  },
  "task_name": "RuSciBenchGRNTIClassification"
}