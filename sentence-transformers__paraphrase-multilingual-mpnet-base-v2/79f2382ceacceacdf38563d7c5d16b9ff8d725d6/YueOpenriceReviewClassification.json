{
  "dataset_revision": "1300d045cf983bac23faadf3aa12a619624769da",
  "evaluation_time": 13.299134731292725,
  "kg_co2_emissions": 0.002097765704093426,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.31181640625,
        "f1": 0.2737262275413824,
        "f1_weighted": 0.33190639980324743,
        "hf_subset": "default",
        "languages": [
          "yue-Hant"
        ],
        "main_score": 0.31181640625,
        "scores_per_experiment": [
          {
            "accuracy": 0.306640625,
            "f1": 0.27636955386364753,
            "f1_weighted": 0.32274586550990336
          },
          {
            "accuracy": 0.3310546875,
            "f1": 0.28864752099918917,
            "f1_weighted": 0.3509232531370743
          },
          {
            "accuracy": 0.349609375,
            "f1": 0.2921588505084495,
            "f1_weighted": 0.36824503821292753
          },
          {
            "accuracy": 0.31298828125,
            "f1": 0.27298885143017443,
            "f1_weighted": 0.3334534974542606
          },
          {
            "accuracy": 0.306640625,
            "f1": 0.27369462730616445,
            "f1_weighted": 0.328891062391112
          },
          {
            "accuracy": 0.32861328125,
            "f1": 0.2825391273415767,
            "f1_weighted": 0.34969299907107604
          },
          {
            "accuracy": 0.2978515625,
            "f1": 0.26735342586395233,
            "f1_weighted": 0.3175077018109219
          },
          {
            "accuracy": 0.28369140625,
            "f1": 0.2569894176310129,
            "f1_weighted": 0.30313478437393293
          },
          {
            "accuracy": 0.31396484375,
            "f1": 0.2631036465895291,
            "f1_weighted": 0.34121375172681695
          },
          {
            "accuracy": 0.287109375,
            "f1": 0.26341725388012815,
            "f1_weighted": 0.3032560443444486
          }
        ]
      }
    ]
  },
  "task_name": "YueOpenriceReviewClassification"
}