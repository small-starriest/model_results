{
  "dataset_revision": "c18a4f81a47ae6fa079fe9d32db288ddde38451d",
  "evaluation_time": 50.242581605911255,
  "kg_co2_emissions": 0.026286867274207922,
  "mteb_version": "1.12.75",
  "scores": {
    "validation": [
      {
        "accuracy": 0.9774774774774775,
        "f1": 0.9725975975975976,
        "hf_subset": "ar-en",
        "languages": [
          "ara-Arab",
          "eng-Latn"
        ],
        "main_score": 0.9725975975975976,
        "precision": 0.970608108108108,
        "recall": 0.9774774774774775
      },
      {
        "accuracy": 0.9898648648648649,
        "f1": 0.988943488943489,
        "hf_subset": "de-en",
        "languages": [
          "deu-Latn",
          "eng-Latn"
        ],
        "main_score": 0.988943488943489,
        "precision": 0.9888513513513514,
        "recall": 0.9898648648648649
      },
      {
        "accuracy": 0.9763513513513513,
        "f1": 0.9714714714714714,
        "hf_subset": "en-ar",
        "languages": [
          "eng-Latn",
          "ara-Arab"
        ],
        "main_score": 0.9714714714714714,
        "precision": 0.969481981981982,
        "recall": 0.9763513513513513
      },
      {
        "accuracy": 0.9887387387387387,
        "f1": 0.9864864864864865,
        "hf_subset": "en-de",
        "languages": [
          "eng-Latn",
          "deu-Latn"
        ],
        "main_score": 0.9864864864864865,
        "precision": 0.9858108108108108,
        "recall": 0.9887387387387387
      },
      {
        "accuracy": 0.9797752808988764,
        "f1": 0.9749063670411985,
        "hf_subset": "en-fr",
        "languages": [
          "eng-Latn",
          "fra-Latn"
        ],
        "main_score": 0.9749063670411985,
        "precision": 0.9729213483146067,
        "recall": 0.9797752808988764
      },
      {
        "accuracy": 0.9720129171151776,
        "f1": 0.9646573376390384,
        "hf_subset": "en-it",
        "languages": [
          "eng-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9646573376390384,
        "precision": 0.9614998205956226,
        "recall": 0.9720129171151776
      },
      {
        "accuracy": 0.931113662456946,
        "f1": 0.9168562780503078,
        "hf_subset": "en-ja",
        "languages": [
          "eng-Latn",
          "jpn-Jpan"
        ],
        "main_score": 0.9168562780503078,
        "precision": 0.9106582472254114,
        "recall": 0.931113662456946
      },
      {
        "accuracy": 0.9089874857792947,
        "f1": 0.8915434205536594,
        "hf_subset": "en-ko",
        "languages": [
          "eng-Latn",
          "kor-Hang"
        ],
        "main_score": 0.8915434205536594,
        "precision": 0.8834281380356466,
        "recall": 0.9089874857792947
      },
      {
        "accuracy": 0.9581256231306082,
        "f1": 0.9459953472914588,
        "hf_subset": "en-nl",
        "languages": [
          "eng-Latn",
          "nld-Latn"
        ],
        "main_score": 0.9459953472914588,
        "precision": 0.9405782652043868,
        "recall": 0.9581256231306082
      },
      {
        "accuracy": 0.975929978118162,
        "f1": 0.9697301239970824,
        "hf_subset": "en-ro",
        "languages": [
          "eng-Latn",
          "ron-Latn"
        ],
        "main_score": 0.9697301239970824,
        "precision": 0.9670678336980306,
        "recall": 0.975929978118162
      },
      {
        "accuracy": 0.9283276450511946,
        "f1": 0.916317243620998,
        "hf_subset": "en-zh",
        "languages": [
          "eng-Latn",
          "cmn-Hans"
        ],
        "main_score": 0.916317243620998,
        "precision": 0.9113503106677169,
        "recall": 0.9283276450511946
      },
      {
        "accuracy": 0.9797752808988764,
        "f1": 0.9759363295880149,
        "hf_subset": "fr-en",
        "languages": [
          "fra-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9759363295880149,
        "precision": 0.9745425361155698,
        "recall": 0.9797752808988764
      },
      {
        "accuracy": 0.9741657696447793,
        "f1": 0.9684607104413349,
        "hf_subset": "it-en",
        "languages": [
          "ita-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9684607104413349,
        "precision": 0.9660327711996173,
        "recall": 0.9741657696447793
      },
      {
        "accuracy": 0.9440559440559441,
        "f1": 0.930854859426288,
        "hf_subset": "it-nl",
        "languages": [
          "ita-Latn",
          "nld-Latn"
        ],
        "main_score": 0.930854859426288,
        "precision": 0.9245754245754245,
        "recall": 0.9440559440559441
      },
      {
        "accuracy": 0.9682713347921226,
        "f1": 0.962071480671043,
        "hf_subset": "it-ro",
        "languages": [
          "ita-Latn",
          "ron-Latn"
        ],
        "main_score": 0.962071480671043,
        "precision": 0.9594357138120814,
        "recall": 0.9682713347921226
      },
      {
        "accuracy": 0.928817451205511,
        "f1": 0.9147193028938149,
        "hf_subset": "ja-en",
        "languages": [
          "jpn-Jpan",
          "eng-Latn"
        ],
        "main_score": 0.9147193028938149,
        "precision": 0.9089934940681209,
        "recall": 0.928817451205511
      },
      {
        "accuracy": 0.8987485779294653,
        "f1": 0.8771963089369232,
        "hf_subset": "ko-en",
        "languages": [
          "kor-Hang",
          "eng-Latn"
        ],
        "main_score": 0.8771963089369232,
        "precision": 0.8670364050056882,
        "recall": 0.8987485779294653
      },
      {
        "accuracy": 0.9571286141575274,
        "f1": 0.9468815774897529,
        "hf_subset": "nl-en",
        "languages": [
          "nld-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9468815774897529,
        "precision": 0.9422981056829511,
        "recall": 0.9571286141575274
      },
      {
        "accuracy": 0.948051948051948,
        "f1": 0.9347509633223919,
        "hf_subset": "nl-it",
        "languages": [
          "nld-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9347509633223919,
        "precision": 0.9288211788211789,
        "recall": 0.948051948051948
      },
      {
        "accuracy": 0.9572836801752465,
        "f1": 0.9477390079799719,
        "hf_subset": "nl-ro",
        "languages": [
          "nld-Latn",
          "ron-Latn"
        ],
        "main_score": 0.9477390079799719,
        "precision": 0.943592552026287,
        "recall": 0.9572836801752465
      },
      {
        "accuracy": 0.9748358862144421,
        "f1": 0.9700948212983224,
        "hf_subset": "ro-en",
        "languages": [
          "ron-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9700948212983224,
        "precision": 0.9679431072210066,
        "recall": 0.9748358862144421
      },
      {
        "accuracy": 0.9693654266958425,
        "f1": 0.9617067833698031,
        "hf_subset": "ro-it",
        "languages": [
          "ron-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9617067833698031,
        "precision": 0.9584974471188913,
        "recall": 0.9693654266958425
      },
      {
        "accuracy": 0.9572836801752465,
        "f1": 0.9466593647316538,
        "hf_subset": "ro-nl",
        "languages": [
          "ron-Latn",
          "nld-Latn"
        ],
        "main_score": 0.9466593647316538,
        "precision": 0.9419861263234757,
        "recall": 0.9572836801752465
      },
      {
        "accuracy": 0.9351535836177475,
        "f1": 0.9188688444661142,
        "hf_subset": "zh-en",
        "languages": [
          "cmn-Hans",
          "eng-Latn"
        ],
        "main_score": 0.9188688444661142,
        "precision": 0.9113576033371256,
        "recall": 0.9351535836177475
      }
    ]
  },
  "task_name": "IWSLT2017BitextMining"
}