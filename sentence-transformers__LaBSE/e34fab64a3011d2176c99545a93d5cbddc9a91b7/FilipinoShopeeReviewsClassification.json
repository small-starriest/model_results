{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 18.12957763671875,
  "kg_co2_emissions": 0.0027580321494711565,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.32734375,
        "f1": 0.3207987940848296,
        "f1_weighted": 0.32077816326649955,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.32734375,
        "scores_per_experiment": [
          {
            "accuracy": 0.33203125,
            "f1": 0.32384016776705293,
            "f1_weighted": 0.3238309264926091
          },
          {
            "accuracy": 0.3544921875,
            "f1": 0.3410652837841633,
            "f1_weighted": 0.34100383721005273
          },
          {
            "accuracy": 0.32763671875,
            "f1": 0.3157670014897479,
            "f1_weighted": 0.3157213436687709
          },
          {
            "accuracy": 0.341796875,
            "f1": 0.34173549128639785,
            "f1_weighted": 0.34174585143165914
          },
          {
            "accuracy": 0.3076171875,
            "f1": 0.3027507430414416,
            "f1_weighted": 0.3027025837906854
          },
          {
            "accuracy": 0.32861328125,
            "f1": 0.32164210231489954,
            "f1_weighted": 0.32161693311817274
          },
          {
            "accuracy": 0.28857421875,
            "f1": 0.29382201809812203,
            "f1_weighted": 0.2937753278024191
          },
          {
            "accuracy": 0.3720703125,
            "f1": 0.3683460654748975,
            "f1_weighted": 0.3683301813609692
          },
          {
            "accuracy": 0.3369140625,
            "f1": 0.3288243691706237,
            "f1_weighted": 0.3287919022622263
          },
          {
            "accuracy": 0.28369140625,
            "f1": 0.27019469842094906,
            "f1_weighted": 0.27026274552743046
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.326513671875,
        "f1": 0.32083268614018856,
        "f1_weighted": 0.32081207801641776,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.326513671875,
        "scores_per_experiment": [
          {
            "accuracy": 0.3251953125,
            "f1": 0.3195266555794375,
            "f1_weighted": 0.31950079534575676
          },
          {
            "accuracy": 0.345703125,
            "f1": 0.3379934143172786,
            "f1_weighted": 0.3379523256747261
          },
          {
            "accuracy": 0.32958984375,
            "f1": 0.31860125819136415,
            "f1_weighted": 0.31854468488762117
          },
          {
            "accuracy": 0.369140625,
            "f1": 0.3670606699896947,
            "f1_weighted": 0.3670677630820961
          },
          {
            "accuracy": 0.29052734375,
            "f1": 0.28744808568675717,
            "f1_weighted": 0.28742812275995205
          },
          {
            "accuracy": 0.31640625,
            "f1": 0.31033941747620875,
            "f1_weighted": 0.3103132385812166
          },
          {
            "accuracy": 0.29052734375,
            "f1": 0.29436892003777526,
            "f1_weighted": 0.2943344258825835
          },
          {
            "accuracy": 0.36328125,
            "f1": 0.3583387619414583,
            "f1_weighted": 0.35830711941165083
          },
          {
            "accuracy": 0.34423828125,
            "f1": 0.335451724509633,
            "f1_weighted": 0.3354095809643074
          },
          {
            "accuracy": 0.29052734375,
            "f1": 0.2791979536722776,
            "f1_weighted": 0.2792627235742674
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}