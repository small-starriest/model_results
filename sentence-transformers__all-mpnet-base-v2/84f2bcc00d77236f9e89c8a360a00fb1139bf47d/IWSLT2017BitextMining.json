{
  "dataset_revision": "c18a4f81a47ae6fa079fe9d32db288ddde38451d",
  "evaluation_time": 27.453744649887085,
  "kg_co2_emissions": 0.004566331471860944,
  "mteb_version": "1.12.75",
  "scores": {
    "validation": [
      {
        "accuracy": 0.01463963963963964,
        "f1": 0.010371595248644429,
        "hf_subset": "ar-en",
        "languages": [
          "ara-Arab",
          "eng-Latn"
        ],
        "main_score": 0.010371595248644429,
        "precision": 0.009625181500181499,
        "recall": 0.01463963963963964
      },
      {
        "accuracy": 0.34572072072072074,
        "f1": 0.28326965524546166,
        "hf_subset": "de-en",
        "languages": [
          "deu-Latn",
          "eng-Latn"
        ],
        "main_score": 0.28326965524546166,
        "precision": 0.2640460308867675,
        "recall": 0.34572072072072074
      },
      {
        "accuracy": 0.021396396396396396,
        "f1": 0.007892044871211538,
        "hf_subset": "en-ar",
        "languages": [
          "eng-Latn",
          "ara-Arab"
        ],
        "main_score": 0.007892044871211538,
        "precision": 0.005967383692903017,
        "recall": 0.021396396396396396
      },
      {
        "accuracy": 0.30180180180180183,
        "f1": 0.21638607460975878,
        "hf_subset": "en-de",
        "languages": [
          "eng-Latn",
          "deu-Latn"
        ],
        "main_score": 0.21638607460975878,
        "precision": 0.19268595720801604,
        "recall": 0.30180180180180183
      },
      {
        "accuracy": 0.401123595505618,
        "f1": 0.31685627927496646,
        "hf_subset": "en-fr",
        "languages": [
          "eng-Latn",
          "fra-Latn"
        ],
        "main_score": 0.31685627927496646,
        "precision": 0.2912617594640067,
        "recall": 0.401123595505618
      },
      {
        "accuracy": 0.2755651237890204,
        "f1": 0.20819932812003245,
        "hf_subset": "en-it",
        "languages": [
          "eng-Latn",
          "ita-Latn"
        ],
        "main_score": 0.20819932812003245,
        "precision": 0.19064102166310154,
        "recall": 0.2755651237890204
      },
      {
        "accuracy": 0.06314580941446613,
        "f1": 0.03303744910755108,
        "hf_subset": "en-ja",
        "languages": [
          "eng-Latn",
          "jpn-Jpan"
        ],
        "main_score": 0.03303744910755108,
        "precision": 0.027665066861730606,
        "recall": 0.06314580941446613
      },
      {
        "accuracy": 0.03299203640500569,
        "f1": 0.015477809927201486,
        "hf_subset": "en-ko",
        "languages": [
          "eng-Latn",
          "kor-Hang"
        ],
        "main_score": 0.015477809927201486,
        "precision": 0.012352403146074704,
        "recall": 0.03299203640500569
      },
      {
        "accuracy": 0.226321036889332,
        "f1": 0.15605214431594358,
        "hf_subset": "en-nl",
        "languages": [
          "eng-Latn",
          "nld-Latn"
        ],
        "main_score": 0.15605214431594358,
        "precision": 0.13832467409818283,
        "recall": 0.226321036889332
      },
      {
        "accuracy": 0.22647702407002188,
        "f1": 0.15876059938089734,
        "hf_subset": "en-ro",
        "languages": [
          "eng-Latn",
          "ron-Latn"
        ],
        "main_score": 0.15876059938089734,
        "precision": 0.14291380102910697,
        "recall": 0.22647702407002188
      },
      {
        "accuracy": 0.07053469852104664,
        "f1": 0.03951832323411158,
        "hf_subset": "en-zh",
        "languages": [
          "eng-Latn",
          "cmn-Hans"
        ],
        "main_score": 0.03951832323411158,
        "precision": 0.03377031639243835,
        "recall": 0.07053469852104664
      },
      {
        "accuracy": 0.5325842696629214,
        "f1": 0.4629477986189026,
        "hf_subset": "fr-en",
        "languages": [
          "fra-Latn",
          "eng-Latn"
        ],
        "main_score": 0.4629477986189026,
        "precision": 0.4395140082019386,
        "recall": 0.5325842696629214
      },
      {
        "accuracy": 0.31646932185145316,
        "f1": 0.2572101242988065,
        "hf_subset": "it-en",
        "languages": [
          "ita-Latn",
          "eng-Latn"
        ],
        "main_score": 0.2572101242988065,
        "precision": 0.2395778170575211,
        "recall": 0.31646932185145316
      },
      {
        "accuracy": 0.16383616383616384,
        "f1": 0.1227844187903767,
        "hf_subset": "it-nl",
        "languages": [
          "ita-Latn",
          "nld-Latn"
        ],
        "main_score": 0.1227844187903767,
        "precision": 0.11281753937321773,
        "recall": 0.16383616383616384
      },
      {
        "accuracy": 0.2800875273522976,
        "f1": 0.24235231668031415,
        "hf_subset": "it-ro",
        "languages": [
          "ita-Latn",
          "ron-Latn"
        ],
        "main_score": 0.24235231668031415,
        "precision": 0.23186565738311443,
        "recall": 0.2800875273522976
      },
      {
        "accuracy": 0.0677382319173364,
        "f1": 0.04310623461238709,
        "hf_subset": "ja-en",
        "languages": [
          "jpn-Jpan",
          "eng-Latn"
        ],
        "main_score": 0.04310623461238709,
        "precision": 0.037847292115015994,
        "recall": 0.0677382319173364
      },
      {
        "accuracy": 0.02502844141069397,
        "f1": 0.016854557645045367,
        "hf_subset": "ko-en",
        "languages": [
          "kor-Hang",
          "eng-Latn"
        ],
        "main_score": 0.016854557645045367,
        "precision": 0.015380765750431348,
        "recall": 0.02502844141069397
      },
      {
        "accuracy": 0.2382851445663011,
        "f1": 0.19890183516063764,
        "hf_subset": "nl-en",
        "languages": [
          "nld-Latn",
          "eng-Latn"
        ],
        "main_score": 0.19890183516063764,
        "precision": 0.1883198238970421,
        "recall": 0.2382851445663011
      },
      {
        "accuracy": 0.17782217782217782,
        "f1": 0.14753095836839678,
        "hf_subset": "nl-it",
        "languages": [
          "nld-Latn",
          "ita-Latn"
        ],
        "main_score": 0.14753095836839678,
        "precision": 0.1384876024909904,
        "recall": 0.17782217782217782
      },
      {
        "accuracy": 0.13143483023001096,
        "f1": 0.11585736886941705,
        "hf_subset": "nl-ro",
        "languages": [
          "nld-Latn",
          "ron-Latn"
        ],
        "main_score": 0.11585736886941705,
        "precision": 0.11055651176133102,
        "recall": 0.13143483023001096
      },
      {
        "accuracy": 0.2549234135667396,
        "f1": 0.2073484672432138,
        "hf_subset": "ro-en",
        "languages": [
          "ron-Latn",
          "eng-Latn"
        ],
        "main_score": 0.2073484672432138,
        "precision": 0.19407909688805894,
        "recall": 0.2549234135667396
      },
      {
        "accuracy": 0.2844638949671772,
        "f1": 0.2449016751913717,
        "hf_subset": "ro-it",
        "languages": [
          "ron-Latn",
          "ita-Latn"
        ],
        "main_score": 0.2449016751913717,
        "precision": 0.23331261160770708,
        "recall": 0.2844638949671772
      },
      {
        "accuracy": 0.16757940854326397,
        "f1": 0.12941789854655156,
        "hf_subset": "ro-nl",
        "languages": [
          "ron-Latn",
          "nld-Latn"
        ],
        "main_score": 0.12941789854655156,
        "precision": 0.1200547601851514,
        "recall": 0.16757940854326397
      },
      {
        "accuracy": 0.06598407281001138,
        "f1": 0.04555141194975573,
        "hf_subset": "zh-en",
        "languages": [
          "cmn-Hans",
          "eng-Latn"
        ],
        "main_score": 0.04555141194975573,
        "precision": 0.040902975946665976,
        "recall": 0.06598407281001138
      }
    ]
  },
  "task_name": "IWSLT2017BitextMining"
}