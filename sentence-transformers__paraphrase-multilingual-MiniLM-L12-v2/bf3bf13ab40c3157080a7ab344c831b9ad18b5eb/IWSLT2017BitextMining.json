{
  "dataset_revision": "c18a4f81a47ae6fa079fe9d32db288ddde38451d",
  "evaluation_time": 20.476414442062378,
  "kg_co2_emissions": 0.0032071755795611075,
  "mteb_version": "1.12.75",
  "scores": {
    "validation": [
      {
        "accuracy": 0.9414414414414415,
        "f1": 0.9264264264264265,
        "hf_subset": "ar-en",
        "languages": [
          "ara-Arab",
          "eng-Latn"
        ],
        "main_score": 0.9264264264264265,
        "precision": 0.9193318318318319,
        "recall": 0.9414414414414415
      },
      {
        "accuracy": 0.9774774774774775,
        "f1": 0.9728023478023478,
        "hf_subset": "de-en",
        "languages": [
          "deu-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9728023478023478,
        "precision": 0.9708333333333333,
        "recall": 0.9774774774774775
      },
      {
        "accuracy": 0.9594594594594594,
        "f1": 0.9487612612612613,
        "hf_subset": "en-ar",
        "languages": [
          "eng-Latn",
          "ara-Arab"
        ],
        "main_score": 0.9487612612612613,
        "precision": 0.9439564564564564,
        "recall": 0.9594594594594594
      },
      {
        "accuracy": 0.9808558558558559,
        "f1": 0.9773068523068522,
        "hf_subset": "en-de",
        "languages": [
          "eng-Latn",
          "deu-Latn"
        ],
        "main_score": 0.9773068523068522,
        "precision": 0.975900900900901,
        "recall": 0.9808558558558559
      },
      {
        "accuracy": 0.9606741573033708,
        "f1": 0.9513278855975484,
        "hf_subset": "en-fr",
        "languages": [
          "eng-Latn",
          "fra-Latn"
        ],
        "main_score": 0.9513278855975484,
        "precision": 0.947116104868914,
        "recall": 0.9606741573033708
      },
      {
        "accuracy": 0.9558665231431647,
        "f1": 0.9451022604951561,
        "hf_subset": "en-it",
        "languages": [
          "eng-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9451022604951561,
        "precision": 0.9401148188015788,
        "recall": 0.9558665231431647
      },
      {
        "accuracy": 0.8691159586681975,
        "f1": 0.8411578471279965,
        "hf_subset": "en-ja",
        "languages": [
          "eng-Latn",
          "jpn-Jpan"
        ],
        "main_score": 0.8411578471279965,
        "precision": 0.8286643704554152,
        "recall": 0.8691159586681975
      },
      {
        "accuracy": 0.8407281001137656,
        "f1": 0.8058020477815699,
        "hf_subset": "en-ko",
        "languages": [
          "eng-Latn",
          "kor-Hang"
        ],
        "main_score": 0.8058020477815699,
        "precision": 0.7897610921501707,
        "recall": 0.8407281001137656
      },
      {
        "accuracy": 0.9391824526420738,
        "f1": 0.9231306081754735,
        "hf_subset": "en-nl",
        "languages": [
          "eng-Latn",
          "nld-Latn"
        ],
        "main_score": 0.9231306081754735,
        "precision": 0.9159022931206382,
        "recall": 0.9391824526420738
      },
      {
        "accuracy": 0.9595185995623632,
        "f1": 0.948650619985412,
        "hf_subset": "en-ro",
        "languages": [
          "eng-Latn",
          "ron-Latn"
        ],
        "main_score": 0.948650619985412,
        "precision": 0.9437819110138586,
        "recall": 0.9595185995623632
      },
      {
        "accuracy": 0.8964732650739476,
        "f1": 0.8729616989002653,
        "hf_subset": "en-zh",
        "languages": [
          "eng-Latn",
          "cmn-Hans"
        ],
        "main_score": 0.8729616989002653,
        "precision": 0.8622298065984072,
        "recall": 0.8964732650739476
      },
      {
        "accuracy": 0.9561797752808989,
        "f1": 0.9448903156768326,
        "hf_subset": "fr-en",
        "languages": [
          "fra-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9448903156768326,
        "precision": 0.9397378277153556,
        "recall": 0.9561797752808989
      },
      {
        "accuracy": 0.9515608180839612,
        "f1": 0.9381054897739505,
        "hf_subset": "it-en",
        "languages": [
          "ita-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9381054897739505,
        "precision": 0.9316828130606386,
        "recall": 0.9515608180839612
      },
      {
        "accuracy": 0.8941058941058941,
        "f1": 0.8672660672660674,
        "hf_subset": "it-nl",
        "languages": [
          "ita-Latn",
          "nld-Latn"
        ],
        "main_score": 0.8672660672660674,
        "precision": 0.8543123543123544,
        "recall": 0.8941058941058941
      },
      {
        "accuracy": 0.9452954048140044,
        "f1": 0.9317104303428154,
        "hf_subset": "it-ro",
        "languages": [
          "ita-Latn",
          "ron-Latn"
        ],
        "main_score": 0.9317104303428154,
        "precision": 0.9253933520891946,
        "recall": 0.9452954048140044
      },
      {
        "accuracy": 0.8576349024110218,
        "f1": 0.8290261976829142,
        "hf_subset": "ja-en",
        "languages": [
          "jpn-Jpan",
          "eng-Latn"
        ],
        "main_score": 0.8290261976829142,
        "precision": 0.8165518561040949,
        "recall": 0.8576349024110218
      },
      {
        "accuracy": 0.8134243458475541,
        "f1": 0.7729996207811908,
        "hf_subset": "ko-en",
        "languages": [
          "kor-Hang",
          "eng-Latn"
        ],
        "main_score": 0.7729996207811908,
        "precision": 0.7553876157971721,
        "recall": 0.8134243458475541
      },
      {
        "accuracy": 0.9142572283150548,
        "f1": 0.8927772238839038,
        "hf_subset": "nl-en",
        "languages": [
          "nld-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8927772238839038,
        "precision": 0.8832253240279162,
        "recall": 0.9142572283150548
      },
      {
        "accuracy": 0.8871128871128872,
        "f1": 0.8608724608724607,
        "hf_subset": "nl-it",
        "languages": [
          "nld-Latn",
          "ita-Latn"
        ],
        "main_score": 0.8608724608724607,
        "precision": 0.8489343989343989,
        "recall": 0.8871128871128872
      },
      {
        "accuracy": 0.8970427163198248,
        "f1": 0.8725690641353292,
        "hf_subset": "nl-ro",
        "languages": [
          "nld-Latn",
          "ron-Latn"
        ],
        "main_score": 0.8725690641353292,
        "precision": 0.8617652427893391,
        "recall": 0.8970427163198248
      },
      {
        "accuracy": 0.9518599562363238,
        "f1": 0.9403198916328019,
        "hf_subset": "ro-en",
        "languages": [
          "ron-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9403198916328019,
        "precision": 0.9349015317286652,
        "recall": 0.9518599562363238
      },
      {
        "accuracy": 0.9452954048140044,
        "f1": 0.9328956965718453,
        "hf_subset": "ro-it",
        "languages": [
          "ron-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9328956965718453,
        "precision": 0.9270970094821297,
        "recall": 0.9452954048140044
      },
      {
        "accuracy": 0.9145673603504929,
        "f1": 0.8930266520627965,
        "hf_subset": "ro-nl",
        "languages": [
          "ron-Latn",
          "nld-Latn"
        ],
        "main_score": 0.8930266520627965,
        "precision": 0.8830230010952903,
        "recall": 0.9145673603504929
      },
      {
        "accuracy": 0.8771331058020477,
        "f1": 0.8444254835039818,
        "hf_subset": "zh-en",
        "languages": [
          "cmn-Hans",
          "eng-Latn"
        ],
        "main_score": 0.8444254835039818,
        "precision": 0.8295140581830002,
        "recall": 0.8771331058020477
      }
    ]
  },
  "task_name": "IWSLT2017BitextMining"
}