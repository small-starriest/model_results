{
  "dataset_revision": "2fe05ee6b5832cda29f2ef7aaad7b7fe6a3609eb",
  "evaluation_time": 10.450562477111816,
  "kg_co2_emissions": 0.0015629359334375763,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.687548828125,
        "f1": 0.6864430933190464,
        "f1_weighted": 0.68646665775748,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.687548828125,
        "scores_per_experiment": [
          {
            "accuracy": 0.65771484375,
            "f1": 0.6591856174050835,
            "f1_weighted": 0.6592162630476766
          },
          {
            "accuracy": 0.69287109375,
            "f1": 0.6916800859088129,
            "f1_weighted": 0.6917055741388757
          },
          {
            "accuracy": 0.67431640625,
            "f1": 0.6729034305925398,
            "f1_weighted": 0.6729380071181755
          },
          {
            "accuracy": 0.71923828125,
            "f1": 0.7194216660056963,
            "f1_weighted": 0.7193922852419491
          },
          {
            "accuracy": 0.69287109375,
            "f1": 0.693328177404775,
            "f1_weighted": 0.693338441769025
          },
          {
            "accuracy": 0.72021484375,
            "f1": 0.7181071158811667,
            "f1_weighted": 0.7181308274363898
          },
          {
            "accuracy": 0.6611328125,
            "f1": 0.6618926580502071,
            "f1_weighted": 0.6619124971062402
          },
          {
            "accuracy": 0.65966796875,
            "f1": 0.6573220318036856,
            "f1_weighted": 0.6573677467219058
          },
          {
            "accuracy": 0.67138671875,
            "f1": 0.6655327781168953,
            "f1_weighted": 0.6655866726961026
          },
          {
            "accuracy": 0.72607421875,
            "f1": 0.7250573720216013,
            "f1_weighted": 0.7250782622984596
          }
        ]
      }
    ]
  },
  "task_name": "HeadlineClassification"
}