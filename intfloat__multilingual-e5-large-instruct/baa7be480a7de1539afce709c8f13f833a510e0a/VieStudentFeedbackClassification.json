{
  "dataset_revision": "7b56c6cb1c9c8523249f407044c838660df3811a",
  "evaluation_time": 26.5365571975708,
  "kg_co2_emissions": 0.004946226127078271,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.791015625,
        "f1": 0.6558202420196617,
        "f1_weighted": 0.8248670063116232,
        "hf_subset": "default",
        "languages": [
          "vie-Latn"
        ],
        "main_score": 0.791015625,
        "scores_per_experiment": [
          {
            "accuracy": 0.84228515625,
            "f1": 0.6962967677050002,
            "f1_weighted": 0.8519335583504886
          },
          {
            "accuracy": 0.74365234375,
            "f1": 0.6178037965207491,
            "f1_weighted": 0.7928372302383987
          },
          {
            "accuracy": 0.8515625,
            "f1": 0.6892290821471471,
            "f1_weighted": 0.8581605791977829
          },
          {
            "accuracy": 0.72900390625,
            "f1": 0.6185178580853393,
            "f1_weighted": 0.7881626807601498
          },
          {
            "accuracy": 0.80810546875,
            "f1": 0.6770042070036352,
            "f1_weighted": 0.8412316193908285
          },
          {
            "accuracy": 0.80859375,
            "f1": 0.676017017267573,
            "f1_weighted": 0.8399940415416987
          },
          {
            "accuracy": 0.7841796875,
            "f1": 0.6489061784365728,
            "f1_weighted": 0.8230509067625738
          },
          {
            "accuracy": 0.779296875,
            "f1": 0.6317509210887179,
            "f1_weighted": 0.8120403572418085
          },
          {
            "accuracy": 0.8046875,
            "f1": 0.6642838935341798,
            "f1_weighted": 0.8333546276077235
          },
          {
            "accuracy": 0.7587890625,
            "f1": 0.6383926984077021,
            "f1_weighted": 0.8079044620247793
          }
        ]
      }
    ]
  },
  "task_name": "VieStudentFeedbackClassification"
}