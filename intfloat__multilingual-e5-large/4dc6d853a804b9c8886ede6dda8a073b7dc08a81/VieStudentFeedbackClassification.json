{
  "dataset_revision": "7b56c6cb1c9c8523249f407044c838660df3811a",
  "evaluation_time": 11.509822607040405,
  "kg_co2_emissions": 0.0017423657651115822,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.759033203125,
        "f1": 0.6351056658098382,
        "f1_weighted": 0.8000809597342181,
        "hf_subset": "default",
        "languages": [
          "vie-Latn"
        ],
        "main_score": 0.759033203125,
        "scores_per_experiment": [
          {
            "accuracy": 0.8203125,
            "f1": 0.6844100302193302,
            "f1_weighted": 0.837757734953023
          },
          {
            "accuracy": 0.68505859375,
            "f1": 0.5742926753184391,
            "f1_weighted": 0.7464826033861136
          },
          {
            "accuracy": 0.83203125,
            "f1": 0.6747366452738884,
            "f1_weighted": 0.8464520377801475
          },
          {
            "accuracy": 0.67529296875,
            "f1": 0.5850287306203898,
            "f1_weighted": 0.7487173883734775
          },
          {
            "accuracy": 0.7587890625,
            "f1": 0.6362731385361938,
            "f1_weighted": 0.7988468202806198
          },
          {
            "accuracy": 0.80712890625,
            "f1": 0.6754848507149577,
            "f1_weighted": 0.8356860105164206
          },
          {
            "accuracy": 0.78857421875,
            "f1": 0.6597159565321679,
            "f1_weighted": 0.8257112745647448
          },
          {
            "accuracy": 0.75,
            "f1": 0.6163876027667975,
            "f1_weighted": 0.7876026907910488
          },
          {
            "accuracy": 0.759765625,
            "f1": 0.6308070605942029,
            "f1_weighted": 0.8000669026118288
          },
          {
            "accuracy": 0.71337890625,
            "f1": 0.6139199675220149,
            "f1_weighted": 0.7734861340847574
          }
        ]
      }
    ]
  },
  "task_name": "VieStudentFeedbackClassification"
}