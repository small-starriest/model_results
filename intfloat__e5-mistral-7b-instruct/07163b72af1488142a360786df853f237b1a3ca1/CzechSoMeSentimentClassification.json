{
  "dataset_revision": "6ced1d87a030915822b087bf539e6d5c658f1988",
  "evaluation_time": 34.01619911193848,
  "kg_co2_emissions": 0.006776844894896964,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.7174,
        "f1": 0.7044922862596236,
        "f1_weighted": 0.7046063401849358,
        "hf_subset": "default",
        "languages": [
          "ces-Latn"
        ],
        "main_score": 0.7174,
        "scores_per_experiment": [
          {
            "accuracy": 0.724,
            "f1": 0.7182813780965219,
            "f1_weighted": 0.7183819072301819
          },
          {
            "accuracy": 0.727,
            "f1": 0.7124193310261818,
            "f1_weighted": 0.7125379366259037
          },
          {
            "accuracy": 0.728,
            "f1": 0.713798215657706,
            "f1_weighted": 0.7138721412272145
          },
          {
            "accuracy": 0.719,
            "f1": 0.7058177667081775,
            "f1_weighted": 0.7059369489414695
          },
          {
            "accuracy": 0.736,
            "f1": 0.7230945344743381,
            "f1_weighted": 0.7231869479612542
          },
          {
            "accuracy": 0.688,
            "f1": 0.6762023271815094,
            "f1_weighted": 0.6763465668432782
          },
          {
            "accuracy": 0.738,
            "f1": 0.7302918468332003,
            "f1_weighted": 0.7304036602495249
          },
          {
            "accuracy": 0.716,
            "f1": 0.7032141618495982,
            "f1_weighted": 0.703296661973463
          },
          {
            "accuracy": 0.695,
            "f1": 0.6779967447418683,
            "f1_weighted": 0.6781493490900226
          },
          {
            "accuracy": 0.703,
            "f1": 0.6838065560271344,
            "f1_weighted": 0.6839512817070468
          }
        ]
      }
    ]
  },
  "task_name": "CzechSoMeSentimentClassification"
}