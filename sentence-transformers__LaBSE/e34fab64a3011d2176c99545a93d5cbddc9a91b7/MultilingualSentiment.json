{
  "dataset_revision": "46958b007a63fdbf239b7672c25d0bea67b5ea1a",
  "evaluation_time": 20.895925283432007,
  "kg_co2_emissions": 0.003214525886128356,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.6552666666666667,
        "f1": 0.6524970091955449,
        "f1_weighted": 0.6524970091955449,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.6552666666666667,
        "scores_per_experiment": [
          {
            "accuracy": 0.657,
            "f1": 0.6533156420401088,
            "f1_weighted": 0.653315642040109
          },
          {
            "accuracy": 0.624,
            "f1": 0.613755286612461,
            "f1_weighted": 0.6137552866124609
          },
          {
            "accuracy": 0.6403333333333333,
            "f1": 0.6381608322268087,
            "f1_weighted": 0.6381608322268087
          },
          {
            "accuracy": 0.6806666666666666,
            "f1": 0.6786675388638187,
            "f1_weighted": 0.6786675388638187
          },
          {
            "accuracy": 0.6676666666666666,
            "f1": 0.6668166331197112,
            "f1_weighted": 0.6668166331197113
          },
          {
            "accuracy": 0.661,
            "f1": 0.660817567958878,
            "f1_weighted": 0.660817567958878
          },
          {
            "accuracy": 0.655,
            "f1": 0.6499211878270353,
            "f1_weighted": 0.6499211878270353
          },
          {
            "accuracy": 0.6676666666666666,
            "f1": 0.6683561030824713,
            "f1_weighted": 0.6683561030824713
          },
          {
            "accuracy": 0.645,
            "f1": 0.6397965244706575,
            "f1_weighted": 0.6397965244706575
          },
          {
            "accuracy": 0.6543333333333333,
            "f1": 0.6553627757534985,
            "f1_weighted": 0.6553627757534987
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.646,
        "f1": 0.6441178050680137,
        "f1_weighted": 0.6441178050680137,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.646,
        "scores_per_experiment": [
          {
            "accuracy": 0.6496666666666666,
            "f1": 0.6457697470987672,
            "f1_weighted": 0.6457697470987671
          },
          {
            "accuracy": 0.6163333333333333,
            "f1": 0.6075598246648476,
            "f1_weighted": 0.6075598246648475
          },
          {
            "accuracy": 0.637,
            "f1": 0.6366367204644744,
            "f1_weighted": 0.6366367204644744
          },
          {
            "accuracy": 0.6686666666666666,
            "f1": 0.666690933133007,
            "f1_weighted": 0.666690933133007
          },
          {
            "accuracy": 0.6533333333333333,
            "f1": 0.6542155608269473,
            "f1_weighted": 0.6542155608269473
          },
          {
            "accuracy": 0.6473333333333333,
            "f1": 0.6483475000833893,
            "f1_weighted": 0.6483475000833894
          },
          {
            "accuracy": 0.6403333333333333,
            "f1": 0.6361538054145556,
            "f1_weighted": 0.6361538054145557
          },
          {
            "accuracy": 0.6516666666666666,
            "f1": 0.6528105523979941,
            "f1_weighted": 0.6528105523979942
          },
          {
            "accuracy": 0.6443333333333333,
            "f1": 0.6405336236862799,
            "f1_weighted": 0.6405336236862798
          },
          {
            "accuracy": 0.6513333333333333,
            "f1": 0.6524597829098744,
            "f1_weighted": 0.6524597829098743
          }
        ]
      }
    ]
  },
  "task_name": "MultilingualSentiment"
}