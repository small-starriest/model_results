{
  "dataset_revision": "f8f98e5c4d3059cf1a00c8eb3d70aa271423f636",
  "evaluation_time": 9.00199842453003,
  "kg_co2_emissions": 0.0013369644391485575,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.5207179487179487,
        "ap": 0.8418694330566276,
        "ap_weighted": 0.8418694330566276,
        "f1": 0.4397249036759753,
        "f1_weighted": 0.5810937529771845,
        "hf_subset": "default",
        "languages": [
          "ita-Latn"
        ],
        "main_score": 0.5207179487179487,
        "scores_per_experiment": [
          {
            "accuracy": 0.5415384615384615,
            "ap": 0.8545605514499204,
            "ap_weighted": 0.8545605514499204,
            "f1": 0.47009645442154707,
            "f1_weighted": 0.6032020933795973
          },
          {
            "accuracy": 0.6297435897435898,
            "ap": 0.837323289369823,
            "ap_weighted": 0.837323289369823,
            "f1": 0.4727764038699762,
            "f1_weighted": 0.6695754884145255
          },
          {
            "accuracy": 0.5405128205128205,
            "ap": 0.8428383975743684,
            "ap_weighted": 0.8428383975743684,
            "f1": 0.4535245663727443,
            "f1_weighted": 0.6026792334972451
          },
          {
            "accuracy": 0.5887179487179487,
            "ap": 0.8364764670976608,
            "ap_weighted": 0.8364764670976608,
            "f1": 0.46001582765807286,
            "f1_weighted": 0.6403610267062603
          },
          {
            "accuracy": 0.5374358974358975,
            "ap": 0.8466055452870314,
            "ap_weighted": 0.8466055452870314,
            "f1": 0.45763115341492866,
            "f1_weighted": 0.5999567263079397
          },
          {
            "accuracy": 0.56,
            "ap": 0.8452339402203346,
            "ap_weighted": 0.8452339402203346,
            "f1": 0.4654803360914987,
            "f1_weighted": 0.6192476917670362
          },
          {
            "accuracy": 0.4492307692307692,
            "ap": 0.8396613770052703,
            "ap_weighted": 0.8396613770052703,
            "f1": 0.4042166542823688,
            "f1_weighted": 0.5162480523069325
          },
          {
            "accuracy": 0.4194871794871795,
            "ap": 0.8412642493519473,
            "ap_weighted": 0.8412642493519473,
            "f1": 0.38804636143466725,
            "f1_weighted": 0.4829378801723398
          },
          {
            "accuracy": 0.44,
            "ap": 0.8269055432900474,
            "ap_weighted": 0.8269055432900474,
            "f1": 0.38440706032544847,
            "f1_weighted": 0.5109616363081306
          },
          {
            "accuracy": 0.5005128205128205,
            "ap": 0.8478249699198728,
            "ap_weighted": 0.8478249699198728,
            "f1": 0.44105421888850044,
            "f1_weighted": 0.5657677009118385
          }
        ]
      }
    ]
  },
  "task_name": "Itacola"
}