{
  "dataset_revision": "2fe05ee6b5832cda29f2ef7aaad7b7fe6a3609eb",
  "evaluation_time": 10.663459777832031,
  "kg_co2_emissions": 0.0017493796022680468,
  "mteb_version": "1.14.12",
  "scores": {
    "test": [
      {
        "accuracy": 0.86181640625,
        "f1": 0.861237290815042,
        "f1_weighted": 0.8612230134928762,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.86181640625,
        "scores_per_experiment": [
          {
            "accuracy": 0.8564453125,
            "f1": 0.8554869643377708,
            "f1_weighted": 0.8554831166329719
          },
          {
            "accuracy": 0.86328125,
            "f1": 0.8628250066916493,
            "f1_weighted": 0.8628126162725775
          },
          {
            "accuracy": 0.865234375,
            "f1": 0.8647214958321197,
            "f1_weighted": 0.8647147162332084
          },
          {
            "accuracy": 0.85498046875,
            "f1": 0.8544281604489431,
            "f1_weighted": 0.854410335975979
          },
          {
            "accuracy": 0.8642578125,
            "f1": 0.8644011749664333,
            "f1_weighted": 0.8643817445113599
          },
          {
            "accuracy": 0.8662109375,
            "f1": 0.8660065851476745,
            "f1_weighted": 0.8659897011976427
          },
          {
            "accuracy": 0.85791015625,
            "f1": 0.8573364564214776,
            "f1_weighted": 0.8573210009529344
          },
          {
            "accuracy": 0.86181640625,
            "f1": 0.8610097036804302,
            "f1_weighted": 0.8609925596364926
          },
          {
            "accuracy": 0.86083984375,
            "f1": 0.8597713780695538,
            "f1_weighted": 0.8597569364322741
          },
          {
            "accuracy": 0.8671875,
            "f1": 0.8663859825543677,
            "f1_weighted": 0.8663674070833219
          }
        ]
      }
    ]
  },
  "task_name": "HeadlineClassification"
}