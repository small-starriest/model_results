{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 2504.084144115448,
  "kg_co2_emissions": 0.46933743413217693,
  "mteb_version": "1.12.75",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.055135321919029784,
        "map": 0.07260554160027016,
        "mrr": 0.055135321919029784,
        "nAUC_map_diff1": 0.2233496448750541,
        "nAUC_map_max": 0.08206391940212067,
        "nAUC_map_std": 0.23408552628682933,
        "nAUC_mrr_diff1": 0.21112842172898819,
        "nAUC_mrr_max": 0.0840783483516649,
        "nAUC_mrr_std": 0.22264590844330467
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08208215480387979,
        "map": 0.09993404111957925,
        "mrr": 0.08208215480387979,
        "nAUC_map_diff1": 0.10678572737654601,
        "nAUC_map_max": 0.042321934891816765,
        "nAUC_map_std": 0.11608398979986839,
        "nAUC_mrr_diff1": 0.11165171551299237,
        "nAUC_mrr_max": 0.04958782486249712,
        "nAUC_mrr_std": 0.11048820408872537
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06346777932313397,
        "map": 0.0814239773640172,
        "mrr": 0.06346777932313397,
        "nAUC_map_diff1": 0.11009619758052398,
        "nAUC_map_max": 0.11743042329860093,
        "nAUC_map_std": 0.17090379670875352,
        "nAUC_mrr_diff1": 0.1319933578546779,
        "nAUC_mrr_max": 0.1346873434663218,
        "nAUC_mrr_std": 0.15342379159898428
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07961637215600975,
        "map": 0.09792753992367655,
        "mrr": 0.07961637215600975,
        "nAUC_map_diff1": 0.181561807071001,
        "nAUC_map_max": 0.14240729738649277,
        "nAUC_map_std": 0.13028193722221615,
        "nAUC_mrr_diff1": 0.1884166887754957,
        "nAUC_mrr_max": 0.14697933967856278,
        "nAUC_mrr_std": 0.11109174953516832
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0544228866270851,
        "map": 0.07257829061915723,
        "mrr": 0.0544228866270851,
        "nAUC_map_diff1": 0.14786739531372783,
        "nAUC_map_max": 0.13558062390055373,
        "nAUC_map_std": 0.08657547607049008,
        "nAUC_mrr_diff1": 0.15881156479665803,
        "nAUC_mrr_max": 0.15131034151183032,
        "nAUC_mrr_std": 0.09606990299376819
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0881894879396801,
        "map": 0.10633466468206378,
        "mrr": 0.0881894879396801,
        "nAUC_map_diff1": 0.08773395874606972,
        "nAUC_map_max": 0.054358756488163976,
        "nAUC_map_std": 0.15520258601919795,
        "nAUC_mrr_diff1": 0.09216877045349178,
        "nAUC_mrr_max": 0.055665986044947977,
        "nAUC_mrr_std": 0.15640446787892365
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}