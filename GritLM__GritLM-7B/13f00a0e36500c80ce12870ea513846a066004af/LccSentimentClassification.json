{
  "dataset_revision": "de7ba3406ee55ea2cc52a0a41408fa6aede6d3c6",
  "evaluation_time": 29.005674600601196,
  "kg_co2_emissions": 0.005497799007583973,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.7013333333333334,
        "f1": 0.7034534874170354,
        "f1_weighted": 0.7018623281454064,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ],
        "main_score": 0.7013333333333334,
        "scores_per_experiment": [
          {
            "accuracy": 0.7266666666666667,
            "f1": 0.7252409203628716,
            "f1_weighted": 0.7273610195561414
          },
          {
            "accuracy": 0.6933333333333334,
            "f1": 0.6970468966175277,
            "f1_weighted": 0.6917906588426124
          },
          {
            "accuracy": 0.66,
            "f1": 0.6652580637362605,
            "f1_weighted": 0.6525433922230328
          },
          {
            "accuracy": 0.68,
            "f1": 0.6832866479925302,
            "f1_weighted": 0.6794145658263305
          },
          {
            "accuracy": 0.72,
            "f1": 0.7211601163894098,
            "f1_weighted": 0.7269926289705294
          },
          {
            "accuracy": 0.7,
            "f1": 0.7004944651090274,
            "f1_weighted": 0.7007299968075076
          },
          {
            "accuracy": 0.72,
            "f1": 0.7208468761285663,
            "f1_weighted": 0.7234647887323944
          },
          {
            "accuracy": 0.7266666666666667,
            "f1": 0.7267320546881635,
            "f1_weighted": 0.7266293020829542
          },
          {
            "accuracy": 0.7,
            "f1": 0.7045995521002452,
            "f1_weighted": 0.7037100003079861
          },
          {
            "accuracy": 0.6866666666666666,
            "f1": 0.6898692810457515,
            "f1_weighted": 0.6859869281045751
          }
        ]
      }
    ]
  },
  "task_name": "LccSentimentClassification"
}