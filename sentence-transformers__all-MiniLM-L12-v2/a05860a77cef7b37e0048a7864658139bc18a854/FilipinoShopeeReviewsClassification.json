{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 17.09169340133667,
  "kg_co2_emissions": 0.0025674081914334613,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.25244140625,
        "f1": 0.24814793658607298,
        "f1_weighted": 0.24814970997949115,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.25244140625,
        "scores_per_experiment": [
          {
            "accuracy": 0.240234375,
            "f1": 0.2367399333411982,
            "f1_weighted": 0.23675390564457827
          },
          {
            "accuracy": 0.255859375,
            "f1": 0.25387849773838944,
            "f1_weighted": 0.2539078917110173
          },
          {
            "accuracy": 0.25048828125,
            "f1": 0.25047194516014815,
            "f1_weighted": 0.2504557493877561
          },
          {
            "accuracy": 0.26904296875,
            "f1": 0.26937178481960344,
            "f1_weighted": 0.2693852329760513
          },
          {
            "accuracy": 0.25439453125,
            "f1": 0.2434268771423287,
            "f1_weighted": 0.24337328408260908
          },
          {
            "accuracy": 0.23779296875,
            "f1": 0.22404612618725536,
            "f1_weighted": 0.22405233033707544
          },
          {
            "accuracy": 0.22900390625,
            "f1": 0.22788003600574142,
            "f1_weighted": 0.22786930031639743
          },
          {
            "accuracy": 0.27294921875,
            "f1": 0.2686071907705334,
            "f1_weighted": 0.26864672571196213
          },
          {
            "accuracy": 0.2744140625,
            "f1": 0.2708127249775977,
            "f1_weighted": 0.2707956571882874
          },
          {
            "accuracy": 0.240234375,
            "f1": 0.23624424971793392,
            "f1_weighted": 0.23625702243917707
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.258203125,
        "f1": 0.2544346507035172,
        "f1_weighted": 0.25443638657182505,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.258203125,
        "scores_per_experiment": [
          {
            "accuracy": 0.25341796875,
            "f1": 0.24913752337624495,
            "f1_weighted": 0.2491519616952525
          },
          {
            "accuracy": 0.23828125,
            "f1": 0.23775097880681745,
            "f1_weighted": 0.23778016866064489
          },
          {
            "accuracy": 0.24267578125,
            "f1": 0.24253227910207092,
            "f1_weighted": 0.24251162660377082
          },
          {
            "accuracy": 0.30712890625,
            "f1": 0.30780946235655027,
            "f1_weighted": 0.30780564214095163
          },
          {
            "accuracy": 0.234375,
            "f1": 0.22814883637228656,
            "f1_weighted": 0.2281082387103911
          },
          {
            "accuracy": 0.259765625,
            "f1": 0.25018757441271855,
            "f1_weighted": 0.25019126404033154
          },
          {
            "accuracy": 0.23583984375,
            "f1": 0.23219050556561166,
            "f1_weighted": 0.23216065194111202
          },
          {
            "accuracy": 0.2841796875,
            "f1": 0.278107426130163,
            "f1_weighted": 0.27813946883251206
          },
          {
            "accuracy": 0.27880859375,
            "f1": 0.27702603135451487,
            "f1_weighted": 0.27703422110668874
          },
          {
            "accuracy": 0.24755859375,
            "f1": 0.24145588955819344,
            "f1_weighted": 0.2414806219865955
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}