{
  "dataset_revision": "c18a4f81a47ae6fa079fe9d32db288ddde38451d",
  "evaluation_time": 22.960001468658447,
  "kg_co2_emissions": 0.0038423630464963715,
  "mteb_version": "1.12.75",
  "scores": {
    "validation": [
      {
        "accuracy": 0.9786036036036037,
        "f1": 0.9752252252252253,
        "hf_subset": "ar-en",
        "languages": [
          "ara-Arab",
          "eng-Latn"
        ],
        "main_score": 0.9752252252252253,
        "precision": 0.9737612612612613,
        "recall": 0.9786036036036037
      },
      {
        "accuracy": 0.9864864864864865,
        "f1": 0.9844389844389845,
        "hf_subset": "de-en",
        "languages": [
          "deu-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9844389844389845,
        "precision": 0.9837837837837838,
        "recall": 0.9864864864864865
      },
      {
        "accuracy": 0.9797297297297297,
        "f1": 0.9756006006006005,
        "hf_subset": "en-ar",
        "languages": [
          "eng-Latn",
          "ara-Arab"
        ],
        "main_score": 0.9756006006006005,
        "precision": 0.9739864864864864,
        "recall": 0.9797297297297297
      },
      {
        "accuracy": 0.9887387387387387,
        "f1": 0.9861861861861863,
        "hf_subset": "en-de",
        "languages": [
          "eng-Latn",
          "deu-Latn"
        ],
        "main_score": 0.9861861861861863,
        "precision": 0.9853040540540541,
        "recall": 0.9887387387387387
      },
      {
        "accuracy": 0.9775280898876404,
        "f1": 0.9722846441947565,
        "hf_subset": "en-fr",
        "languages": [
          "eng-Latn",
          "fra-Latn"
        ],
        "main_score": 0.9722846441947565,
        "precision": 0.9701123595505617,
        "recall": 0.9775280898876404
      },
      {
        "accuracy": 0.9784714747039828,
        "f1": 0.9728022963760316,
        "hf_subset": "en-it",
        "languages": [
          "eng-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9728022963760316,
        "precision": 0.9703444564047363,
        "recall": 0.9784714747039828
      },
      {
        "accuracy": 0.9196326061997704,
        "f1": 0.8995581532894965,
        "hf_subset": "en-ja",
        "languages": [
          "eng-Latn",
          "jpn-Jpan"
        ],
        "main_score": 0.8995581532894965,
        "precision": 0.8902793723689245,
        "recall": 0.9196326061997704
      },
      {
        "accuracy": 0.8953356086461889,
        "f1": 0.8751232461130072,
        "hf_subset": "en-ko",
        "languages": [
          "eng-Latn",
          "kor-Hang"
        ],
        "main_score": 0.8751232461130072,
        "precision": 0.8661167993932499,
        "recall": 0.8953356086461889
      },
      {
        "accuracy": 0.9621136590229312,
        "f1": 0.9526420737786641,
        "hf_subset": "en-nl",
        "languages": [
          "eng-Latn",
          "nld-Latn"
        ],
        "main_score": 0.9526420737786641,
        "precision": 0.9483881688268528,
        "recall": 0.9621136590229312
      },
      {
        "accuracy": 0.9846827133479212,
        "f1": 0.9814004376367614,
        "hf_subset": "en-ro",
        "languages": [
          "eng-Latn",
          "ron-Latn"
        ],
        "main_score": 0.9814004376367614,
        "precision": 0.9801969365426696,
        "recall": 0.9846827133479212
      },
      {
        "accuracy": 0.931740614334471,
        "f1": 0.9136139552521805,
        "hf_subset": "en-zh",
        "languages": [
          "eng-Latn",
          "cmn-Hans"
        ],
        "main_score": 0.9136139552521805,
        "precision": 0.905365946150929,
        "recall": 0.931740614334471
      },
      {
        "accuracy": 0.9764044943820225,
        "f1": 0.970733012306046,
        "hf_subset": "fr-en",
        "languages": [
          "fra-Latn",
          "eng-Latn"
        ],
        "main_score": 0.970733012306046,
        "precision": 0.9683895131086141,
        "recall": 0.9764044943820225
      },
      {
        "accuracy": 0.9730893433799784,
        "f1": 0.9662207186426777,
        "hf_subset": "it-en",
        "languages": [
          "ita-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9662207186426777,
        "precision": 0.9632579834947973,
        "recall": 0.9730893433799784
      },
      {
        "accuracy": 0.957042957042957,
        "f1": 0.9463393749108036,
        "hf_subset": "it-nl",
        "languages": [
          "ita-Latn",
          "nld-Latn"
        ],
        "main_score": 0.9463393749108036,
        "precision": 0.9413919413919414,
        "recall": 0.957042957042957
      },
      {
        "accuracy": 0.9781181619256017,
        "f1": 0.9736896946962592,
        "hf_subset": "it-ro",
        "languages": [
          "ita-Latn",
          "ron-Latn"
        ],
        "main_score": 0.9736896946962592,
        "precision": 0.9717359591539022,
        "recall": 0.9781181619256017
      },
      {
        "accuracy": 0.9299655568312285,
        "f1": 0.9128587830080367,
        "hf_subset": "ja-en",
        "languages": [
          "jpn-Jpan",
          "eng-Latn"
        ],
        "main_score": 0.9128587830080367,
        "precision": 0.905504528638857,
        "recall": 0.9299655568312285
      },
      {
        "accuracy": 0.8873720136518771,
        "f1": 0.8645999241562382,
        "hf_subset": "ko-en",
        "languages": [
          "kor-Hang",
          "eng-Latn"
        ],
        "main_score": 0.8645999241562382,
        "precision": 0.8538788666775016,
        "recall": 0.8873720136518771
      },
      {
        "accuracy": 0.9481555333998006,
        "f1": 0.9336846603047999,
        "hf_subset": "nl-en",
        "languages": [
          "nld-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9336846603047999,
        "precision": 0.9273014290461947,
        "recall": 0.9481555333998006
      },
      {
        "accuracy": 0.9370629370629371,
        "f1": 0.9203796203796203,
        "hf_subset": "nl-it",
        "languages": [
          "nld-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9203796203796203,
        "precision": 0.9127872127872129,
        "recall": 0.9370629370629371
      },
      {
        "accuracy": 0.9605695509309967,
        "f1": 0.9497991967871485,
        "hf_subset": "nl-ro",
        "languages": [
          "nld-Latn",
          "ron-Latn"
        ],
        "main_score": 0.9497991967871485,
        "precision": 0.9447243519532675,
        "recall": 0.9605695509309967
      },
      {
        "accuracy": 0.9803063457330415,
        "f1": 0.976971970407419,
        "hf_subset": "ro-en",
        "languages": [
          "ron-Latn",
          "eng-Latn"
        ],
        "main_score": 0.976971970407419,
        "precision": 0.975565280816922,
        "recall": 0.9803063457330415
      },
      {
        "accuracy": 0.9726477024070022,
        "f1": 0.9668854850474107,
        "hf_subset": "ro-it",
        "languages": [
          "ron-Latn",
          "ita-Latn"
        ],
        "main_score": 0.9668854850474107,
        "precision": 0.9641684901531729,
        "recall": 0.9726477024070022
      },
      {
        "accuracy": 0.9583789704271632,
        "f1": 0.947243519532676,
        "hf_subset": "ro-nl",
        "languages": [
          "ron-Latn",
          "nld-Latn"
        ],
        "main_score": 0.947243519532676,
        "precision": 0.9423512230741147,
        "recall": 0.9583789704271632
      },
      {
        "accuracy": 0.9453924914675768,
        "f1": 0.9313830651714611,
        "hf_subset": "zh-en",
        "languages": [
          "cmn-Hans",
          "eng-Latn"
        ],
        "main_score": 0.9313830651714611,
        "precision": 0.9250094804702313,
        "recall": 0.9453924914675768
      }
    ]
  },
  "task_name": "IWSLT2017BitextMining"
}