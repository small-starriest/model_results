{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 12.546623706817627,
  "kg_co2_emissions": 0.0018790324301962789,
  "mteb_version": "1.14.12",
  "scores": {
    "test": [
      {
        "accuracy": 0.6759583053127101,
        "f1": 0.6674685954864829,
        "f1_weighted": 0.6741290789678889,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6759583053127101,
        "scores_per_experiment": [
          {
            "accuracy": 0.6788836583725623,
            "f1": 0.6683488191649135,
            "f1_weighted": 0.6750670027276209
          },
          {
            "accuracy": 0.67955615332885,
            "f1": 0.6707937823788162,
            "f1_weighted": 0.6764274477571149
          },
          {
            "accuracy": 0.6751849361129791,
            "f1": 0.6658044175145817,
            "f1_weighted": 0.6741379029116847
          },
          {
            "accuracy": 0.7098184263618023,
            "f1": 0.6929913649044296,
            "f1_weighted": 0.7111120867933303
          },
          {
            "accuracy": 0.6661062542030934,
            "f1": 0.6555293145495555,
            "f1_weighted": 0.6629834236001856
          },
          {
            "accuracy": 0.6607262945527909,
            "f1": 0.6500831620733032,
            "f1_weighted": 0.6525118065618567
          },
          {
            "accuracy": 0.6671149966375253,
            "f1": 0.6525959690698684,
            "f1_weighted": 0.6676647236700317
          },
          {
            "accuracy": 0.6546738399462004,
            "f1": 0.6517722583834045,
            "f1_weighted": 0.6522530241259148
          },
          {
            "accuracy": 0.7024209818426361,
            "f1": 0.6962118403399565,
            "f1_weighted": 0.7047924596379708
          },
          {
            "accuracy": 0.6650975117686617,
            "f1": 0.6705550264859989,
            "f1_weighted": 0.6643409118931778
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6835218888342351,
        "f1": 0.6709389414304089,
        "f1_weighted": 0.6825891156453237,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6835218888342351,
        "scores_per_experiment": [
          {
            "accuracy": 0.6984751598622725,
            "f1": 0.6839547941680072,
            "f1_weighted": 0.697886140336386
          },
          {
            "accuracy": 0.6812592228234137,
            "f1": 0.6731062592496593,
            "f1_weighted": 0.682410444244469
          },
          {
            "accuracy": 0.6886374815543532,
            "f1": 0.6755063394770864,
            "f1_weighted": 0.6887451715135442
          },
          {
            "accuracy": 0.7004426955238564,
            "f1": 0.6851838794908498,
            "f1_weighted": 0.7009756210058695
          },
          {
            "accuracy": 0.6719134284308903,
            "f1": 0.6564956816945231,
            "f1_weighted": 0.6674310429465027
          },
          {
            "accuracy": 0.6576487948844073,
            "f1": 0.6488007959810326,
            "f1_weighted": 0.6544051589194143
          },
          {
            "accuracy": 0.690113133300541,
            "f1": 0.6689239724022679,
            "f1_weighted": 0.6904762634056167
          },
          {
            "accuracy": 0.6606000983767831,
            "f1": 0.6492418098469672,
            "f1_weighted": 0.6583004581390898
          },
          {
            "accuracy": 0.7078209542547959,
            "f1": 0.693582283339373,
            "f1_weighted": 0.7077781616240841
          },
          {
            "accuracy": 0.6783079193310378,
            "f1": 0.6745935986543212,
            "f1_weighted": 0.6774826943182601
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}